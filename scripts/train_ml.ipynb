{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from IPython.display import display\n",
    "from joblib import dump, load\n",
    "from sklearn import set_config\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    auc,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from xgboost import XGBClassifier, plot_tree\n",
    "\n",
    "from ml_model import PipelineManager, PredictionPipeline\n",
    "from utils.GPU_models import KerasClassifierModel, gpu_model_hub\n",
    "from utils.preprocessing_utils import (\n",
    "    clean_and_preprocess_datasets,\n",
    "    clean_labs,\n",
    "    clean_notas,\n",
    "    clean_sociodemograficos,\n",
    "    disease_tests_list,\n",
    "    merge_classes,\n",
    "    merge_labs_notas,\n",
    "    word_count_feat_engineering,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display=\"diagram\")\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "png_renderer = pio.renderers[\"png\"]\n",
    "png_renderer.width = 1024\n",
    "png_renderer.height = 768\n",
    "\n",
    "pio.renderers.default = \"png\"\n",
    "\n",
    "# Feature and data config\n",
    "balance_classes = \"oversample\"  # False, 'oversample', or 'undersample'\n",
    "retrain_with_class_weight = False\n",
    "add_gpu_prediction = False\n",
    "consolidate_classes = False\n",
    "as_dual_class = False\n",
    "target_feature = \"Código\"\n",
    "text_feature = \"Plan\"\n",
    "\n",
    "# Should the best performing model be retrained with the fulldataset?\n",
    "refit_model = True\n",
    "\n",
    "# Hyperparameter tuning configuration\n",
    "fit_pipeline = True\n",
    "cv = 3\n",
    "n_iter = 8\n",
    "n_jobs = -3\n",
    "random_state=612\n",
    "\n",
    "# Paths\n",
    "cleaning_dict_path = \"utils/lab_test_name_aggregation.json\"\n",
    "save_path = Path(\"data\") / \"output\" / \"best_model.pickle\"\n",
    "fig_savepath = Path(\"data\") / \"output\" / \"figures\"\n",
    "prediction_pipeline_savepath = Path(\"model\") / \"prediction_pipeline.pickle\"\n",
    "\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "fig_savepath.mkdir(parents=True, exist_ok=True)\n",
    "prediction_pipeline_savepath.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cleaning_dict_path, \"r\") as in_file:\n",
    "    dict_tests = json.load(in_file)\n",
    "\n",
    "df_notas = pd.read_csv(\"data/notas.csv\", sep=\";\")\n",
    "df_laboratorios = pd.read_csv(\"data/laboratorios.csv\", sep=\";\")\n",
    "df_sociodemografico = pd.read_csv(\"data/sociodemografico.csv\", sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDRecord</th>\n",
       "      <th>Código</th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Tipo</th>\n",
       "      <th>Plan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44600</td>\n",
       "      <td>A539</td>\n",
       "      <td>SIFILIS, NO ESPECIFICADA</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>- ORDENO TAR ABC +3TC +ATV/r  - PROFILAXIS NO ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45038</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>- TAF/FTC/EVG/C MIPRES POR 2 MESES 20200602158...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40391</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>usuaria la cual se ve pertinente seguimiento d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106350</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>1. Se formula TAR (TDF/FTC+EFV)  2. S/S Paracl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105840</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>EDUCACIÓN  Se brinda retroalimentación con rel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  IDRecord Código                                             Nombre  \\\n",
       "0    44600   A539                           SIFILIS, NO ESPECIFICADA   \n",
       "1    45038   A530  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "2    40391   A530  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "3   106350   A530  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "4   105840   A530  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "\n",
       "                  Tipo                                               Plan  \n",
       "0  Confirmado Repetido  - ORDENO TAR ABC +3TC +ATV/r  - PROFILAXIS NO ...  \n",
       "1  Confirmado Repetido  - TAF/FTC/EVG/C MIPRES POR 2 MESES 20200602158...  \n",
       "2  Confirmado Repetido  usuaria la cual se ve pertinente seguimiento d...  \n",
       "3  Confirmado Repetido  1. Se formula TAR (TDF/FTC+EFV)  2. S/S Paracl...  \n",
       "4  Confirmado Repetido  EDUCACIÓN  Se brinda retroalimentación con rel...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notas.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDRecord</th>\n",
       "      <th>Codigo</th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Valor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95627</td>\n",
       "      <td>902045</td>\n",
       "      <td>TIEMPO DE PROTROMBINA (PT)</td>\n",
       "      <td>22/02/2022 18:43</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125572</td>\n",
       "      <td>902045</td>\n",
       "      <td>TIEMPO DE PROTROMBINA (PT)</td>\n",
       "      <td>17/02/2022 13:41</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55788</td>\n",
       "      <td>902045</td>\n",
       "      <td>TIEMPO DE PROTROMBINA (PT)</td>\n",
       "      <td>22/06/2021 12:50</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113766</td>\n",
       "      <td>902045</td>\n",
       "      <td>TIEMPO DE PROTROMBINA (PT)</td>\n",
       "      <td>5/08/2021 12:11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44596</td>\n",
       "      <td>902045</td>\n",
       "      <td>TIEMPO DE PROTROMBINA (PT)</td>\n",
       "      <td>5/08/2021 13:15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDRecord  Codigo                      Nombre             Fecha Valor\n",
       "0     95627  902045  TIEMPO DE PROTROMBINA (PT)  22/02/2022 18:43   NaN\n",
       "1    125572  902045  TIEMPO DE PROTROMBINA (PT)  17/02/2022 13:41   NaN\n",
       "2     55788  902045  TIEMPO DE PROTROMBINA (PT)  22/06/2021 12:50  1.05\n",
       "3    113766  902045  TIEMPO DE PROTROMBINA (PT)   5/08/2021 12:11   NaN\n",
       "4     44596  902045  TIEMPO DE PROTROMBINA (PT)   5/08/2021 13:15   NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_laboratorios.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDRecord</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Genero</th>\n",
       "      <th>GrupoEtnico</th>\n",
       "      <th>AreaResidencial</th>\n",
       "      <th>EstadoCivil</th>\n",
       "      <th>TSangre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Separado</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>292</td>\n",
       "      <td>84</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Casado</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>307</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>No reportado</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>325</td>\n",
       "      <td>94</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Rural</td>\n",
       "      <td>Viudo/a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDRecord  Edad  Genero                GrupoEtnico AreaResidencial  \\\n",
       "0         5    39   Mujer                    Mestizo     Zona Urbana   \n",
       "1       292    84  Hombre  Ninguno de los anteriores     Zona Urbana   \n",
       "2       300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "3       307    88  Hombre  Ninguno de los anteriores     Zona Urbana   \n",
       "4       325    94  Hombre  Ninguno de los anteriores      Zona Rural   \n",
       "\n",
       "    EstadoCivil TSangre  \n",
       "0      Separado     NaN  \n",
       "1        Casado     NaN  \n",
       "2       Soltero      O+  \n",
       "3  No reportado     NaN  \n",
       "4       Viudo/a     NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sociodemografico.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sociodemografico = clean_sociodemograficos(df_sociodemografico)\n",
    "df_laboratorios = clean_labs(df_laboratorios, name_aggregation_dict=dict_tests)\n",
    "df_notas = clean_notas(df_notas, apply_lemmatization=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the sociodemographic data with the medical notes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDRecord</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Genero</th>\n",
       "      <th>GrupoEtnico</th>\n",
       "      <th>AreaResidencial</th>\n",
       "      <th>EstadoCivil</th>\n",
       "      <th>TSangre</th>\n",
       "      <th>Código</th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Tipo</th>\n",
       "      <th>Plan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Separado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E109</td>\n",
       "      <td>DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>PACIENTE QUIEN CONTINUA EN PROGRAMA DE NEFROPR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>292</td>\n",
       "      <td>84</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Casado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>CONTINUA EN PROGRAMA DE CRONICOS.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>1- CONTINUAR EN PAD 2 - RECOMENDACIONES DE DIE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>E109</td>\n",
       "      <td>DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>1- CONTINUAR EN PAD 2 - RECOMENDACIONES DE DIE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Confirmado Nuevo</td>\n",
       "      <td>1- CONTINUAR EN PAD 2 - RECOMENDACIONES DE DIE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140167</th>\n",
       "      <td>205218</td>\n",
       "      <td>28</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A539</td>\n",
       "      <td>SIFILIS, NO ESPECIFICADA</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>se explica acerca del programa, Se recomienda ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140168</th>\n",
       "      <td>205227</td>\n",
       "      <td>24</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>Elaboracion duelo frente al diagnostico.   Ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140169</th>\n",
       "      <td>205253</td>\n",
       "      <td>84</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Casado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E109</td>\n",
       "      <td>DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>FUROATO MOMETASONA 1 SPRY NASAL POR CADA FOSA ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140170</th>\n",
       "      <td>205577</td>\n",
       "      <td>62</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Desconocido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Impresión Diagnóstica</td>\n",
       "      <td>CONTROL MEICO EN UN MES-INFECTOLOGIA  VALORACI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140171</th>\n",
       "      <td>206307</td>\n",
       "      <td>57</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Desconocido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E149</td>\n",
       "      <td>DIABETES MELLITUS, NO ESPECIFICADA SIN MENCION...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>CONTINUA EN SEGUIMIENTO POR PROGRAMA NEFROPROT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140172 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        IDRecord  Edad  Genero                GrupoEtnico AreaResidencial  \\\n",
       "0              5    39   Mujer                    Mestizo     Zona Urbana   \n",
       "1            292    84  Hombre  Ninguno de los anteriores     Zona Urbana   \n",
       "2            300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "3            300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "4            300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "...          ...   ...     ...                        ...             ...   \n",
       "140167    205218    28  Hombre  Ninguno de los anteriores     Zona Urbana   \n",
       "140168    205227    24  Hombre  Ninguno de los anteriores     Zona Urbana   \n",
       "140169    205253    84  Hombre                    Mestizo     Zona Urbana   \n",
       "140170    205577    62  Hombre                    Mestizo     Zona Urbana   \n",
       "140171    206307    57  Hombre                    Mestizo     Zona Urbana   \n",
       "\n",
       "        EstadoCivil TSangre Código  \\\n",
       "0          Separado     NaN   E109   \n",
       "1            Casado     NaN   E119   \n",
       "2           Soltero      O+   E119   \n",
       "3           Soltero      O+   E109   \n",
       "4           Soltero      O+   E119   \n",
       "...             ...     ...    ...   \n",
       "140167          NaN     NaN   A539   \n",
       "140168      Soltero      O+   A530   \n",
       "140169       Casado     NaN   E109   \n",
       "140170  Desconocido     NaN   E119   \n",
       "140171  Desconocido     NaN   E149   \n",
       "\n",
       "                                                   Nombre  \\\n",
       "0       DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...   \n",
       "1       DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...   \n",
       "2       DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...   \n",
       "3       DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...   \n",
       "4       DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...   \n",
       "...                                                   ...   \n",
       "140167                           SIFILIS, NO ESPECIFICADA   \n",
       "140168  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "140169  DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...   \n",
       "140170  DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...   \n",
       "140171  DIABETES MELLITUS, NO ESPECIFICADA SIN MENCION...   \n",
       "\n",
       "                         Tipo  \\\n",
       "0         Confirmado Repetido   \n",
       "1         Confirmado Repetido   \n",
       "2         Confirmado Repetido   \n",
       "3         Confirmado Repetido   \n",
       "4            Confirmado Nuevo   \n",
       "...                       ...   \n",
       "140167    Confirmado Repetido   \n",
       "140168    Confirmado Repetido   \n",
       "140169    Confirmado Repetido   \n",
       "140170  Impresión Diagnóstica   \n",
       "140171    Confirmado Repetido   \n",
       "\n",
       "                                                     Plan  \n",
       "0       PACIENTE QUIEN CONTINUA EN PROGRAMA DE NEFROPR...  \n",
       "1                      CONTINUA EN PROGRAMA DE CRONICOS.   \n",
       "2       1- CONTINUAR EN PAD 2 - RECOMENDACIONES DE DIE...  \n",
       "3       1- CONTINUAR EN PAD 2 - RECOMENDACIONES DE DIE...  \n",
       "4       1- CONTINUAR EN PAD 2 - RECOMENDACIONES DE DIE...  \n",
       "...                                                   ...  \n",
       "140167  se explica acerca del programa, Se recomienda ...  \n",
       "140168  Elaboracion duelo frente al diagnostico.   Ref...  \n",
       "140169  FUROATO MOMETASONA 1 SPRY NASAL POR CADA FOSA ...  \n",
       "140170  CONTROL MEICO EN UN MES-INFECTOLOGIA  VALORACI...  \n",
       "140171  CONTINUA EN SEGUIMIENTO POR PROGRAMA NEFROPROT...  \n",
       "\n",
       "[140172 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge = df_sociodemografico.merge(df_notas, how=\"inner\", on=\"IDRecord\")\n",
    "df_merge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDRecord</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Genero</th>\n",
       "      <th>GrupoEtnico</th>\n",
       "      <th>AreaResidencial</th>\n",
       "      <th>EstadoCivil</th>\n",
       "      <th>TSangre</th>\n",
       "      <th>Código</th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Tipo</th>\n",
       "      <th>Plan</th>\n",
       "      <th>acido</th>\n",
       "      <th>antibio</th>\n",
       "      <th>asintoma</th>\n",
       "      <th>cabeza</th>\n",
       "      <th>diabet</th>\n",
       "      <th>diet</th>\n",
       "      <th>gluco</th>\n",
       "      <th>hepat</th>\n",
       "      <th>insulin</th>\n",
       "      <th>keto</th>\n",
       "      <th>penici</th>\n",
       "      <th>preservativo</th>\n",
       "      <th>rpr</th>\n",
       "      <th>sable</th>\n",
       "      <th>serolo</th>\n",
       "      <th>sifili</th>\n",
       "      <th>test_reloj_orden</th>\n",
       "      <th>vih</th>\n",
       "      <th>top_lab_name</th>\n",
       "      <th>top_lab_avg_value</th>\n",
       "      <th>top_lab_max_value</th>\n",
       "      <th>top_lab_count</th>\n",
       "      <th>total_lab_count</th>\n",
       "      <th>first_lab_date</th>\n",
       "      <th>last_lab_date</th>\n",
       "      <th>date_diff_first_last</th>\n",
       "      <th>date_diff_mean</th>\n",
       "      <th>date_diff_max</th>\n",
       "      <th>liver_damage_count</th>\n",
       "      <th>liver_damage_max</th>\n",
       "      <th>hematic_info_count</th>\n",
       "      <th>hematic_info_max</th>\n",
       "      <th>bacterias_count</th>\n",
       "      <th>bacterias_max</th>\n",
       "      <th>hormones_count</th>\n",
       "      <th>hormones_max</th>\n",
       "      <th>other_diseases_count</th>\n",
       "      <th>other_diseases_max</th>\n",
       "      <th>kidney_damage_count</th>\n",
       "      <th>kidney_damage_max</th>\n",
       "      <th>heart_damage_count</th>\n",
       "      <th>heart_damage_max</th>\n",
       "      <th>minerals_count</th>\n",
       "      <th>minerals_max</th>\n",
       "      <th>white_cells_count</th>\n",
       "      <th>white_cells_max</th>\n",
       "      <th>vih_count</th>\n",
       "      <th>vih_max</th>\n",
       "      <th>diabetes_tests_count</th>\n",
       "      <th>diabetes_tests_max</th>\n",
       "      <th>syphilis_tests_count</th>\n",
       "      <th>syphilis_tests_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Separado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E109</td>\n",
       "      <td>DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>PACIENTE QUIEN CONTINUA EN PROGRAMA DE NEFROPR...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>colesterol test</td>\n",
       "      <td>108.333333</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.609114e+09</td>\n",
       "      <td>1.609114e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>292</td>\n",
       "      <td>84</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Casado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>CONTINUA EN PROGRAMA DE CRONICOS.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>1- CONTINUAR EN PAD 2 - RECOMENDACIONES DE DIE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>E109</td>\n",
       "      <td>DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>1- CONTINUAR EN PAD 2 - RECOMENDACIONES DE DIE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Confirmado Nuevo</td>\n",
       "      <td>1- CONTINUAR EN PAD 2 - RECOMENDACIONES DE DIE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDRecord  Edad  Genero                GrupoEtnico AreaResidencial  \\\n",
       "0         5    39   Mujer                    Mestizo     Zona Urbana   \n",
       "1       292    84  Hombre  Ninguno de los anteriores     Zona Urbana   \n",
       "2       300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "3       300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "4       300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "\n",
       "  EstadoCivil TSangre Código  \\\n",
       "0    Separado     NaN   E109   \n",
       "1      Casado     NaN   E119   \n",
       "2     Soltero      O+   E119   \n",
       "3     Soltero      O+   E109   \n",
       "4     Soltero      O+   E119   \n",
       "\n",
       "                                              Nombre                 Tipo  \\\n",
       "0  DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...  Confirmado Repetido   \n",
       "1  DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...  Confirmado Repetido   \n",
       "2  DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...  Confirmado Repetido   \n",
       "3  DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...  Confirmado Repetido   \n",
       "4  DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...     Confirmado Nuevo   \n",
       "\n",
       "                                                Plan  acido  antibio  \\\n",
       "0  PACIENTE QUIEN CONTINUA EN PROGRAMA DE NEFROPR...      0        0   \n",
       "1                 CONTINUA EN PROGRAMA DE CRONICOS.       0        0   \n",
       "2  1- CONTINUAR EN PAD 2 - RECOMENDACIONES DE DIE...      0        0   \n",
       "3  1- CONTINUAR EN PAD 2 - RECOMENDACIONES DE DIE...      0        0   \n",
       "4  1- CONTINUAR EN PAD 2 - RECOMENDACIONES DE DIE...      0        0   \n",
       "\n",
       "   asintoma  cabeza  diabet  diet  gluco  hepat  insulin  keto  penici  \\\n",
       "0         0       0       0     0      0      0        0     0       0   \n",
       "1         0       0       0     0      0      0        0     0       0   \n",
       "2         0       0       0     1      0      0        0     0       0   \n",
       "3         0       0       0     1      0      0        0     0       0   \n",
       "4         0       0       0     1      0      0        0     0       0   \n",
       "\n",
       "   preservativo  rpr  sable  serolo  sifili  test_reloj_orden  vih  \\\n",
       "0             0    0      0       0       0                 0    0   \n",
       "1             0    0      0       0       0                 0    0   \n",
       "2             0    0      0       0       0                 0    0   \n",
       "3             0    0      0       0       0                 0    0   \n",
       "4             0    0      0       0       0                 0    0   \n",
       "\n",
       "      top_lab_name  top_lab_avg_value  top_lab_max_value  top_lab_count  \\\n",
       "0  colesterol test         108.333333              150.0            3.0   \n",
       "1              NaN                NaN                NaN            NaN   \n",
       "2              NaN                NaN                NaN            NaN   \n",
       "3              NaN                NaN                NaN            NaN   \n",
       "4              NaN                NaN                NaN            NaN   \n",
       "\n",
       "   total_lab_count  first_lab_date  last_lab_date  date_diff_first_last  \\\n",
       "0              8.0    1.609114e+09   1.609114e+09                   0.0   \n",
       "1              NaN             NaN            NaN                   NaN   \n",
       "2              NaN             NaN            NaN                   NaN   \n",
       "3              NaN             NaN            NaN                   NaN   \n",
       "4              NaN             NaN            NaN                   NaN   \n",
       "\n",
       "   date_diff_mean  date_diff_max  liver_damage_count  liver_damage_max  \\\n",
       "0             0.0            0.0                 0.0               0.0   \n",
       "1             NaN            NaN                 NaN               NaN   \n",
       "2             NaN            NaN                 NaN               NaN   \n",
       "3             NaN            NaN                 NaN               NaN   \n",
       "4             NaN            NaN                 NaN               NaN   \n",
       "\n",
       "   hematic_info_count  hematic_info_max  bacterias_count  bacterias_max  \\\n",
       "0                 1.0              10.0              0.0            0.0   \n",
       "1                 NaN               NaN              NaN            NaN   \n",
       "2                 NaN               NaN              NaN            NaN   \n",
       "3                 NaN               NaN              NaN            NaN   \n",
       "4                 NaN               NaN              NaN            NaN   \n",
       "\n",
       "   hormones_count  hormones_max  other_diseases_count  other_diseases_max  \\\n",
       "0             1.0         189.0                   0.0                 0.0   \n",
       "1             NaN           NaN                   NaN                 NaN   \n",
       "2             NaN           NaN                   NaN                 NaN   \n",
       "3             NaN           NaN                   NaN                 NaN   \n",
       "4             NaN           NaN                   NaN                 NaN   \n",
       "\n",
       "   kidney_damage_count  kidney_damage_max  heart_damage_count  \\\n",
       "0                  2.0              500.0                 3.0   \n",
       "1                  NaN                NaN                 NaN   \n",
       "2                  NaN                NaN                 NaN   \n",
       "3                  NaN                NaN                 NaN   \n",
       "4                  NaN                NaN                 NaN   \n",
       "\n",
       "   heart_damage_max  minerals_count  minerals_max  white_cells_count  \\\n",
       "0             150.0             1.0           8.0                0.0   \n",
       "1               NaN             NaN           NaN                NaN   \n",
       "2               NaN             NaN           NaN                NaN   \n",
       "3               NaN             NaN           NaN                NaN   \n",
       "4               NaN             NaN           NaN                NaN   \n",
       "\n",
       "   white_cells_max  vih_count  vih_max  diabetes_tests_count  \\\n",
       "0              0.0        0.0      0.0                   0.0   \n",
       "1              NaN        NaN      NaN                   NaN   \n",
       "2              NaN        NaN      NaN                   NaN   \n",
       "3              NaN        NaN      NaN                   NaN   \n",
       "4              NaN        NaN      NaN                   NaN   \n",
       "\n",
       "   diabetes_tests_max  syphilis_tests_count  syphilis_tests_max  \n",
       "0                 0.0                   0.0                 0.0  \n",
       "1                 NaN                   NaN                 NaN  \n",
       "2                 NaN                   NaN                 NaN  \n",
       "3                 NaN                   NaN                 NaN  \n",
       "4                 NaN                   NaN                 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Consolidate the classes\n",
    "if consolidate_classes:\n",
    "    df_merge = merge_classes(df_merge)\n",
    "\n",
    "# Perform word count feature engineering\n",
    "df_merge = word_count_feat_engineering(df_merge)\n",
    "\n",
    "# Preprocess the lab data and merge it with the sociodemographic data\n",
    "df_merge = merge_labs_notas(df_laboratorios, df_merge)\n",
    "\n",
    "with pd.option_context(\"display.max_columns\", None):\n",
    "    display(df_merge.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]),\n",
       " array([  977,    94,  2614,  1970, 60586, 47408,  6278, 17437,  2808]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_merge.drop(labels=[target_feature], axis=1)\n",
    "y = df_merge[target_feature]\n",
    "if as_dual_class:\n",
    "    # Remove all but the first two characters of the classes, i.e. A5 or E1\n",
    "    y = y.str[:2]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_labels = label_encoder.fit_transform(y)\n",
    "\n",
    "np.unique(y_labels, return_counts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]),\n",
       " array([  195,    19,   523,   394, 12117,  9481,  1256,  3487,   562]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_labels, train_size=0.2, random_state=42, stratify=y_labels\n",
    ")\n",
    "np.unique(y_train, return_counts=True)  # Let's check the number of samples per label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]),\n",
       " array([12117, 12117, 12117, 12117, 12117, 12117, 12117, 12117, 12117]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if balance_classes == \"oversample\":\n",
    "    # Using a naive oversampling approach\n",
    "    sampler = RandomOverSampler(random_state=42)\n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "elif balance_classes == \"undersample\":\n",
    "    # Using a naive oversampling approach\n",
    "    sampler = RandomUnderSampler(random_state=42)\n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "np.unique(y_train, return_counts=True)  # Let's check the number of samples per label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further (optional) feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = \"nnlm-es-dim128\"\n",
    "embedding = \"nnlm-es-dim128-with-normalization\"\n",
    "# embedding = \"universal\"\n",
    "\n",
    "if add_gpu_prediction:\n",
    "    model_function = gpu_model_hub\n",
    "    clf = KerasClassifierModel(\n",
    "        build_fn=model_function,\n",
    "        class_number=len(df_notas[target_feature].unique()),\n",
    "        embedding=embedding,\n",
    "        epochs=400,\n",
    "        batch_size=400,\n",
    "        verbose=10,\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train[text_feature], y_train)\n",
    "    clf.plot_learning_curves(\"data/output/gpu_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_gpu_prediction:\n",
    "    y_pred = clf.predict(X_test[text_feature])\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            classification_report(y_test, y_pred, output_dict=True)\n",
    "        ).transpose()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_gpu_prediction:\n",
    "    X_pred = clf.predict(df_merge[text_feature])\n",
    "    df_merge[\"GPU_prediction\"] = X_pred\n",
    "    df_merge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the numerical features that will be used in the model\n",
    "numerical_features = list(\n",
    "    set(\n",
    "        [\n",
    "            \"Edad\",\n",
    "            \"top_lab_avg_value\",\n",
    "            \"top_lab_max_value\",\n",
    "            \"top_lab_count\",\n",
    "            \"total_lab_count\",\n",
    "            \"date_diff_mean\",\n",
    "            \"date_diff_max\",\n",
    "            \"first_lab_date\",\n",
    "            \"last_lab_date\",\n",
    "            \"date_diff_first_last\",\n",
    "        ]\n",
    "        + list(df_merge.drop(columns=\"IDRecord\").select_dtypes(include=\"int64\").columns)\n",
    "        + [f\"{test[1]}_count\" for test in disease_tests_list()]\n",
    "        + [f\"{test[1]}_max\" for test in disease_tests_list()]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Now select the categorical features\n",
    "categorical_features = [\n",
    "    \"Genero\",\n",
    "    \"GrupoEtnico\",\n",
    "    # \"AreaResidencial\",\n",
    "    \"EstadoCivil\",\n",
    "    # \"TSangre\",\n",
    "    # \"Tipo\",\n",
    "    \"top_lab_name\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(fill_value=&#x27;missing&#x27;,\n",
       "                                                                                 strategy=&#x27;constant&#x27;)),\n",
       "                                                                  (&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;Genero&#x27;, &#x27;GrupoEtnico&#x27;,\n",
       "                                                   &#x27;EstadoCivil&#x27;,\n",
       "                                                   &#x27;top_lab_name&#x27;]),\n",
       "                                                 (&#x27;numerical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  (&#x27;scaler&#x27;,\n",
       "                                                                   StandardScal...\n",
       "                                                                   CountVectorizer(stop_words=[&#x27;de&#x27;,\n",
       "                                                                                               &#x27;la&#x27;,\n",
       "                                                                                               &#x27;que&#x27;,\n",
       "                                                                                               &#x27;el&#x27;,\n",
       "                                                                                               &#x27;en&#x27;,\n",
       "                                                                                               &#x27;y&#x27;,\n",
       "                                                                                               &#x27;a&#x27;,\n",
       "                                                                                               &#x27;los&#x27;,\n",
       "                                                                                               &#x27;del&#x27;,\n",
       "                                                                                               &#x27;se&#x27;,\n",
       "                                                                                               &#x27;las&#x27;,\n",
       "                                                                                               &#x27;por&#x27;,\n",
       "                                                                                               &#x27;un&#x27;,\n",
       "                                                                                               &#x27;para&#x27;,\n",
       "                                                                                               &#x27;con&#x27;,\n",
       "                                                                                               &#x27;no&#x27;,\n",
       "                                                                                               &#x27;una&#x27;,\n",
       "                                                                                               &#x27;su&#x27;,\n",
       "                                                                                               &#x27;al&#x27;,\n",
       "                                                                                               &#x27;lo&#x27;,\n",
       "                                                                                               &#x27;como&#x27;,\n",
       "                                                                                               &#x27;mas&#x27;,\n",
       "                                                                                               &#x27;pero&#x27;,\n",
       "                                                                                               &#x27;sus&#x27;,\n",
       "                                                                                               &#x27;le&#x27;,\n",
       "                                                                                               &#x27;ya&#x27;,\n",
       "                                                                                               &#x27;o&#x27;,\n",
       "                                                                                               &#x27;este&#x27;,\n",
       "                                                                                               &#x27;si&#x27;,\n",
       "                                                                                               &#x27;porque&#x27;, ...],\n",
       "                                                                                   strip_accents=&#x27;unicode&#x27;)),\n",
       "                                                                  (&#x27;tfidf&#x27;,\n",
       "                                                                   TfidfTransformer())]),\n",
       "                                                  &#x27;Plan&#x27;)])),\n",
       "                (&#x27;feature_selector&#x27;,\n",
       "                 SelectFromModel(estimator=RandomForestRegressor())),\n",
       "                (&#x27;estimator&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(fill_value=&#x27;missing&#x27;,\n",
       "                                                                                 strategy=&#x27;constant&#x27;)),\n",
       "                                                                  (&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;Genero&#x27;, &#x27;GrupoEtnico&#x27;,\n",
       "                                                   &#x27;EstadoCivil&#x27;,\n",
       "                                                   &#x27;top_lab_name&#x27;]),\n",
       "                                                 (&#x27;numerical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  (&#x27;scaler&#x27;,\n",
       "                                                                   StandardScal...\n",
       "                                                                   CountVectorizer(stop_words=[&#x27;de&#x27;,\n",
       "                                                                                               &#x27;la&#x27;,\n",
       "                                                                                               &#x27;que&#x27;,\n",
       "                                                                                               &#x27;el&#x27;,\n",
       "                                                                                               &#x27;en&#x27;,\n",
       "                                                                                               &#x27;y&#x27;,\n",
       "                                                                                               &#x27;a&#x27;,\n",
       "                                                                                               &#x27;los&#x27;,\n",
       "                                                                                               &#x27;del&#x27;,\n",
       "                                                                                               &#x27;se&#x27;,\n",
       "                                                                                               &#x27;las&#x27;,\n",
       "                                                                                               &#x27;por&#x27;,\n",
       "                                                                                               &#x27;un&#x27;,\n",
       "                                                                                               &#x27;para&#x27;,\n",
       "                                                                                               &#x27;con&#x27;,\n",
       "                                                                                               &#x27;no&#x27;,\n",
       "                                                                                               &#x27;una&#x27;,\n",
       "                                                                                               &#x27;su&#x27;,\n",
       "                                                                                               &#x27;al&#x27;,\n",
       "                                                                                               &#x27;lo&#x27;,\n",
       "                                                                                               &#x27;como&#x27;,\n",
       "                                                                                               &#x27;mas&#x27;,\n",
       "                                                                                               &#x27;pero&#x27;,\n",
       "                                                                                               &#x27;sus&#x27;,\n",
       "                                                                                               &#x27;le&#x27;,\n",
       "                                                                                               &#x27;ya&#x27;,\n",
       "                                                                                               &#x27;o&#x27;,\n",
       "                                                                                               &#x27;este&#x27;,\n",
       "                                                                                               &#x27;si&#x27;,\n",
       "                                                                                               &#x27;porque&#x27;, ...],\n",
       "                                                                                   strip_accents=&#x27;unicode&#x27;)),\n",
       "                                                                  (&#x27;tfidf&#x27;,\n",
       "                                                                   TfidfTransformer())]),\n",
       "                                                  &#x27;Plan&#x27;)])),\n",
       "                (&#x27;feature_selector&#x27;,\n",
       "                 SelectFromModel(estimator=RandomForestRegressor())),\n",
       "                (&#x27;estimator&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(fill_value=&#x27;missing&#x27;,\n",
       "                                                                strategy=&#x27;constant&#x27;)),\n",
       "                                                 (&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                 [&#x27;Genero&#x27;, &#x27;GrupoEtnico&#x27;, &#x27;EstadoCivil&#x27;,\n",
       "                                  &#x27;top_lab_name&#x27;]),\n",
       "                                (&#x27;numerical&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer()),\n",
       "                                                 (&#x27;scaler&#x27;, StandardScaler())]),\n",
       "                                 [&#x27;bacterias_count&#x27;, &#x27;kidn...\n",
       "                                  &#x27;preservativo&#x27;, &#x27;heart_damage_count&#x27;,\n",
       "                                  &#x27;syphilis_tests_max&#x27;, &#x27;keto&#x27;, ...]),\n",
       "                                (&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                  CountVectorizer(stop_words=[&#x27;de&#x27;,\n",
       "                                                                              &#x27;la&#x27;,\n",
       "                                                                              &#x27;que&#x27;,\n",
       "                                                                              &#x27;el&#x27;,\n",
       "                                                                              &#x27;en&#x27;,\n",
       "                                                                              &#x27;y&#x27;,\n",
       "                                                                              &#x27;a&#x27;,\n",
       "                                                                              &#x27;los&#x27;,\n",
       "                                                                              &#x27;del&#x27;,\n",
       "                                                                              &#x27;se&#x27;,\n",
       "                                                                              &#x27;las&#x27;,\n",
       "                                                                              &#x27;por&#x27;,\n",
       "                                                                              &#x27;un&#x27;,\n",
       "                                                                              &#x27;para&#x27;,\n",
       "                                                                              &#x27;con&#x27;,\n",
       "                                                                              &#x27;no&#x27;,\n",
       "                                                                              &#x27;una&#x27;,\n",
       "                                                                              &#x27;su&#x27;,\n",
       "                                                                              &#x27;al&#x27;,\n",
       "                                                                              &#x27;lo&#x27;,\n",
       "                                                                              &#x27;como&#x27;,\n",
       "                                                                              &#x27;mas&#x27;,\n",
       "                                                                              &#x27;pero&#x27;,\n",
       "                                                                              &#x27;sus&#x27;,\n",
       "                                                                              &#x27;le&#x27;,\n",
       "                                                                              &#x27;ya&#x27;,\n",
       "                                                                              &#x27;o&#x27;,\n",
       "                                                                              &#x27;este&#x27;,\n",
       "                                                                              &#x27;si&#x27;,\n",
       "                                                                              &#x27;porque&#x27;, ...],\n",
       "                                                                  strip_accents=&#x27;unicode&#x27;)),\n",
       "                                                 (&#x27;tfidf&#x27;,\n",
       "                                                  TfidfTransformer())]),\n",
       "                                 &#x27;Plan&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Genero&#x27;, &#x27;GrupoEtnico&#x27;, &#x27;EstadoCivil&#x27;, &#x27;top_lab_name&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(fill_value=&#x27;missing&#x27;, strategy=&#x27;constant&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numerical</label><div class=\"sk-toggleable__content\"><pre>[&#x27;bacterias_count&#x27;, &#x27;kidney_damage_count&#x27;, &#x27;minerals_max&#x27;, &#x27;hormones_max&#x27;, &#x27;vih&#x27;, &#x27;liver_damage_max&#x27;, &#x27;other_diseases_max&#x27;, &#x27;vih_max&#x27;, &#x27;asintoma&#x27;, &#x27;hematic_info_max&#x27;, &#x27;diabet&#x27;, &#x27;serolo&#x27;, &#x27;gluco&#x27;, &#x27;first_lab_date&#x27;, &#x27;other_diseases_count&#x27;, &#x27;insulin&#x27;, &#x27;penici&#x27;, &#x27;hormones_count&#x27;, &#x27;syphilis_tests_count&#x27;, &#x27;liver_damage_count&#x27;, &#x27;Edad&#x27;, &#x27;top_lab_max_value&#x27;, &#x27;top_lab_count&#x27;, &#x27;diet&#x27;, &#x27;last_lab_date&#x27;, &#x27;test_reloj_orden&#x27;, &#x27;preservativo&#x27;, &#x27;heart_damage_count&#x27;, &#x27;syphilis_tests_max&#x27;, &#x27;keto&#x27;, &#x27;antibio&#x27;, &#x27;top_lab_avg_value&#x27;, &#x27;sifili&#x27;, &#x27;date_diff_mean&#x27;, &#x27;date_diff_first_last&#x27;, &#x27;total_lab_count&#x27;, &#x27;diabetes_tests_count&#x27;, &#x27;hepat&#x27;, &#x27;date_diff_max&#x27;, &#x27;kidney_damage_max&#x27;, &#x27;heart_damage_max&#x27;, &#x27;minerals_count&#x27;, &#x27;white_cells_count&#x27;, &#x27;rpr&#x27;, &#x27;hematic_info_count&#x27;, &#x27;white_cells_max&#x27;, &#x27;vih_count&#x27;, &#x27;diabetes_tests_max&#x27;, &#x27;sable&#x27;, &#x27;bacterias_max&#x27;, &#x27;cabeza&#x27;, &#x27;acido&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>Plan</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=[&#x27;de&#x27;, &#x27;la&#x27;, &#x27;que&#x27;, &#x27;el&#x27;, &#x27;en&#x27;, &#x27;y&#x27;, &#x27;a&#x27;, &#x27;los&#x27;,\n",
       "                            &#x27;del&#x27;, &#x27;se&#x27;, &#x27;las&#x27;, &#x27;por&#x27;, &#x27;un&#x27;, &#x27;para&#x27;, &#x27;con&#x27;,\n",
       "                            &#x27;no&#x27;, &#x27;una&#x27;, &#x27;su&#x27;, &#x27;al&#x27;, &#x27;lo&#x27;, &#x27;como&#x27;, &#x27;mas&#x27;,\n",
       "                            &#x27;pero&#x27;, &#x27;sus&#x27;, &#x27;le&#x27;, &#x27;ya&#x27;, &#x27;o&#x27;, &#x27;este&#x27;, &#x27;si&#x27;,\n",
       "                            &#x27;porque&#x27;, ...],\n",
       "                strip_accents=&#x27;unicode&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">feature_selector: SelectFromModel</label><div class=\"sk-toggleable__content\"><pre>SelectFromModel(estimator=RandomForestRegressor())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('encoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['Genero', 'GrupoEtnico',\n",
       "                                                   'EstadoCivil',\n",
       "                                                   'top_lab_name']),\n",
       "                                                 ('numerical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScal...\n",
       "                                                                   CountVectorizer(stop_words=['de',\n",
       "                                                                                               'la',\n",
       "                                                                                               'que',\n",
       "                                                                                               'el',\n",
       "                                                                                               'en',\n",
       "                                                                                               'y',\n",
       "                                                                                               'a',\n",
       "                                                                                               'los',\n",
       "                                                                                               'del',\n",
       "                                                                                               'se',\n",
       "                                                                                               'las',\n",
       "                                                                                               'por',\n",
       "                                                                                               'un',\n",
       "                                                                                               'para',\n",
       "                                                                                               'con',\n",
       "                                                                                               'no',\n",
       "                                                                                               'una',\n",
       "                                                                                               'su',\n",
       "                                                                                               'al',\n",
       "                                                                                               'lo',\n",
       "                                                                                               'como',\n",
       "                                                                                               'mas',\n",
       "                                                                                               'pero',\n",
       "                                                                                               'sus',\n",
       "                                                                                               'le',\n",
       "                                                                                               'ya',\n",
       "                                                                                               'o',\n",
       "                                                                                               'este',\n",
       "                                                                                               'si',\n",
       "                                                                                               'porque', ...],\n",
       "                                                                                   strip_accents='unicode')),\n",
       "                                                                  ('tfidf',\n",
       "                                                                   TfidfTransformer())]),\n",
       "                                                  'Plan')])),\n",
       "                ('feature_selector',\n",
       "                 SelectFromModel(estimator=RandomForestRegressor())),\n",
       "                ('estimator', RandomForestClassifier())])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the GPU prediction if we are using a GPU model for predicting the data\n",
    "if \"GPU_prediction\" in df_merge:\n",
    "    categorical_features.append(\"GPU_prediction\")\n",
    "\n",
    "pipeline = PipelineManager(estimator=\"classifier\")\n",
    "pipeline.set_numerical_features(numerical_features)\n",
    "pipeline.set_categorical_features(categorical_features)\n",
    "pipeline.set_text_feature(text_feature)\n",
    "pipeline.set_basic_pipeline()\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": np.linspace(10, 100, 10, dtype=int),\n",
    "    \"max_depth\": list(np.linspace(2, 15, 5, dtype=int)),\n",
    "    \"eta\": np.linspace(0.01, 0.5, 10, dtype=float),\n",
    "    \"min_child_weight\": np.linspace(0.5, 20, 5, dtype=float),\n",
    "    \"gamma\": np.linspace(0, 1, 5, dtype=float),\n",
    "    \"subsample\": np.linspace(0.1, 1, 5, dtype=float),\n",
    "    \"colsample_bytree\": np.linspace(0.2, 1, 5, dtype=float),\n",
    "    \"reg_lambda\": np.linspace(0, 10, 5, dtype=float),\n",
    "    \"reg_alpha\": np.linspace(0, 10, 5, dtype=float),\n",
    "    # \"scale_pos_weight\": np.linspace(0.1, 500, 100, dtype=float),\n",
    "}\n",
    "estimator = XGBClassifier()\n",
    "pipeline.add_estimator(estimator, param_grid)\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": np.linspace(0, 5, 20, dtype=float),\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"gamma\": [\"auto\", \"scale\"],\n",
    "    \"class_weight\": [\"balanced\", None],\n",
    "    \"coef0\": np.linspace(0, 5, 20, dtype=float),\n",
    "    \"degree\": np.linspace(1, 5, 10, dtype=int),\n",
    "}\n",
    "estimator = SVC()\n",
    "pipeline.add_estimator(estimator, param_grid)\n",
    "\n",
    "\n",
    "pipeline.pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3] END estimator=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...), estimator__colsample_bytree=0.6000000000000001, estimator__eta=0.17333333333333334, estimator__gamma=1.0, estimator__max_depth=11, estimator__min_child_weight=20.0, estimator__n_estimators=45, estimator__reg_alpha=0.0, estimator__reg_lambda=10.0, estimator__subsample=0.325, feature_selector=VarianceThreshold(), preprocessor__categorical__imputer=SimpleImputer(strategy='most_frequent'), preprocessor__numerical__imputer=SimpleImputer(), preprocessor__numerical__scaler=MinMaxScaler(), preprocessor__text__tfidf=TfidfTransformer(norm='l1'), preprocessor__text__vectorizer=CountVectorizer(ngram_range=(2, 2),\n",
      "                stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
      "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
      "                            'no', 'una', 'su', 'al', 'lo', 'como', 'mas',\n",
      "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 'si',\n",
      "                            'porque', ...],\n",
      "                strip_accents='unicode'); Accuracy: (test=0.913) Weighted_F1: (test=0.912) total time= 3.7min\n",
      "[CV 2/3] END estimator=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...), estimator__colsample_bytree=0.6000000000000001, estimator__eta=0.17333333333333334, estimator__gamma=1.0, estimator__max_depth=11, estimator__min_child_weight=20.0, estimator__n_estimators=45, estimator__reg_alpha=0.0, estimator__reg_lambda=10.0, estimator__subsample=0.325, feature_selector=VarianceThreshold(), preprocessor__categorical__imputer=SimpleImputer(strategy='most_frequent'), preprocessor__numerical__imputer=SimpleImputer(), preprocessor__numerical__scaler=MinMaxScaler(), preprocessor__text__tfidf=TfidfTransformer(norm='l1'), preprocessor__text__vectorizer=CountVectorizer(ngram_range=(2, 2),\n",
      "                stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
      "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
      "                            'no', 'una', 'su', 'al', 'lo', 'como', 'mas',\n",
      "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 'si',\n",
      "                            'porque', ...],\n",
      "                strip_accents='unicode'); Accuracy: (test=0.914) Weighted_F1: (test=0.912) total time= 2.5min\n",
      "[CV 3/3] END estimator=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...), estimator__colsample_bytree=0.6000000000000001, estimator__eta=0.17333333333333334, estimator__gamma=1.0, estimator__max_depth=11, estimator__min_child_weight=20.0, estimator__n_estimators=45, estimator__reg_alpha=0.0, estimator__reg_lambda=10.0, estimator__subsample=0.325, feature_selector=VarianceThreshold(), preprocessor__categorical__imputer=SimpleImputer(strategy='most_frequent'), preprocessor__numerical__imputer=SimpleImputer(), preprocessor__numerical__scaler=MinMaxScaler(), preprocessor__text__tfidf=TfidfTransformer(norm='l1'), preprocessor__text__vectorizer=CountVectorizer(ngram_range=(2, 2),\n",
      "                stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
      "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
      "                            'no', 'una', 'su', 'al', 'lo', 'como', 'mas',\n",
      "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 'si',\n",
      "                            'porque', ...],\n",
      "                strip_accents='unicode'); Accuracy: (test=0.910) Weighted_F1: (test=0.909) total time= 2.6min\n",
      "[CV 1/3] END estimator=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...), estimator__colsample_bytree=0.8, estimator__eta=0.44555555555555554, estimator__gamma=0.5, estimator__max_depth=5, estimator__min_child_weight=5.375, estimator__n_estimators=23, estimator__reg_alpha=2.5, estimator__reg_lambda=0.0, estimator__subsample=0.1, feature_selector=SelectFromModel(estimator=ElasticNet()), preprocessor__categorical__imputer=SimpleImputer(strategy='most_frequent'), preprocessor__numerical__imputer=SimpleImputer(), preprocessor__numerical__scaler=RobustScaler(), preprocessor__text__tfidf=TfidfTransformer(norm='l1', sublinear_tf=True), preprocessor__text__vectorizer=CountVectorizer(ngram_range=(2, 3),\n",
      "                stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
      "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
      "                            'no', 'una', 'su', 'al', 'lo', 'como', 'mas',\n",
      "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 'si',\n",
      "                            'porque', ...],\n",
      "                strip_accents='unicode'); Accuracy: (test=0.798) Weighted_F1: (test=0.791) total time=19.2min\n",
      "[CV 2/3] END estimator=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...), estimator__colsample_bytree=0.8, estimator__eta=0.44555555555555554, estimator__gamma=0.5, estimator__max_depth=5, estimator__min_child_weight=5.375, estimator__n_estimators=23, estimator__reg_alpha=2.5, estimator__reg_lambda=0.0, estimator__subsample=0.1, feature_selector=SelectFromModel(estimator=ElasticNet()), preprocessor__categorical__imputer=SimpleImputer(strategy='most_frequent'), preprocessor__numerical__imputer=SimpleImputer(), preprocessor__numerical__scaler=RobustScaler(), preprocessor__text__tfidf=TfidfTransformer(norm='l1', sublinear_tf=True), preprocessor__text__vectorizer=CountVectorizer(ngram_range=(2, 3),\n",
      "                stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
      "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
      "                            'no', 'una', 'su', 'al', 'lo', 'como', 'mas',\n",
      "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 'si',\n",
      "                            'porque', ...],\n",
      "                strip_accents='unicode'); Accuracy: (test=0.785) Weighted_F1: (test=0.778) total time=19.6min\n",
      "[CV 3/3] END estimator=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...), estimator__colsample_bytree=0.8, estimator__eta=0.44555555555555554, estimator__gamma=0.5, estimator__max_depth=5, estimator__min_child_weight=5.375, estimator__n_estimators=23, estimator__reg_alpha=2.5, estimator__reg_lambda=0.0, estimator__subsample=0.1, feature_selector=SelectFromModel(estimator=ElasticNet()), preprocessor__categorical__imputer=SimpleImputer(strategy='most_frequent'), preprocessor__numerical__imputer=SimpleImputer(), preprocessor__numerical__scaler=RobustScaler(), preprocessor__text__tfidf=TfidfTransformer(norm='l1', sublinear_tf=True), preprocessor__text__vectorizer=CountVectorizer(ngram_range=(2, 3),\n",
      "                stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
      "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
      "                            'no', 'una', 'su', 'al', 'lo', 'como', 'mas',\n",
      "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 'si',\n",
      "                            'porque', ...],\n",
      "                strip_accents='unicode'); Accuracy: (test=0.793) Weighted_F1: (test=0.786) total time=18.8min\n",
      "[CV 1/3] END estimator=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...), estimator__colsample_bytree=0.8, estimator__eta=0.44555555555555554, estimator__gamma=0.0, estimator__max_depth=11, estimator__min_child_weight=15.125, estimator__n_estimators=200, estimator__reg_alpha=10.0, estimator__reg_lambda=10.0, estimator__subsample=0.55, feature_selector=SelectFromModel(estimator=Ridge()), preprocessor__categorical__imputer=SimpleImputer(strategy='most_frequent'), preprocessor__numerical__imputer=SimpleImputer(strategy='most_frequent'), preprocessor__numerical__scaler=RobustScaler(), preprocessor__text__tfidf=TfidfTransformer(), preprocessor__text__vectorizer=CountVectorizer(ngram_range=(1, 3),\n",
      "                stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
      "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
      "                            'no', 'una', 'su', 'al', 'lo', 'como', 'mas',\n",
      "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 'si',\n",
      "                            'porque', ...],\n",
      "                strip_accents='unicode'); Accuracy: (test=0.949) Weighted_F1: (test=0.948) total time=37.5min\n",
      "[CV 2/3] END estimator=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...), estimator__colsample_bytree=0.8, estimator__eta=0.44555555555555554, estimator__gamma=0.0, estimator__max_depth=11, estimator__min_child_weight=15.125, estimator__n_estimators=200, estimator__reg_alpha=10.0, estimator__reg_lambda=10.0, estimator__subsample=0.55, feature_selector=SelectFromModel(estimator=Ridge()), preprocessor__categorical__imputer=SimpleImputer(strategy='most_frequent'), preprocessor__numerical__imputer=SimpleImputer(strategy='most_frequent'), preprocessor__numerical__scaler=RobustScaler(), preprocessor__text__tfidf=TfidfTransformer(), preprocessor__text__vectorizer=CountVectorizer(ngram_range=(1, 3),\n",
      "                stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
      "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
      "                            'no', 'una', 'su', 'al', 'lo', 'como', 'mas',\n",
      "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 'si',\n",
      "                            'porque', ...],\n",
      "                strip_accents='unicode'); Accuracy: (test=0.949) Weighted_F1: (test=0.948) total time=27.8min\n",
      "[CV 3/3] END estimator=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...), estimator__colsample_bytree=0.8, estimator__eta=0.44555555555555554, estimator__gamma=0.0, estimator__max_depth=11, estimator__min_child_weight=15.125, estimator__n_estimators=200, estimator__reg_alpha=10.0, estimator__reg_lambda=10.0, estimator__subsample=0.55, feature_selector=SelectFromModel(estimator=Ridge()), preprocessor__categorical__imputer=SimpleImputer(strategy='most_frequent'), preprocessor__numerical__imputer=SimpleImputer(strategy='most_frequent'), preprocessor__numerical__scaler=RobustScaler(), preprocessor__text__tfidf=TfidfTransformer(), preprocessor__text__vectorizer=CountVectorizer(ngram_range=(1, 3),\n",
      "                stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
      "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
      "                            'no', 'una', 'su', 'al', 'lo', 'como', 'mas',\n",
      "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 'si',\n",
      "                            'porque', ...],\n",
      "                strip_accents='unicode'); Accuracy: (test=0.957) Weighted_F1: (test=0.956) total time=26.9min\n",
      "[CV 1/3] END estimator=XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, gamma=None,\n",
      "              gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...), estimator__colsample_bytree=0.8, estimator__eta=0.01, estimator__gamma=0.0, estimator__max_depth=11, estimator__min_child_weight=5.375, estimator__n_estimators=155, estimator__reg_alpha=2.5, estimator__reg_lambda=2.5, estimator__subsample=0.325, feature_selector=VarianceThreshold(), preprocessor__categorical__imputer=SimpleImputer(strategy='most_frequent'), preprocessor__numerical__imputer=KNNImputer(), preprocessor__numerical__scaler=MinMaxScaler(), preprocessor__text__tfidf=TfidfTransformer(sublinear_tf=True), preprocessor__text__vectorizer=CountVectorizer(ngram_range=(2, 3),\n",
      "                stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
      "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
      "                            'no', 'una', 'su', 'al', 'lo', 'como', 'mas',\n",
      "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 'si',\n",
      "                            'porque', ...],\n",
      "                strip_accents='unicode'); Accuracy: (test=0.904) Weighted_F1: (test=0.901) total time=36.6min\n"
     ]
    }
   ],
   "source": [
    "scoring = {\n",
    "    \"Accuracy\": \"balanced_accuracy\",\n",
    "    \"Weighted_F1\": make_scorer(f1_score, average=\"weighted\"),\n",
    "    # 'roc_auc':make_scorer(roc_auc_score, average='weighted'),\n",
    "}\n",
    "if not fit_pipeline and not save_path.is_file():\n",
    "    print(f\"Could not find saved model in {save_path.resolve()}, retraining pipeline\")\n",
    "    fit_pipeline = True\n",
    "\n",
    "if fit_pipeline:\n",
    "    best_model = pipeline.find_best_model(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=cv,\n",
    "        n_iter=n_iter,\n",
    "        n_jobs=n_jobs,\n",
    "        scoring=scoring,\n",
    "        random_state=random_state,\n",
    "        refit=\"Weighted_F1\",\n",
    "        verbose=5,\n",
    "        # error_score='raise',\n",
    "    )\n",
    "    with pd.option_context(\"display.max_columns\", None):\n",
    "        display(pipeline.cv_results.sort_values(by=[\"rank_test_Weighted_F1\"]).head(30))\n",
    "else:\n",
    "    best_model = load(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_columns\", None):\n",
    "    display(pipeline.cv_results.sort_values(by=[\"rank_test_Weighted_F1\"]).head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.cv_results.sort_values(by=[\"rank_test_Weighted_F1\"])[\n",
    "    [\n",
    "        \"rank_test_Weighted_F1\",\n",
    "        \"rank_test_Accuracy\",\n",
    "        \"mean_fit_time\",\n",
    "        \"param_estimator\",\n",
    "        \"param_preprocessor__text__vectorizer\",\n",
    "        \"param_preprocessor__text__tfidf\",\n",
    "        \"param_preprocessor__numerical__scaler\",\n",
    "        \"param_preprocessor__numerical__imputer\",\n",
    "        \"param_feature_selector\",\n",
    "        \"mean_test_Accuracy\",\n",
    "        \"mean_test_Weighted_F1\",\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.cv_results.sort_values(by=[\"rank_test_Weighted_F1\"])[\n",
    "    [\n",
    "        \"rank_test_Weighted_F1\",\n",
    "        \"rank_test_Accuracy\",\n",
    "        \"mean_fit_time\",\n",
    "        \"param_estimator\",\n",
    "        \"param_preprocessor__text__vectorizer\",\n",
    "        \"param_preprocessor__text__tfidf\",\n",
    "        \"param_preprocessor__numerical__scaler\",\n",
    "        \"param_preprocessor__numerical__imputer\",\n",
    "        \"param_feature_selector\",\n",
    "        \"mean_test_Accuracy\",\n",
    "        \"mean_test_Weighted_F1\",\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model using a sample-weighting mechanism to try to compensate for the dataset imbalance\n",
    "if retrain_with_class_weight:\n",
    "    sample_weights = compute_sample_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        y=y_train,\n",
    "    )\n",
    "\n",
    "    best_model.fit(X_train, y_train, estimator__sample_weight=sample_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pipeline.score(X_test, y_test)\n",
    "score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_decoded = label_encoder.inverse_transform(best_model[\"estimator\"].classes_)\n",
    "\n",
    "font = {\"family\": \"normal\", \"weight\": \"bold\", \"size\": 14}\n",
    "\n",
    "plt.rc(\"font\", **font)\n",
    "\n",
    "with sns.axes_style(\"dark\"):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test,\n",
    "        best_model.predict(X_test),\n",
    "        display_labels=labels_decoded,\n",
    "        normalize=\"true\",\n",
    "        ax=ax,\n",
    "        cmap=\"PuBu\",\n",
    "        values_format=\".2f\",\n",
    "    )\n",
    "    _ = ax.set_title(f\"Confusion matrix, normalized on True labels\")\n",
    "    plt.show()\n",
    "\n",
    "    fig_savepath.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(str(fig_savepath / \"confusion_matrix\"), dpi=fig.dpi, transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.loc[0, \"classification_report\"].round(2).rename(\n",
    "    index={\n",
    "        str(class_label): label\n",
    "        for class_label, label in zip(best_model[\"estimator\"].classes_, labels_decoded)\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = best_model.predict_proba(X_test)\n",
    "\n",
    "n_classes = len(best_model[\"estimator\"].classes_)\n",
    "\n",
    "y_test_bin = label_binarize(y_test, classes=best_model[\"estimator\"].classes_)\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fig = go.Figure()\n",
    "for c in range(n_classes):\n",
    "    label = f\"{label_encoder.inverse_transform([np.array(c)])[0]}, area ROC: {round(roc_auc[c], 2)}\"\n",
    "    fig.add_traces(go.Scatter(x=fpr[c], y=tpr[c], mode=\"lines\", name=label))\n",
    "fig.add_traces(\n",
    "    go.Scatter(x=fpr[c], y=fpr[c], name=\"Random\", line=dict(color=\"black\", dash=\"dash\"))\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"False Positive Rate\")\n",
    "fig.update_yaxes(title_text=\"True Positive Rate\")\n",
    "fig.show(rendered=\"png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Characterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the fmap file needed to correctly give names to the features in the decision tree plot\n",
    "fmap = pd.DataFrame(\n",
    "    best_model.named_steps[\"feature_selector\"].get_feature_names_out(\n",
    "        best_model.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "    ),\n",
    "    columns=[\"feature\"],\n",
    ")\n",
    "# Feature type q is quantitative, feature type i is binary\n",
    "fmap[\"feature_type\"] = \"i\"\n",
    "fmap.loc[fmap.feature.str.contains(\"numerical__\"), \"feature_type\"] = \"q\"\n",
    "fmap[\"feature\"] = fmap.feature.str.replace(\" \", \"_\")\n",
    "fmap[\"feature\"] = fmap.feature.str.replace(\"numerical__\", \"\")\n",
    "fmap[\"feature\"] = fmap.feature.str.replace(\"categorical__\", \"\")\n",
    "fmap_save_path = save_path.parent / \"feature_map.txt\"\n",
    "fmap.to_csv(str(fmap_save_path), sep=\"\\t\", header=False)\n",
    "fmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20), dpi=800)\n",
    "plot_tree(\n",
    "    best_model.named_steps[\"estimator\"],\n",
    "    num_trees=0,\n",
    "    ax=ax,\n",
    "    fmap=str(fmap_save_path),\n",
    "    rankdir=\"LR\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_dict = dict(\n",
    "    zip(\n",
    "        best_model.named_steps[\"feature_selector\"].get_feature_names_out(\n",
    "            best_model.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "        ),\n",
    "        best_model.named_steps[\"estimator\"].feature_importances_,\n",
    "    )\n",
    ")\n",
    "\n",
    "for feature in categorical_features:\n",
    "    feature_total_importance = 0\n",
    "    for key, value in list(importances_dict.items()):\n",
    "        if \"categorical__\" in key:\n",
    "            if key[key.find(\"__\") + 2 :].startswith(feature):\n",
    "                feature_total_importance += importances_dict[key]\n",
    "                del importances_dict[key]\n",
    "    importances_dict[f\"categorical__{feature}\"] = feature_total_importance\n",
    "\n",
    "importance_data = sorted(\n",
    "    list(importances_dict.items()), key=lambda tpl: tpl[1], reverse=True\n",
    ")\n",
    "importance_data = importance_data[0:20]\n",
    "\n",
    "feat_type = [val[0][: val[0].find(\"__\")] for val in importance_data]\n",
    "\n",
    "\n",
    "fig = px.bar(\n",
    "    x=[val[0].split(\"__\")[1] for val in importance_data],\n",
    "    y=[val[1] for val in importance_data],\n",
    "    color=feat_type,\n",
    "    text_auto=True,\n",
    ")\n",
    "fig.update_layout(barmode=\"stack\", xaxis={\"categoryorder\": \"total descending\"})\n",
    "fig.update_xaxes(title_text=\"Feature\")\n",
    "fig.update_yaxes(title_text=\"Mean decrease in impurity\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model test\n",
    "print(\n",
    "    f\"Predicted :{label_encoder.inverse_transform(best_model.predict(X_test.iloc[905].to_frame().T))}, real: {label_encoder.inverse_transform([y_test[905]])}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the best performing model and pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "dump(best_model, str(save_path))\n",
    "dump(score, str(save_path.parent / f\"best_model_score{save_path.suffix}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and save the full prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First retrain the model\n",
    "if refit_model:\n",
    "    best_model.fit(sampler.fit_resample(X, y_labels))\n",
    "\n",
    "# Create the full pipeline, with the preprocessing function and label encoder/decoder\n",
    "prediction_pipeline = PredictionPipeline(\n",
    "    estimator=best_model,\n",
    "    preprocessing_fn=clean_and_preprocess_datasets,\n",
    "    label_encoder=label_encoder,\n",
    ")\n",
    "\n",
    "# Save it\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "dump(\n",
    "    prediction_pipeline,\n",
    "    str(prediction_pipeline_savepath),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_pipeline.predict(X_test, preprocess_data=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_pipeline.predict(\n",
    "    X={\n",
    "        \"df_sociodemograficos\": pd.read_csv(\"data/sociodemografico.csv\", sep=\";\"),\n",
    "        \"df_laboratorios\": pd.read_csv(\"data/laboratorios.csv\", sep=\";\"),\n",
    "        \"df_notas\": pd.read_csv(\"data/notas.csv\", sep=\";\"),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_and_preprocess_datasets(\n",
    "    {\n",
    "        \"df_sociodemograficos\": pd.read_csv(\"data/sociodemografico.csv\", sep=\";\"),\n",
    "        \"df_laboratorios\": pd.read_csv(\"data/laboratorios.csv\", sep=\";\"),\n",
    "        \"df_notas\": pd.read_csv(\"data/notas.csv\", sep=\";\"),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing_utils import preprocess_json\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"utils/sample_example.json\") as in_file:\n",
    "    sample_data = json.load(in_file)\n",
    "sample_data\n",
    "\n",
    "preprocess_json(sample_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_pipeline.predict(X=preprocess_json(sample_data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DS4A_Project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32aecc324f8df26357122ac4599014cc1f704524e77889bea473ffbdc8bf89d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
