{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-28 00:51:47.244992: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-28 00:51:47.245033: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package stopwords to /home/juanma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/juanma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from IPython.display import display\n",
    "from joblib import dump\n",
    "from sklearn import set_config\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay, classification_report,\n",
    "                             f1_score, make_scorer)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from xgboost import XGBClassifier, plot_tree\n",
    "\n",
    "from ml_model import PipelineManager, PredictionPipeline\n",
    "from utils.GPU_models import KerasClassifierModel, gpu_model_hub\n",
    "from utils.preprocessing_utils import (clean_and_preprocess_datasets,\n",
    "                                       clean_labs, clean_notas,\n",
    "                                       clean_sociodemograficos, merge_classes,\n",
    "                                       merge_labs_notas,\n",
    "                                       word_count_feat_engineering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display=\"diagram\")\n",
    "sns.set_style(\"darkgrid\")\n",
    "as_dual_class=False\n",
    "target_feature = 'Código'\n",
    "text_feature = 'Plan'\n",
    "retrain_with_class_weight=False\n",
    "add_gpu_prediction = False\n",
    "consolidate_classes = False\n",
    "cv = 5\n",
    "n_iter = 16\n",
    "n_jobs = -2\n",
    "\n",
    "# False, 'oversample', or 'undersample'\n",
    "balance_classes = 'oversample'\n",
    "save_path = Path('data') / 'output' / 'best_model.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notas = pd.read_csv('data/notas.csv', sep=';')\n",
    "df_laboratorios = pd.read_csv('data/laboratorios.csv', sep=';')\n",
    "df_sociodemografico = pd.read_csv('data/sociodemografico.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDRecord</th>\n",
       "      <th>Código</th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Tipo</th>\n",
       "      <th>Plan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44600</td>\n",
       "      <td>A539</td>\n",
       "      <td>SIFILIS, NO ESPECIFICADA</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>- ORDENO TAR ABC +3TC +ATV/r  - PROFILAXIS NO ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45038</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>- TAF/FTC/EVG/C MIPRES POR 2 MESES 20200602158...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40391</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>usuaria la cual se ve pertinente seguimiento d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106350</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>1. Se formula TAR (TDF/FTC+EFV)  2. S/S Paracl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105840</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>EDUCACIÓN  Se brinda retroalimentación con rel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  IDRecord Código                                             Nombre  \\\n",
       "0    44600   A539                           SIFILIS, NO ESPECIFICADA   \n",
       "1    45038   A530  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "2    40391   A530  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "3   106350   A530  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "4   105840   A530  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "\n",
       "                  Tipo                                               Plan  \n",
       "0  Confirmado Repetido  - ORDENO TAR ABC +3TC +ATV/r  - PROFILAXIS NO ...  \n",
       "1  Confirmado Repetido  - TAF/FTC/EVG/C MIPRES POR 2 MESES 20200602158...  \n",
       "2  Confirmado Repetido  usuaria la cual se ve pertinente seguimiento d...  \n",
       "3  Confirmado Repetido  1. Se formula TAR (TDF/FTC+EFV)  2. S/S Paracl...  \n",
       "4  Confirmado Repetido  EDUCACIÓN  Se brinda retroalimentación con rel...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDRecord</th>\n",
       "      <th>Codigo</th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Valor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95627</td>\n",
       "      <td>902045</td>\n",
       "      <td>TIEMPO DE PROTROMBINA (PT)</td>\n",
       "      <td>22/02/2022 18:43</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125572</td>\n",
       "      <td>902045</td>\n",
       "      <td>TIEMPO DE PROTROMBINA (PT)</td>\n",
       "      <td>17/02/2022 13:41</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55788</td>\n",
       "      <td>902045</td>\n",
       "      <td>TIEMPO DE PROTROMBINA (PT)</td>\n",
       "      <td>22/06/2021 12:50</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113766</td>\n",
       "      <td>902045</td>\n",
       "      <td>TIEMPO DE PROTROMBINA (PT)</td>\n",
       "      <td>5/08/2021 12:11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44596</td>\n",
       "      <td>902045</td>\n",
       "      <td>TIEMPO DE PROTROMBINA (PT)</td>\n",
       "      <td>5/08/2021 13:15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDRecord  Codigo                      Nombre             Fecha Valor\n",
       "0     95627  902045  TIEMPO DE PROTROMBINA (PT)  22/02/2022 18:43   NaN\n",
       "1    125572  902045  TIEMPO DE PROTROMBINA (PT)  17/02/2022 13:41   NaN\n",
       "2     55788  902045  TIEMPO DE PROTROMBINA (PT)  22/06/2021 12:50  1.05\n",
       "3    113766  902045  TIEMPO DE PROTROMBINA (PT)   5/08/2021 12:11   NaN\n",
       "4     44596  902045  TIEMPO DE PROTROMBINA (PT)   5/08/2021 13:15   NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_laboratorios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDRecord</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Genero</th>\n",
       "      <th>GrupoEtnico</th>\n",
       "      <th>AreaResidencial</th>\n",
       "      <th>EstadoCivil</th>\n",
       "      <th>TSangre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Separado</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>292</td>\n",
       "      <td>84</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Casado</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>307</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>No reportado</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>325</td>\n",
       "      <td>94</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Rural</td>\n",
       "      <td>Viudo/a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDRecord  Edad  Genero                GrupoEtnico AreaResidencial  \\\n",
       "0         5    39   Mujer                    Mestizo     Zona Urbana   \n",
       "1       292    84  Hombre  Ninguno de los anteriores     Zona Urbana   \n",
       "2       300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "3       307    88  Hombre  Ninguno de los anteriores     Zona Urbana   \n",
       "4       325    94  Hombre  Ninguno de los anteriores      Zona Rural   \n",
       "\n",
       "    EstadoCivil TSangre  \n",
       "0      Separado     NaN  \n",
       "1        Casado     NaN  \n",
       "2       Soltero      O+  \n",
       "3  No reportado     NaN  \n",
       "4       Viudo/a     NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sociodemografico.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDRecord</th>\n",
       "      <th>Código</th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Tipo</th>\n",
       "      <th>Plan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44600</td>\n",
       "      <td>A539</td>\n",
       "      <td>SIFILIS, NO ESPECIFICADA</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>- ORDENO TAR ABC +3TC +ATV/r  - PROFILAXIS NO ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45038</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>- TAF/FTC/EVG/C MIPRES POR 2 MESES 20200602158...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40391</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>usuaria la cual se ve pertinente seguimiento d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106350</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>1. Se formula TAR (TDF/FTC+EFV)  2. S/S Paracl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105840</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>EDUCACIÓN  Se brinda retroalimentación con rel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  IDRecord Código                                             Nombre  \\\n",
       "0    44600   A539                           SIFILIS, NO ESPECIFICADA   \n",
       "1    45038   A530  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "2    40391   A530  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "3   106350   A530  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "4   105840   A530  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "\n",
       "                  Tipo                                               Plan  \n",
       "0  Confirmado Repetido  - ORDENO TAR ABC +3TC +ATV/r  - PROFILAXIS NO ...  \n",
       "1  Confirmado Repetido  - TAF/FTC/EVG/C MIPRES POR 2 MESES 20200602158...  \n",
       "2  Confirmado Repetido  usuaria la cual se ve pertinente seguimiento d...  \n",
       "3  Confirmado Repetido  1. Se formula TAR (TDF/FTC+EFV)  2. S/S Paracl...  \n",
       "4  Confirmado Repetido  EDUCACIÓN  Se brinda retroalimentación con rel...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sociodemografico = clean_sociodemograficos(df_sociodemografico)\n",
    "df_laboratorios = clean_labs(df_laboratorios)\n",
    "df_notas = clean_notas(df_notas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the sociodemographic data with the medical notes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDRecord</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Genero</th>\n",
       "      <th>GrupoEtnico</th>\n",
       "      <th>AreaResidencial</th>\n",
       "      <th>EstadoCivil</th>\n",
       "      <th>TSangre</th>\n",
       "      <th>Código</th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Tipo</th>\n",
       "      <th>Plan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Separado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E109</td>\n",
       "      <td>DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>PACIENTE CONTINUA PROGRAMA NEFROPROTECCION   S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>292</td>\n",
       "      <td>84</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Casado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>CONTINUA PROGRAMA CRONICOS.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>1- CONTINUAR PAD 2 - RECOMENDACIONES DIETA HIP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>E109</td>\n",
       "      <td>DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>1- CONTINUAR PAD 2 - RECOMENDACIONES DIETA HIP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Confirmado Nuevo</td>\n",
       "      <td>1- CONTINUAR PAD 2 - RECOMENDACIONES DIETA HIP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140167</th>\n",
       "      <td>205218</td>\n",
       "      <td>28</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A539</td>\n",
       "      <td>SIFILIS, NO ESPECIFICADA</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>explica acerca programa, recomienda adherencia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140168</th>\n",
       "      <td>205227</td>\n",
       "      <td>24</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>A530</td>\n",
       "      <td>SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>Elaboracion duelo frente diagnostico.   Reforz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140169</th>\n",
       "      <td>205253</td>\n",
       "      <td>84</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Casado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E109</td>\n",
       "      <td>DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>FUROATO MOMETASONA 1 SPRY NASAL CADA FOSA NASA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140170</th>\n",
       "      <td>205577</td>\n",
       "      <td>62</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Desconocido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Impresión Diagnóstica</td>\n",
       "      <td>CONTROL MEICO MES-INFECTOLOGIA  VALORACIOJN IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140171</th>\n",
       "      <td>206307</td>\n",
       "      <td>57</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Desconocido</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E149</td>\n",
       "      <td>DIABETES MELLITUS, NO ESPECIFICADA SIN MENCION...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>CONTINUA SEGUIMIENTO PROGRAMA NEFROPROTECCION ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140172 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        IDRecord  Edad  Genero                GrupoEtnico AreaResidencial  \\\n",
       "0              5    39   Mujer                    Mestizo     Zona Urbana   \n",
       "1            292    84  Hombre  Ninguno de los anteriores     Zona Urbana   \n",
       "2            300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "3            300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "4            300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "...          ...   ...     ...                        ...             ...   \n",
       "140167    205218    28  Hombre  Ninguno de los anteriores     Zona Urbana   \n",
       "140168    205227    24  Hombre  Ninguno de los anteriores     Zona Urbana   \n",
       "140169    205253    84  Hombre                    Mestizo     Zona Urbana   \n",
       "140170    205577    62  Hombre                    Mestizo     Zona Urbana   \n",
       "140171    206307    57  Hombre                    Mestizo     Zona Urbana   \n",
       "\n",
       "        EstadoCivil TSangre Código  \\\n",
       "0          Separado     NaN   E109   \n",
       "1            Casado     NaN   E119   \n",
       "2           Soltero      O+   E119   \n",
       "3           Soltero      O+   E109   \n",
       "4           Soltero      O+   E119   \n",
       "...             ...     ...    ...   \n",
       "140167          NaN     NaN   A539   \n",
       "140168      Soltero      O+   A530   \n",
       "140169       Casado     NaN   E109   \n",
       "140170  Desconocido     NaN   E119   \n",
       "140171  Desconocido     NaN   E149   \n",
       "\n",
       "                                                   Nombre  \\\n",
       "0       DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...   \n",
       "1       DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...   \n",
       "2       DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...   \n",
       "3       DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...   \n",
       "4       DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...   \n",
       "...                                                   ...   \n",
       "140167                           SIFILIS, NO ESPECIFICADA   \n",
       "140168  SIFILIS LATENTE, NO ESPECIFICADA COMO PRECOZ O...   \n",
       "140169  DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...   \n",
       "140170  DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...   \n",
       "140171  DIABETES MELLITUS, NO ESPECIFICADA SIN MENCION...   \n",
       "\n",
       "                         Tipo  \\\n",
       "0         Confirmado Repetido   \n",
       "1         Confirmado Repetido   \n",
       "2         Confirmado Repetido   \n",
       "3         Confirmado Repetido   \n",
       "4            Confirmado Nuevo   \n",
       "...                       ...   \n",
       "140167    Confirmado Repetido   \n",
       "140168    Confirmado Repetido   \n",
       "140169    Confirmado Repetido   \n",
       "140170  Impresión Diagnóstica   \n",
       "140171    Confirmado Repetido   \n",
       "\n",
       "                                                     Plan  \n",
       "0       PACIENTE CONTINUA PROGRAMA NEFROPROTECCION   S...  \n",
       "1                            CONTINUA PROGRAMA CRONICOS.   \n",
       "2       1- CONTINUAR PAD 2 - RECOMENDACIONES DIETA HIP...  \n",
       "3       1- CONTINUAR PAD 2 - RECOMENDACIONES DIETA HIP...  \n",
       "4       1- CONTINUAR PAD 2 - RECOMENDACIONES DIETA HIP...  \n",
       "...                                                   ...  \n",
       "140167  explica acerca programa, recomienda adherencia...  \n",
       "140168  Elaboracion duelo frente diagnostico.   Reforz...  \n",
       "140169  FUROATO MOMETASONA 1 SPRY NASAL CADA FOSA NASA...  \n",
       "140170  CONTROL MEICO MES-INFECTOLOGIA  VALORACIOJN IN...  \n",
       "140171  CONTINUA SEGUIMIENTO PROGRAMA NEFROPROTECCION ...  \n",
       "\n",
       "[140172 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge = df_sociodemografico.merge(df_notas, how='inner', on='IDRecord')\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDRecord</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Genero</th>\n",
       "      <th>GrupoEtnico</th>\n",
       "      <th>AreaResidencial</th>\n",
       "      <th>EstadoCivil</th>\n",
       "      <th>TSangre</th>\n",
       "      <th>Código</th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Tipo</th>\n",
       "      <th>Plan</th>\n",
       "      <th>acido</th>\n",
       "      <th>antibio</th>\n",
       "      <th>asintoma</th>\n",
       "      <th>cabeza</th>\n",
       "      <th>diabet</th>\n",
       "      <th>diet</th>\n",
       "      <th>gluco</th>\n",
       "      <th>hepat</th>\n",
       "      <th>insulin</th>\n",
       "      <th>keto</th>\n",
       "      <th>penici</th>\n",
       "      <th>preservativo</th>\n",
       "      <th>rpr</th>\n",
       "      <th>sable</th>\n",
       "      <th>serolo</th>\n",
       "      <th>sifili</th>\n",
       "      <th>test_reloj_orden</th>\n",
       "      <th>vih</th>\n",
       "      <th>top_lab_code</th>\n",
       "      <th>top_lab_avg_value</th>\n",
       "      <th>top_lab_max_value</th>\n",
       "      <th>top_lab_count</th>\n",
       "      <th>total_lab_count</th>\n",
       "      <th>first_lab_date</th>\n",
       "      <th>last_lab_date</th>\n",
       "      <th>date_diff_first_last</th>\n",
       "      <th>date_diff_mean</th>\n",
       "      <th>date_diff_max</th>\n",
       "      <th>hepatitis_count</th>\n",
       "      <th>hepatitis_max</th>\n",
       "      <th>glucosa_count</th>\n",
       "      <th>glucosa_max</th>\n",
       "      <th>linfocitos_count</th>\n",
       "      <th>linfocitos_max</th>\n",
       "      <th>vih_count</th>\n",
       "      <th>vih_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>Mujer</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Separado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E109</td>\n",
       "      <td>DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>PACIENTE CONTINUA PROGRAMA NEFROPROTECCION   S...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>902213</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.609114e+18</td>\n",
       "      <td>1.609114e+18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>292</td>\n",
       "      <td>84</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Ninguno de los anteriores</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Casado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>CONTINUA PROGRAMA CRONICOS.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>1- CONTINUAR PAD 2 - RECOMENDACIONES DIETA HIP...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>E109</td>\n",
       "      <td>DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...</td>\n",
       "      <td>Confirmado Repetido</td>\n",
       "      <td>1- CONTINUAR PAD 2 - RECOMENDACIONES DIETA HIP...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>88</td>\n",
       "      <td>Hombre</td>\n",
       "      <td>Mestizo</td>\n",
       "      <td>Zona Urbana</td>\n",
       "      <td>Soltero</td>\n",
       "      <td>O+</td>\n",
       "      <td>E119</td>\n",
       "      <td>DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...</td>\n",
       "      <td>Confirmado Nuevo</td>\n",
       "      <td>1- CONTINUAR PAD 2 - RECOMENDACIONES DIETA HIP...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDRecord  Edad  Genero                GrupoEtnico AreaResidencial  \\\n",
       "0         5    39   Mujer                    Mestizo     Zona Urbana   \n",
       "1       292    84  Hombre  Ninguno de los anteriores     Zona Urbana   \n",
       "2       300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "3       300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "4       300    88  Hombre                    Mestizo     Zona Urbana   \n",
       "\n",
       "  EstadoCivil TSangre Código  \\\n",
       "0    Separado     NaN   E109   \n",
       "1      Casado     NaN   E119   \n",
       "2     Soltero      O+   E119   \n",
       "3     Soltero      O+   E109   \n",
       "4     Soltero      O+   E119   \n",
       "\n",
       "                                              Nombre                 Tipo  \\\n",
       "0  DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...  Confirmado Repetido   \n",
       "1  DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...  Confirmado Repetido   \n",
       "2  DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...  Confirmado Repetido   \n",
       "3  DIABETES MELLITUSINSULINODEPENDIENTE SIN MENCI...  Confirmado Repetido   \n",
       "4  DIABETES MELLITUS NOINSULINODEPENDIENTE SIN ME...     Confirmado Nuevo   \n",
       "\n",
       "                                                Plan  acido  antibio  \\\n",
       "0  PACIENTE CONTINUA PROGRAMA NEFROPROTECCION   S...      0        0   \n",
       "1                       CONTINUA PROGRAMA CRONICOS.       0        0   \n",
       "2  1- CONTINUAR PAD 2 - RECOMENDACIONES DIETA HIP...      0        0   \n",
       "3  1- CONTINUAR PAD 2 - RECOMENDACIONES DIETA HIP...      0        0   \n",
       "4  1- CONTINUAR PAD 2 - RECOMENDACIONES DIETA HIP...      0        0   \n",
       "\n",
       "   asintoma  cabeza  diabet  diet  gluco  hepat  insulin  keto  penici  \\\n",
       "0         0       0       0     0      0      0        0     0       0   \n",
       "1         0       0       0     0      0      0        0     0       0   \n",
       "2         0       0       0     1      0      0        0     0       0   \n",
       "3         0       0       0     1      0      0        0     0       0   \n",
       "4         0       0       0     1      0      0        0     0       0   \n",
       "\n",
       "   preservativo  rpr  sable  serolo  sifili  test_reloj_orden  vih  \\\n",
       "0             0    0      0       0       0                 0    0   \n",
       "1             0    0      0       0       0                 0    0   \n",
       "2             0    0      0       0       0                 0    0   \n",
       "3             0    0      0       0       0                 0    0   \n",
       "4             0    0      0       0       0                 0    0   \n",
       "\n",
       "  top_lab_code  top_lab_avg_value  top_lab_max_value  top_lab_count  \\\n",
       "0       902213               10.0               10.0            1.0   \n",
       "1          NaN                NaN                NaN            NaN   \n",
       "2          NaN                NaN                NaN            NaN   \n",
       "3          NaN                NaN                NaN            NaN   \n",
       "4          NaN                NaN                NaN            NaN   \n",
       "\n",
       "   total_lab_count  first_lab_date  last_lab_date  date_diff_first_last  \\\n",
       "0              8.0    1.609114e+18   1.609114e+18                   0.0   \n",
       "1              NaN             NaN            NaN                   NaN   \n",
       "2              NaN             NaN            NaN                   NaN   \n",
       "3              NaN             NaN            NaN                   NaN   \n",
       "4              NaN             NaN            NaN                   NaN   \n",
       "\n",
       "   date_diff_mean  date_diff_max  hepatitis_count  hepatitis_max  \\\n",
       "0             0.0            0.0              0.0            0.0   \n",
       "1             NaN            NaN              NaN            NaN   \n",
       "2             NaN            NaN              NaN            NaN   \n",
       "3             NaN            NaN              NaN            NaN   \n",
       "4             NaN            NaN              NaN            NaN   \n",
       "\n",
       "   glucosa_count  glucosa_max  linfocitos_count  linfocitos_max  vih_count  \\\n",
       "0            0.0          0.0               0.0             0.0        0.0   \n",
       "1            NaN          NaN               NaN             NaN        NaN   \n",
       "2            NaN          NaN               NaN             NaN        NaN   \n",
       "3            NaN          NaN               NaN             NaN        NaN   \n",
       "4            NaN          NaN               NaN             NaN        NaN   \n",
       "\n",
       "   vih_max  \n",
       "0      0.0  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Consolidate the classes\n",
    "if consolidate_classes:\n",
    "    df_merge = merge_classes(df_merge)\n",
    "\n",
    "# Perform word count feature engineering\n",
    "df_merge = word_count_feat_engineering(df_merge)\n",
    "\n",
    "# Preprocess the lab data and merge it with the sociodemographic data\n",
    "df_merge = merge_labs_notas(df_laboratorios, df_merge)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_merge.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]),\n",
       " array([  977,    94,  2614,  1970, 60586, 47408,  6278, 17437,  2808]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_merge.drop(labels=[target_feature], axis=1)\n",
    "y = df_merge[target_feature]\n",
    "if as_dual_class:\n",
    "    y = y.str[:2]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_labels = label_encoder.fit_transform(y)\n",
    "\n",
    "np.unique(y_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]),\n",
       " array([  195,    19,   523,   394, 12117,  9481,  1256,  3487,   562]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_labels, train_size=0.2, random_state=42, stratify=y_labels)\n",
    "np.unique(y_train, return_counts=True) # Let's check the number of samples per label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]),\n",
       " array([19, 19, 19, 19, 19, 19, 19, 19, 19]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if balance_classes == 'oversample':\n",
    "    # Using a naive oversampling approach\n",
    "    sampler = RandomOverSampler(random_state=42)\n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "elif balance_classes == 'undersample':\n",
    "    # Using a naive oversampling approach\n",
    "    sampler = RandomUnderSampler(random_state=42)\n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "np.unique(y_train, return_counts=True) # Let's check the number of samples per label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further (optional) feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = \"nnlm-es-dim128\"\n",
    "embedding = \"nnlm-es-dim128-with-normalization\"\n",
    "# embedding = \"universal\"\n",
    "\n",
    "if add_gpu_prediction:\n",
    "    model_function = gpu_model_hub\n",
    "    clf = KerasClassifierModel(\n",
    "        build_fn=model_function,\n",
    "        class_number=len(df_notas[target_feature].unique()),\n",
    "        embedding = embedding,\n",
    "        epochs=400,\n",
    "        batch_size=400,\n",
    "        verbose=10,\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train[text_feature], y_train)\n",
    "    clf.plot_learning_curves('data/output/gpu_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_gpu_prediction:\n",
    "    y_pred = clf.predict(X_test[text_feature])\n",
    "    display(pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_gpu_prediction:\n",
    "    X_pred = clf.predict(df_merge[text_feature])\n",
    "    df_merge['GPU_prediction'] = X_pred\n",
    "    df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the numerical features that will be used in the model\n",
    "numerical_features = list(\n",
    "    set(\n",
    "        [\n",
    "            \"Edad\",\n",
    "            \"top_lab_avg_value\",\n",
    "            \"top_lab_max_value\",\n",
    "            \"top_lab_count\",\n",
    "            \"total_lab_count\",\n",
    "            \"date_diff_mean\",\n",
    "            \"date_diff_max\",\n",
    "            \"first_lab_date\",\t\n",
    "            \"last_lab_date\",\n",
    "            \"date_diff_first_last\",\n",
    "        ]\n",
    "        + list(df_merge.drop(columns=\"IDRecord\").select_dtypes(include=\"int64\").columns)\n",
    "        + ['glucosa_count', 'glucosa_max', 'hepatitis_count', 'hepatitis_max', 'linfocitos_count', 'linfocitos_max', 'vih_count', 'vih_max']\n",
    "    )\n",
    ")\n",
    "\n",
    "# Now select the categorical features\n",
    "categorical_features = [\n",
    "    \"Genero\",\n",
    "    \"GrupoEtnico\",\n",
    "    # \"AreaResidencial\",\n",
    "    \"EstadoCivil\",\n",
    "    # \"TSangre\",\n",
    "    # \"Tipo\",\n",
    "    \"top_lab_code\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(fill_value=&#x27;missing&#x27;,\n",
       "                                                                                 strategy=&#x27;constant&#x27;)),\n",
       "                                                                  (&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;Genero&#x27;, &#x27;GrupoEtnico&#x27;,\n",
       "                                                   &#x27;EstadoCivil&#x27;,\n",
       "                                                   &#x27;top_lab_code&#x27;]),\n",
       "                                                 (&#x27;numerical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  (&#x27;scaler&#x27;,\n",
       "                                                                   StandardScal...\n",
       "                                                                   CountVectorizer(stop_words=[&#x27;de&#x27;,\n",
       "                                                                                               &#x27;la&#x27;,\n",
       "                                                                                               &#x27;que&#x27;,\n",
       "                                                                                               &#x27;el&#x27;,\n",
       "                                                                                               &#x27;en&#x27;,\n",
       "                                                                                               &#x27;y&#x27;,\n",
       "                                                                                               &#x27;a&#x27;,\n",
       "                                                                                               &#x27;los&#x27;,\n",
       "                                                                                               &#x27;del&#x27;,\n",
       "                                                                                               &#x27;se&#x27;,\n",
       "                                                                                               &#x27;las&#x27;,\n",
       "                                                                                               &#x27;por&#x27;,\n",
       "                                                                                               &#x27;un&#x27;,\n",
       "                                                                                               &#x27;para&#x27;,\n",
       "                                                                                               &#x27;con&#x27;,\n",
       "                                                                                               &#x27;no&#x27;,\n",
       "                                                                                               &#x27;una&#x27;,\n",
       "                                                                                               &#x27;su&#x27;,\n",
       "                                                                                               &#x27;al&#x27;,\n",
       "                                                                                               &#x27;lo&#x27;,\n",
       "                                                                                               &#x27;como&#x27;,\n",
       "                                                                                               &#x27;más&#x27;,\n",
       "                                                                                               &#x27;pero&#x27;,\n",
       "                                                                                               &#x27;sus&#x27;,\n",
       "                                                                                               &#x27;le&#x27;,\n",
       "                                                                                               &#x27;ya&#x27;,\n",
       "                                                                                               &#x27;o&#x27;,\n",
       "                                                                                               &#x27;este&#x27;,\n",
       "                                                                                               &#x27;sí&#x27;,\n",
       "                                                                                               &#x27;porque&#x27;, ...],\n",
       "                                                                                   strip_accents=&#x27;unicode&#x27;)),\n",
       "                                                                  (&#x27;tfidf&#x27;,\n",
       "                                                                   TfidfTransformer())]),\n",
       "                                                  &#x27;Plan&#x27;)])),\n",
       "                (&#x27;feature_selector&#x27;,\n",
       "                 SelectFromModel(estimator=RandomForestRegressor())),\n",
       "                (&#x27;estimator&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(fill_value=&#x27;missing&#x27;,\n",
       "                                                                                 strategy=&#x27;constant&#x27;)),\n",
       "                                                                  (&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;Genero&#x27;, &#x27;GrupoEtnico&#x27;,\n",
       "                                                   &#x27;EstadoCivil&#x27;,\n",
       "                                                   &#x27;top_lab_code&#x27;]),\n",
       "                                                 (&#x27;numerical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  (&#x27;scaler&#x27;,\n",
       "                                                                   StandardScal...\n",
       "                                                                   CountVectorizer(stop_words=[&#x27;de&#x27;,\n",
       "                                                                                               &#x27;la&#x27;,\n",
       "                                                                                               &#x27;que&#x27;,\n",
       "                                                                                               &#x27;el&#x27;,\n",
       "                                                                                               &#x27;en&#x27;,\n",
       "                                                                                               &#x27;y&#x27;,\n",
       "                                                                                               &#x27;a&#x27;,\n",
       "                                                                                               &#x27;los&#x27;,\n",
       "                                                                                               &#x27;del&#x27;,\n",
       "                                                                                               &#x27;se&#x27;,\n",
       "                                                                                               &#x27;las&#x27;,\n",
       "                                                                                               &#x27;por&#x27;,\n",
       "                                                                                               &#x27;un&#x27;,\n",
       "                                                                                               &#x27;para&#x27;,\n",
       "                                                                                               &#x27;con&#x27;,\n",
       "                                                                                               &#x27;no&#x27;,\n",
       "                                                                                               &#x27;una&#x27;,\n",
       "                                                                                               &#x27;su&#x27;,\n",
       "                                                                                               &#x27;al&#x27;,\n",
       "                                                                                               &#x27;lo&#x27;,\n",
       "                                                                                               &#x27;como&#x27;,\n",
       "                                                                                               &#x27;más&#x27;,\n",
       "                                                                                               &#x27;pero&#x27;,\n",
       "                                                                                               &#x27;sus&#x27;,\n",
       "                                                                                               &#x27;le&#x27;,\n",
       "                                                                                               &#x27;ya&#x27;,\n",
       "                                                                                               &#x27;o&#x27;,\n",
       "                                                                                               &#x27;este&#x27;,\n",
       "                                                                                               &#x27;sí&#x27;,\n",
       "                                                                                               &#x27;porque&#x27;, ...],\n",
       "                                                                                   strip_accents=&#x27;unicode&#x27;)),\n",
       "                                                                  (&#x27;tfidf&#x27;,\n",
       "                                                                   TfidfTransformer())]),\n",
       "                                                  &#x27;Plan&#x27;)])),\n",
       "                (&#x27;feature_selector&#x27;,\n",
       "                 SelectFromModel(estimator=RandomForestRegressor())),\n",
       "                (&#x27;estimator&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(fill_value=&#x27;missing&#x27;,\n",
       "                                                                strategy=&#x27;constant&#x27;)),\n",
       "                                                 (&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                 [&#x27;Genero&#x27;, &#x27;GrupoEtnico&#x27;, &#x27;EstadoCivil&#x27;,\n",
       "                                  &#x27;top_lab_code&#x27;]),\n",
       "                                (&#x27;numerical&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer()),\n",
       "                                                 (&#x27;scaler&#x27;, StandardScaler())]),\n",
       "                                 [&#x27;top_lab_avg_value&#x27;, &#x27;la...\n",
       "                                  &#x27;vih_count&#x27;, &#x27;date_diff_first_last&#x27;,\n",
       "                                  &#x27;linfocitos_count&#x27;, ...]),\n",
       "                                (&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                  CountVectorizer(stop_words=[&#x27;de&#x27;,\n",
       "                                                                              &#x27;la&#x27;,\n",
       "                                                                              &#x27;que&#x27;,\n",
       "                                                                              &#x27;el&#x27;,\n",
       "                                                                              &#x27;en&#x27;,\n",
       "                                                                              &#x27;y&#x27;,\n",
       "                                                                              &#x27;a&#x27;,\n",
       "                                                                              &#x27;los&#x27;,\n",
       "                                                                              &#x27;del&#x27;,\n",
       "                                                                              &#x27;se&#x27;,\n",
       "                                                                              &#x27;las&#x27;,\n",
       "                                                                              &#x27;por&#x27;,\n",
       "                                                                              &#x27;un&#x27;,\n",
       "                                                                              &#x27;para&#x27;,\n",
       "                                                                              &#x27;con&#x27;,\n",
       "                                                                              &#x27;no&#x27;,\n",
       "                                                                              &#x27;una&#x27;,\n",
       "                                                                              &#x27;su&#x27;,\n",
       "                                                                              &#x27;al&#x27;,\n",
       "                                                                              &#x27;lo&#x27;,\n",
       "                                                                              &#x27;como&#x27;,\n",
       "                                                                              &#x27;más&#x27;,\n",
       "                                                                              &#x27;pero&#x27;,\n",
       "                                                                              &#x27;sus&#x27;,\n",
       "                                                                              &#x27;le&#x27;,\n",
       "                                                                              &#x27;ya&#x27;,\n",
       "                                                                              &#x27;o&#x27;,\n",
       "                                                                              &#x27;este&#x27;,\n",
       "                                                                              &#x27;sí&#x27;,\n",
       "                                                                              &#x27;porque&#x27;, ...],\n",
       "                                                                  strip_accents=&#x27;unicode&#x27;)),\n",
       "                                                 (&#x27;tfidf&#x27;,\n",
       "                                                  TfidfTransformer())]),\n",
       "                                 &#x27;Plan&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Genero&#x27;, &#x27;GrupoEtnico&#x27;, &#x27;EstadoCivil&#x27;, &#x27;top_lab_code&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(fill_value=&#x27;missing&#x27;, strategy=&#x27;constant&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numerical</label><div class=\"sk-toggleable__content\"><pre>[&#x27;top_lab_avg_value&#x27;, &#x27;last_lab_date&#x27;, &#x27;sable&#x27;, &#x27;date_diff_mean&#x27;, &#x27;sifili&#x27;, &#x27;first_lab_date&#x27;, &#x27;asintoma&#x27;, &#x27;test_reloj_orden&#x27;, &#x27;antibio&#x27;, &#x27;hepatitis_count&#x27;, &#x27;insulin&#x27;, &#x27;linfocitos_max&#x27;, &#x27;glucosa_max&#x27;, &#x27;date_diff_max&#x27;, &#x27;cabeza&#x27;, &#x27;rpr&#x27;, &#x27;diabet&#x27;, &#x27;top_lab_max_value&#x27;, &#x27;acido&#x27;, &#x27;Edad&#x27;, &#x27;top_lab_count&#x27;, &#x27;glucosa_count&#x27;, &#x27;keto&#x27;, &#x27;vih_max&#x27;, &#x27;serolo&#x27;, &#x27;preservativo&#x27;, &#x27;vih&#x27;, &#x27;vih_count&#x27;, &#x27;date_diff_first_last&#x27;, &#x27;linfocitos_count&#x27;, &#x27;gluco&#x27;, &#x27;penici&#x27;, &#x27;diet&#x27;, &#x27;hepat&#x27;, &#x27;total_lab_count&#x27;, &#x27;hepatitis_max&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>Plan</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=[&#x27;de&#x27;, &#x27;la&#x27;, &#x27;que&#x27;, &#x27;el&#x27;, &#x27;en&#x27;, &#x27;y&#x27;, &#x27;a&#x27;, &#x27;los&#x27;,\n",
       "                            &#x27;del&#x27;, &#x27;se&#x27;, &#x27;las&#x27;, &#x27;por&#x27;, &#x27;un&#x27;, &#x27;para&#x27;, &#x27;con&#x27;,\n",
       "                            &#x27;no&#x27;, &#x27;una&#x27;, &#x27;su&#x27;, &#x27;al&#x27;, &#x27;lo&#x27;, &#x27;como&#x27;, &#x27;más&#x27;,\n",
       "                            &#x27;pero&#x27;, &#x27;sus&#x27;, &#x27;le&#x27;, &#x27;ya&#x27;, &#x27;o&#x27;, &#x27;este&#x27;, &#x27;sí&#x27;,\n",
       "                            &#x27;porque&#x27;, ...],\n",
       "                strip_accents=&#x27;unicode&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">feature_selector: SelectFromModel</label><div class=\"sk-toggleable__content\"><pre>SelectFromModel(estimator=RandomForestRegressor())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('encoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['Genero', 'GrupoEtnico',\n",
       "                                                   'EstadoCivil',\n",
       "                                                   'top_lab_code']),\n",
       "                                                 ('numerical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScal...\n",
       "                                                                   CountVectorizer(stop_words=['de',\n",
       "                                                                                               'la',\n",
       "                                                                                               'que',\n",
       "                                                                                               'el',\n",
       "                                                                                               'en',\n",
       "                                                                                               'y',\n",
       "                                                                                               'a',\n",
       "                                                                                               'los',\n",
       "                                                                                               'del',\n",
       "                                                                                               'se',\n",
       "                                                                                               'las',\n",
       "                                                                                               'por',\n",
       "                                                                                               'un',\n",
       "                                                                                               'para',\n",
       "                                                                                               'con',\n",
       "                                                                                               'no',\n",
       "                                                                                               'una',\n",
       "                                                                                               'su',\n",
       "                                                                                               'al',\n",
       "                                                                                               'lo',\n",
       "                                                                                               'como',\n",
       "                                                                                               'más',\n",
       "                                                                                               'pero',\n",
       "                                                                                               'sus',\n",
       "                                                                                               'le',\n",
       "                                                                                               'ya',\n",
       "                                                                                               'o',\n",
       "                                                                                               'este',\n",
       "                                                                                               'sí',\n",
       "                                                                                               'porque', ...],\n",
       "                                                                                   strip_accents='unicode')),\n",
       "                                                                  ('tfidf',\n",
       "                                                                   TfidfTransformer())]),\n",
       "                                                  'Plan')])),\n",
       "                ('feature_selector',\n",
       "                 SelectFromModel(estimator=RandomForestRegressor())),\n",
       "                ('estimator', RandomForestClassifier())])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the GPU prediction if we are using a GPU model for predicting the data\n",
    "if 'GPU_prediction' in df_merge:\n",
    "    categorical_features.append('GPU_prediction')\n",
    "\n",
    "pipeline = PipelineManager(estimator=\"classifier\")\n",
    "pipeline.set_numerical_features(numerical_features)\n",
    "pipeline.set_categorical_features(categorical_features)\n",
    "pipeline.set_text_feature(text_feature)\n",
    "pipeline.set_basic_pipeline()\n",
    "\n",
    "# param_grid = {\n",
    "#     \"n_estimators\": np.linspace(1, 100, 10, dtype=int),\n",
    "#     \"max_depth\": list(np.linspace(1, 10, 5, dtype=int)) + [None],\n",
    "#     \"bootstrap\": [True, False],\n",
    "# }\n",
    "# estimator = RandomForestClassifier()\n",
    "# pipeline.add_estimator(estimator, param_grid)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": np.linspace(1, 200, 10, dtype=int),\n",
    "    \"max_depth\": list(np.linspace(2, 10, 5, dtype=int)) + [None],\n",
    "    \"eta\": np.linspace(0.01, 0.5, 10, dtype=float),\n",
    "    \"min_child_weight\": np.linspace(0.5, 20, 5, dtype=float),\n",
    "    \"gamma\": np.linspace(0, 1, 5, dtype=float),\n",
    "    \"subsample\": np.linspace(0.1, 1, 5, dtype=float),\n",
    "    \"colsample_bytree\": np.linspace(0.2, 1, 5, dtype=float),\n",
    "    \"reg_lambda\": np.linspace(0, 10, 5, dtype=float),\n",
    "    \"reg_alpha\": np.linspace(0, 10, 5, dtype=float),\n",
    "    # \"scale_pos_weight\": np.linspace(0.1, 500, 100, dtype=float),\n",
    "}\n",
    "estimator = XGBClassifier()\n",
    "pipeline.add_estimator(estimator, param_grid)\n",
    "\n",
    "param_grid = {}\n",
    "estimator = PassiveAggressiveClassifier()\n",
    "pipeline.add_estimator(estimator, param_grid)\n",
    "\n",
    "\n",
    "param_grid = {}\n",
    "estimator = SGDClassifier()\n",
    "pipeline.add_estimator(estimator, param_grid)\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": np.linspace(0, 5, 20, dtype=float),\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"gamma\": [\"auto\", \"scale\"],\n",
    "    \"class_weight\": [\"balanced\", None],\n",
    "    \"coef0\": np.linspace(0, 5, 20, dtype=float),\n",
    "    \"degree\": np.linspace(1, 5, 10, dtype=int),\n",
    "}\n",
    "estimator = SVC()\n",
    "pipeline.add_estimator(estimator, param_grid)\n",
    "\n",
    "\n",
    "pipeline.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/juanma/miniconda3/envs/DS4A_Project/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample_weight = compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=df_merge[target_feature])\n",
    "\n",
    "scoring = {\n",
    "    \"Accuracy\": \"balanced_accuracy\",\n",
    "    \"Weighted_F1\": make_scorer(f1_score, average='weighted'),\n",
    "    # 'roc_auc':make_scorer(roc_auc_score, average='weighted'),\n",
    "    }\n",
    "best_model = pipeline.find_best_model(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=cv,\n",
    "    n_iter=n_iter,\n",
    "    n_jobs=n_jobs,\n",
    "    scoring=scoring,\n",
    "    random_state=7,\n",
    "    refit='Weighted_F1',\n",
    "    verbose = 1,\n",
    "    # error_score='raise',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;Genero&#x27;, &#x27;GrupoEtnico&#x27;,\n",
       "                                                   &#x27;AreaResidencial&#x27;,\n",
       "                                                   &#x27;EstadoCivil&#x27;,\n",
       "                                                   &#x27;top_lab_code&#x27;]),\n",
       "                                                 (&#x27;numerical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent...\n",
       "                               gamma=0.5, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "                               importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "                               learning_rate=0.227777779, max_bin=256,\n",
       "                               max_cat_to_onehot=4, max_delta_step=0,\n",
       "                               max_depth=6, max_leaves=0,\n",
       "                               min_child_weight=5.375, missing=nan,\n",
       "                               monotone_constraints=&#x27;()&#x27;, n_estimators=67,\n",
       "                               n_jobs=0, num_parallel_tree=1,\n",
       "                               objective=&#x27;multi:softprob&#x27;, predictor=&#x27;auto&#x27;,\n",
       "                               random_state=0, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;Genero&#x27;, &#x27;GrupoEtnico&#x27;,\n",
       "                                                   &#x27;AreaResidencial&#x27;,\n",
       "                                                   &#x27;EstadoCivil&#x27;,\n",
       "                                                   &#x27;top_lab_code&#x27;]),\n",
       "                                                 (&#x27;numerical&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent...\n",
       "                               gamma=0.5, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "                               importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "                               learning_rate=0.227777779, max_bin=256,\n",
       "                               max_cat_to_onehot=4, max_delta_step=0,\n",
       "                               max_depth=6, max_leaves=0,\n",
       "                               min_child_weight=5.375, missing=nan,\n",
       "                               monotone_constraints=&#x27;()&#x27;, n_estimators=67,\n",
       "                               n_jobs=0, num_parallel_tree=1,\n",
       "                               objective=&#x27;multi:softprob&#x27;, predictor=&#x27;auto&#x27;,\n",
       "                               random_state=0, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;categorical&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                 [&#x27;Genero&#x27;, &#x27;GrupoEtnico&#x27;, &#x27;AreaResidencial&#x27;,\n",
       "                                  &#x27;EstadoCivil&#x27;, &#x27;top_lab_code&#x27;]),\n",
       "                                (&#x27;numerical&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;scaler&#x27;, StandardScaler())]...\n",
       "                                (&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                                  CountVectorizer(ngram_range=(1,\n",
       "                                                                               3),\n",
       "                                                                  stop_words=[&#x27;de&#x27;,\n",
       "                                                                              &#x27;la&#x27;,\n",
       "                                                                              &#x27;que&#x27;,\n",
       "                                                                              &#x27;el&#x27;,\n",
       "                                                                              &#x27;en&#x27;,\n",
       "                                                                              &#x27;y&#x27;,\n",
       "                                                                              &#x27;a&#x27;,\n",
       "                                                                              &#x27;los&#x27;,\n",
       "                                                                              &#x27;del&#x27;,\n",
       "                                                                              &#x27;se&#x27;,\n",
       "                                                                              &#x27;las&#x27;,\n",
       "                                                                              &#x27;por&#x27;,\n",
       "                                                                              &#x27;un&#x27;,\n",
       "                                                                              &#x27;para&#x27;,\n",
       "                                                                              &#x27;con&#x27;,\n",
       "                                                                              &#x27;no&#x27;,\n",
       "                                                                              &#x27;una&#x27;,\n",
       "                                                                              &#x27;su&#x27;,\n",
       "                                                                              &#x27;al&#x27;,\n",
       "                                                                              &#x27;lo&#x27;,\n",
       "                                                                              &#x27;como&#x27;,\n",
       "                                                                              &#x27;más&#x27;,\n",
       "                                                                              &#x27;pero&#x27;,\n",
       "                                                                              &#x27;sus&#x27;,\n",
       "                                                                              &#x27;le&#x27;,\n",
       "                                                                              &#x27;ya&#x27;,\n",
       "                                                                              &#x27;o&#x27;,\n",
       "                                                                              &#x27;este&#x27;,\n",
       "                                                                              &#x27;sí&#x27;,\n",
       "                                                                              &#x27;porque&#x27;, ...],\n",
       "                                                                  strip_accents=&#x27;unicode&#x27;)),\n",
       "                                                 (&#x27;tfidf&#x27;,\n",
       "                                                  TfidfTransformer(norm=&#x27;l1&#x27;,\n",
       "                                                                   sublinear_tf=True))]),\n",
       "                                 &#x27;Plan&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">categorical</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Genero&#x27;, &#x27;GrupoEtnico&#x27;, &#x27;AreaResidencial&#x27;, &#x27;EstadoCivil&#x27;, &#x27;top_lab_code&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numerical</label><div class=\"sk-toggleable__content\"><pre>[&#x27;hepatitis_count&#x27;, &#x27;linfocitos_max&#x27;, &#x27;last_lab_date&#x27;, &#x27;date_diff_max&#x27;, &#x27;keto&#x27;, &#x27;vih&#x27;, &#x27;penici&#x27;, &#x27;test_reloj_orden&#x27;, &#x27;glucosa_count&#x27;, &#x27;linfocitos_count&#x27;, &#x27;top_lab_max_value&#x27;, &#x27;rpr&#x27;, &#x27;date_diff_first_last&#x27;, &#x27;vih_max&#x27;, &#x27;sifili&#x27;, &#x27;diet&#x27;, &#x27;top_lab_count&#x27;, &#x27;top_lab_avg_value&#x27;, &#x27;cabeza&#x27;, &#x27;acido&#x27;, &#x27;Edad&#x27;, &#x27;hepat&#x27;, &#x27;insulin&#x27;, &#x27;hepatitis_max&#x27;, &#x27;diabet&#x27;, &#x27;glucosa_max&#x27;, &#x27;antibio&#x27;, &#x27;asintoma&#x27;, &#x27;sable&#x27;, &#x27;preservativo&#x27;, &#x27;total_lab_count&#x27;, &#x27;date_diff_mean&#x27;, &#x27;first_lab_date&#x27;, &#x27;gluco&#x27;, &#x27;serolo&#x27;, &#x27;vih_count&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>Plan</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 3),\n",
       "                stop_words=[&#x27;de&#x27;, &#x27;la&#x27;, &#x27;que&#x27;, &#x27;el&#x27;, &#x27;en&#x27;, &#x27;y&#x27;, &#x27;a&#x27;, &#x27;los&#x27;,\n",
       "                            &#x27;del&#x27;, &#x27;se&#x27;, &#x27;las&#x27;, &#x27;por&#x27;, &#x27;un&#x27;, &#x27;para&#x27;, &#x27;con&#x27;,\n",
       "                            &#x27;no&#x27;, &#x27;una&#x27;, &#x27;su&#x27;, &#x27;al&#x27;, &#x27;lo&#x27;, &#x27;como&#x27;, &#x27;más&#x27;,\n",
       "                            &#x27;pero&#x27;, &#x27;sus&#x27;, &#x27;le&#x27;, &#x27;ya&#x27;, &#x27;o&#x27;, &#x27;este&#x27;, &#x27;sí&#x27;,\n",
       "                            &#x27;porque&#x27;, ...],\n",
       "                strip_accents=&#x27;unicode&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer(norm=&#x27;l1&#x27;, sublinear_tf=True)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">feature_selector: SelectFromModel</label><div class=\"sk-toggleable__content\"><pre>SelectFromModel(estimator=Ridge())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.2,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eta=0.22777777777777777, eval_metric=None, gamma=0.5, gpu_id=-1,\n",
       "              grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.227777779,\n",
       "              max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=6,\n",
       "              max_leaves=0, min_child_weight=5.375, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=67, n_jobs=0,\n",
       "              num_parallel_tree=1, objective=&#x27;multi:softprob&#x27;, predictor=&#x27;auto&#x27;,\n",
       "              random_state=0, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('encoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['Genero', 'GrupoEtnico',\n",
       "                                                   'AreaResidencial',\n",
       "                                                   'EstadoCivil',\n",
       "                                                   'top_lab_code']),\n",
       "                                                 ('numerical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent...\n",
       "                               gamma=0.5, gpu_id=-1, grow_policy='depthwise',\n",
       "                               importance_type=None, interaction_constraints='',\n",
       "                               learning_rate=0.227777779, max_bin=256,\n",
       "                               max_cat_to_onehot=4, max_delta_step=0,\n",
       "                               max_depth=6, max_leaves=0,\n",
       "                               min_child_weight=5.375, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=67,\n",
       "                               n_jobs=0, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', predictor='auto',\n",
       "                               random_state=0, ...))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_preprocessor__text__vectorizer</th>\n",
       "      <th>param_preprocessor__text__tfidf</th>\n",
       "      <th>param_preprocessor__numerical__scaler</th>\n",
       "      <th>param_preprocessor__numerical__imputer</th>\n",
       "      <th>param_preprocessor__categorical__imputer</th>\n",
       "      <th>param_feature_selector</th>\n",
       "      <th>param_estimator__subsample</th>\n",
       "      <th>param_estimator__reg_lambda</th>\n",
       "      <th>param_estimator__reg_alpha</th>\n",
       "      <th>param_estimator__n_estimators</th>\n",
       "      <th>param_estimator__min_child_weight</th>\n",
       "      <th>param_estimator__max_depth</th>\n",
       "      <th>param_estimator__gamma</th>\n",
       "      <th>param_estimator__eta</th>\n",
       "      <th>param_estimator__colsample_bytree</th>\n",
       "      <th>param_estimator</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_Accuracy</th>\n",
       "      <th>split1_test_Accuracy</th>\n",
       "      <th>split2_test_Accuracy</th>\n",
       "      <th>mean_test_Accuracy</th>\n",
       "      <th>std_test_Accuracy</th>\n",
       "      <th>rank_test_Accuracy</th>\n",
       "      <th>split0_test_Weighted_F1</th>\n",
       "      <th>split1_test_Weighted_F1</th>\n",
       "      <th>split2_test_Weighted_F1</th>\n",
       "      <th>mean_test_Weighted_F1</th>\n",
       "      <th>std_test_Weighted_F1</th>\n",
       "      <th>rank_test_Weighted_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.277985</td>\n",
       "      <td>0.005590</td>\n",
       "      <td>0.019038</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67</td>\n",
       "      <td>5.375</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.227778</td>\n",
       "      <td>0.2</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.349206</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.267196</td>\n",
       "      <td>0.328924</td>\n",
       "      <td>0.044496</td>\n",
       "      <td>1</td>\n",
       "      <td>0.347210</td>\n",
       "      <td>0.370255</td>\n",
       "      <td>0.262272</td>\n",
       "      <td>0.326579</td>\n",
       "      <td>0.046435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.567838</td>\n",
       "      <td>0.167535</td>\n",
       "      <td>0.021854</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.775</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>200</td>\n",
       "      <td>5.375</td>\n",
       "      <td>8</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.118889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.394180</td>\n",
       "      <td>0.298942</td>\n",
       "      <td>0.219577</td>\n",
       "      <td>0.304233</td>\n",
       "      <td>0.071380</td>\n",
       "      <td>2</td>\n",
       "      <td>0.357020</td>\n",
       "      <td>0.295769</td>\n",
       "      <td>0.182842</td>\n",
       "      <td>0.278544</td>\n",
       "      <td>0.072143</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.078664</td>\n",
       "      <td>0.040046</td>\n",
       "      <td>0.012428</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>5.375</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.227778</td>\n",
       "      <td>0.4</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.441799</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.276896</td>\n",
       "      <td>0.116849</td>\n",
       "      <td>3</td>\n",
       "      <td>0.408136</td>\n",
       "      <td>0.078365</td>\n",
       "      <td>0.061264</td>\n",
       "      <td>0.182589</td>\n",
       "      <td>0.159639</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.778019</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>0.020693</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.55</td>\n",
       "      <td>7.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>177</td>\n",
       "      <td>5.375</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.282222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.208995</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.217813</td>\n",
       "      <td>0.016356</td>\n",
       "      <td>4</td>\n",
       "      <td>0.092038</td>\n",
       "      <td>0.138210</td>\n",
       "      <td>0.119950</td>\n",
       "      <td>0.116733</td>\n",
       "      <td>0.018986</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.126407</td>\n",
       "      <td>0.073448</td>\n",
       "      <td>0.015020</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>0.775</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>67</td>\n",
       "      <td>5.375</td>\n",
       "      <td>8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.227778</td>\n",
       "      <td>0.4</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.267196</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.163139</td>\n",
       "      <td>0.073579</td>\n",
       "      <td>5</td>\n",
       "      <td>0.135536</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.058546</td>\n",
       "      <td>0.054441</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.229574</td>\n",
       "      <td>0.006713</td>\n",
       "      <td>0.021407</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>CountVectorizer(stop_words=['de', 'la', 'que',...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.55</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>89</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.391111</td>\n",
       "      <td>0.6</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.137566</td>\n",
       "      <td>0.037413</td>\n",
       "      <td>6</td>\n",
       "      <td>0.093792</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.044631</td>\n",
       "      <td>0.034762</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.123770</td>\n",
       "      <td>0.053560</td>\n",
       "      <td>0.015239</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>89</td>\n",
       "      <td>10.25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.282222</td>\n",
       "      <td>0.2</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.121693</td>\n",
       "      <td>0.014965</td>\n",
       "      <td>7</td>\n",
       "      <td>0.050125</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.014178</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.763731</td>\n",
       "      <td>0.037080</td>\n",
       "      <td>0.019205</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>177</td>\n",
       "      <td>5.375</td>\n",
       "      <td>8</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.064444</td>\n",
       "      <td>0.2</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.181924</td>\n",
       "      <td>0.008950</td>\n",
       "      <td>0.014707</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.775</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.173333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.201903</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.019468</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>CountVectorizer(ngram_range=(4, 4),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>177</td>\n",
       "      <td>15.125</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.336667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.414289</td>\n",
       "      <td>0.015407</td>\n",
       "      <td>0.019181</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>CountVectorizer(stop_words=['de', 'la', 'que',...</td>\n",
       "      <td>TfidfTransformer()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>15.125</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.227778</td>\n",
       "      <td>0.6</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.033627</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.012361</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>10.25</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.282222</td>\n",
       "      <td>0.4</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.591169</td>\n",
       "      <td>1.673502</td>\n",
       "      <td>0.020178</td>\n",
       "      <td>0.006598</td>\n",
       "      <td>CountVectorizer(ngram_range=(4, 4),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.325</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>133</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.391111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.716587</td>\n",
       "      <td>0.578727</td>\n",
       "      <td>0.020758</td>\n",
       "      <td>0.006721</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.775</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>200</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.118889</td>\n",
       "      <td>0.2</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.752196</td>\n",
       "      <td>0.840868</td>\n",
       "      <td>0.015278</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>0.55</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>200</td>\n",
       "      <td>5.375</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.118889</td>\n",
       "      <td>0.6</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.432170</td>\n",
       "      <td>0.014783</td>\n",
       "      <td>0.018117</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.325</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>177</td>\n",
       "      <td>10.25</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.118889</td>\n",
       "      <td>0.8</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.224303</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>0.021704</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.325</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>89</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.282222</td>\n",
       "      <td>0.2</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.219563</td>\n",
       "      <td>0.027853</td>\n",
       "      <td>0.017880</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>111</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.336667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.633894</td>\n",
       "      <td>0.050821</td>\n",
       "      <td>0.018620</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>155</td>\n",
       "      <td>15.125</td>\n",
       "      <td>10</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.227778</td>\n",
       "      <td>0.8</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.491963</td>\n",
       "      <td>0.397337</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.325</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.064444</td>\n",
       "      <td>1.0</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.887414</td>\n",
       "      <td>0.884333</td>\n",
       "      <td>0.027508</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>155</td>\n",
       "      <td>5.375</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.064444</td>\n",
       "      <td>0.2</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.362478</td>\n",
       "      <td>0.024692</td>\n",
       "      <td>0.019921</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>CountVectorizer(stop_words=['de', 'la', 'que',...</td>\n",
       "      <td>TfidfTransformer()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.325</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>133</td>\n",
       "      <td>10.25</td>\n",
       "      <td>None</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.445556</td>\n",
       "      <td>0.8</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.421572</td>\n",
       "      <td>0.008263</td>\n",
       "      <td>0.015292</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>CountVectorizer(stop_words=['de', 'la', 'que',...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.55</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>177</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.242504</td>\n",
       "      <td>0.011706</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.006307</td>\n",
       "      <td>CountVectorizer(ngram_range=(4, 4),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.325</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>177</td>\n",
       "      <td>5.375</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.336667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>{'preprocessor__text__vectorizer': CountVector...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.020050</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.003212</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "11       0.277985      0.005590         0.019038        0.003064   \n",
       "21       1.567838      0.167535         0.021854        0.000690   \n",
       "6        0.078664      0.040046         0.012428        0.000216   \n",
       "8        0.778019      0.011824         0.020693        0.002599   \n",
       "13       0.126407      0.073448         0.015020        0.000628   \n",
       "22       0.229574      0.006713         0.021407        0.001244   \n",
       "1        0.123770      0.053560         0.015239        0.001750   \n",
       "20       0.763731      0.037080         0.019205        0.000947   \n",
       "19       0.181924      0.008950         0.014707        0.002279   \n",
       "18       0.201903      0.010279         0.019468        0.002108   \n",
       "17       0.414289      0.015407         0.019181        0.001570   \n",
       "16       0.033627      0.001412         0.012361        0.000564   \n",
       "15       2.591169      1.673502         0.020178        0.006598   \n",
       "14       0.716587      0.578727         0.020758        0.006721   \n",
       "0        0.752196      0.840868         0.015278        0.000311   \n",
       "10       0.432170      0.014783         0.018117        0.001274   \n",
       "9        0.224303      0.009089         0.021704        0.000545   \n",
       "7        0.219563      0.027853         0.017880        0.000499   \n",
       "5        0.633894      0.050821         0.018620        0.000450   \n",
       "4        0.491963      0.397337         0.017798        0.001372   \n",
       "3        0.887414      0.884333         0.027508        0.006453   \n",
       "2        0.362478      0.024692         0.019921        0.000893   \n",
       "12       0.421572      0.008263         0.015292        0.000343   \n",
       "23       0.242504      0.011706         0.024038        0.006307   \n",
       "\n",
       "                 param_preprocessor__text__vectorizer  \\\n",
       "11  CountVectorizer(ngram_range=(1, 3),\\n         ...   \n",
       "21  CountVectorizer(ngram_range=(1, 2),\\n         ...   \n",
       "6   CountVectorizer(ngram_range=(2, 2),\\n         ...   \n",
       "8   CountVectorizer(ngram_range=(2, 3),\\n         ...   \n",
       "13  CountVectorizer(ngram_range=(2, 2),\\n         ...   \n",
       "22  CountVectorizer(stop_words=['de', 'la', 'que',...   \n",
       "1   CountVectorizer(ngram_range=(2, 2),\\n         ...   \n",
       "20  CountVectorizer(ngram_range=(1, 3),\\n         ...   \n",
       "19  CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "18  CountVectorizer(ngram_range=(4, 4),\\n         ...   \n",
       "17  CountVectorizer(stop_words=['de', 'la', 'que',...   \n",
       "16  CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "15  CountVectorizer(ngram_range=(4, 4),\\n         ...   \n",
       "14  CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "0   CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "10  CountVectorizer(ngram_range=(1, 2),\\n         ...   \n",
       "9   CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "7   CountVectorizer(ngram_range=(1, 3),\\n         ...   \n",
       "5   CountVectorizer(ngram_range=(2, 3),\\n         ...   \n",
       "4   CountVectorizer(ngram_range=(1, 3),\\n         ...   \n",
       "3   CountVectorizer(ngram_range=(1, 2),\\n         ...   \n",
       "2   CountVectorizer(stop_words=['de', 'la', 'que',...   \n",
       "12  CountVectorizer(stop_words=['de', 'la', 'que',...   \n",
       "23  CountVectorizer(ngram_range=(4, 4),\\n         ...   \n",
       "\n",
       "                   param_preprocessor__text__tfidf  \\\n",
       "11  TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "21                     TfidfTransformer(norm='l1')   \n",
       "6   TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "8                      TfidfTransformer(norm='l1')   \n",
       "13                              TfidfTransformer()   \n",
       "22             TfidfTransformer(sublinear_tf=True)   \n",
       "1                      TfidfTransformer(norm='l1')   \n",
       "20                     TfidfTransformer(norm='l1')   \n",
       "19             TfidfTransformer(sublinear_tf=True)   \n",
       "18  TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "17                              TfidfTransformer()   \n",
       "16  TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "15             TfidfTransformer(sublinear_tf=True)   \n",
       "14             TfidfTransformer(sublinear_tf=True)   \n",
       "0   TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "10                              TfidfTransformer()   \n",
       "9                      TfidfTransformer(norm='l1')   \n",
       "7                      TfidfTransformer(norm='l1')   \n",
       "5              TfidfTransformer(sublinear_tf=True)   \n",
       "4              TfidfTransformer(sublinear_tf=True)   \n",
       "3              TfidfTransformer(sublinear_tf=True)   \n",
       "2                               TfidfTransformer()   \n",
       "12             TfidfTransformer(sublinear_tf=True)   \n",
       "23                     TfidfTransformer(norm='l1')   \n",
       "\n",
       "   param_preprocessor__numerical__scaler  \\\n",
       "11                      StandardScaler()   \n",
       "21                      StandardScaler()   \n",
       "6                         MinMaxScaler()   \n",
       "8                           Normalizer()   \n",
       "13                        MinMaxScaler()   \n",
       "22                        RobustScaler()   \n",
       "1                         MinMaxScaler()   \n",
       "20                      StandardScaler()   \n",
       "19                        RobustScaler()   \n",
       "18                        MinMaxScaler()   \n",
       "17                      StandardScaler()   \n",
       "16                          Normalizer()   \n",
       "15                      StandardScaler()   \n",
       "14                          Normalizer()   \n",
       "0                         RobustScaler()   \n",
       "10                      StandardScaler()   \n",
       "9                         RobustScaler()   \n",
       "7                       StandardScaler()   \n",
       "5                           Normalizer()   \n",
       "4                         MinMaxScaler()   \n",
       "3                         MinMaxScaler()   \n",
       "2                       StandardScaler()   \n",
       "12                      StandardScaler()   \n",
       "23                        MinMaxScaler()   \n",
       "\n",
       "     param_preprocessor__numerical__imputer  \\\n",
       "11  SimpleImputer(strategy='most_frequent')   \n",
       "21                             KNNImputer()   \n",
       "6   SimpleImputer(strategy='most_frequent')   \n",
       "8   SimpleImputer(strategy='most_frequent')   \n",
       "13  SimpleImputer(strategy='most_frequent')   \n",
       "22                             KNNImputer()   \n",
       "1          SimpleImputer(strategy='median')   \n",
       "20                          SimpleImputer()   \n",
       "19  SimpleImputer(strategy='most_frequent')   \n",
       "18                          SimpleImputer()   \n",
       "17                             KNNImputer()   \n",
       "16                          SimpleImputer()   \n",
       "15  SimpleImputer(strategy='most_frequent')   \n",
       "14         SimpleImputer(strategy='median')   \n",
       "0                           SimpleImputer()   \n",
       "10         SimpleImputer(strategy='median')   \n",
       "9                              KNNImputer()   \n",
       "7          SimpleImputer(strategy='median')   \n",
       "5          SimpleImputer(strategy='median')   \n",
       "4          SimpleImputer(strategy='median')   \n",
       "3   SimpleImputer(strategy='most_frequent')   \n",
       "2                              KNNImputer()   \n",
       "12         SimpleImputer(strategy='median')   \n",
       "23         SimpleImputer(strategy='median')   \n",
       "\n",
       "   param_preprocessor__categorical__imputer  \\\n",
       "11  SimpleImputer(strategy='most_frequent')   \n",
       "21  SimpleImputer(strategy='most_frequent')   \n",
       "6   SimpleImputer(strategy='most_frequent')   \n",
       "8   SimpleImputer(strategy='most_frequent')   \n",
       "13  SimpleImputer(strategy='most_frequent')   \n",
       "22  SimpleImputer(strategy='most_frequent')   \n",
       "1   SimpleImputer(strategy='most_frequent')   \n",
       "20  SimpleImputer(strategy='most_frequent')   \n",
       "19  SimpleImputer(strategy='most_frequent')   \n",
       "18  SimpleImputer(strategy='most_frequent')   \n",
       "17  SimpleImputer(strategy='most_frequent')   \n",
       "16  SimpleImputer(strategy='most_frequent')   \n",
       "15  SimpleImputer(strategy='most_frequent')   \n",
       "14  SimpleImputer(strategy='most_frequent')   \n",
       "0   SimpleImputer(strategy='most_frequent')   \n",
       "10  SimpleImputer(strategy='most_frequent')   \n",
       "9   SimpleImputer(strategy='most_frequent')   \n",
       "7   SimpleImputer(strategy='most_frequent')   \n",
       "5   SimpleImputer(strategy='most_frequent')   \n",
       "4   SimpleImputer(strategy='most_frequent')   \n",
       "3   SimpleImputer(strategy='most_frequent')   \n",
       "2   SimpleImputer(strategy='most_frequent')   \n",
       "12  SimpleImputer(strategy='most_frequent')   \n",
       "23  SimpleImputer(strategy='most_frequent')   \n",
       "\n",
       "                     param_feature_selector param_estimator__subsample  \\\n",
       "11       SelectFromModel(estimator=Ridge())                      0.775   \n",
       "21                      VarianceThreshold()                      0.775   \n",
       "6   SelectFromModel(estimator=ElasticNet())                        1.0   \n",
       "8                       VarianceThreshold()                       0.55   \n",
       "13  SelectFromModel(estimator=ElasticNet())                      0.775   \n",
       "22                      VarianceThreshold()                       0.55   \n",
       "1   SelectFromModel(estimator=ElasticNet())                        1.0   \n",
       "20                      VarianceThreshold()                        0.1   \n",
       "19                      VarianceThreshold()                      0.775   \n",
       "18       SelectFromModel(estimator=Ridge())                        1.0   \n",
       "17                      VarianceThreshold()                        1.0   \n",
       "16                      VarianceThreshold()                        1.0   \n",
       "15                      VarianceThreshold()                      0.325   \n",
       "14       SelectFromModel(estimator=Ridge())                      0.775   \n",
       "0   SelectFromModel(estimator=ElasticNet())                       0.55   \n",
       "10       SelectFromModel(estimator=Ridge())                      0.325   \n",
       "9                       VarianceThreshold()                      0.325   \n",
       "7   SelectFromModel(estimator=ElasticNet())                        0.1   \n",
       "5        SelectFromModel(estimator=Ridge())                        1.0   \n",
       "4        SelectFromModel(estimator=Ridge())                      0.325   \n",
       "3        SelectFromModel(estimator=Ridge())                       0.55   \n",
       "2                       VarianceThreshold()                      0.325   \n",
       "12                      VarianceThreshold()                       0.55   \n",
       "23       SelectFromModel(estimator=Ridge())                      0.325   \n",
       "\n",
       "   param_estimator__reg_lambda param_estimator__reg_alpha  \\\n",
       "11                         0.0                        0.0   \n",
       "21                         5.0                        2.5   \n",
       "6                          7.5                        0.0   \n",
       "8                          7.5                        5.0   \n",
       "13                        10.0                        5.0   \n",
       "22                        10.0                        7.5   \n",
       "1                          0.0                        5.0   \n",
       "20                         2.5                        7.5   \n",
       "19                        10.0                        0.0   \n",
       "18                         7.5                       10.0   \n",
       "17                         0.0                        7.5   \n",
       "16                         7.5                        7.5   \n",
       "15                         2.5                        7.5   \n",
       "14                         5.0                        7.5   \n",
       "0                          5.0                       10.0   \n",
       "10                         7.5                       10.0   \n",
       "9                          5.0                       10.0   \n",
       "7                          5.0                        7.5   \n",
       "5                          5.0                        2.5   \n",
       "4                          7.5                       10.0   \n",
       "3                          0.0                        7.5   \n",
       "2                          5.0                        5.0   \n",
       "12                         7.5                        7.5   \n",
       "23                         7.5                       10.0   \n",
       "\n",
       "   param_estimator__n_estimators param_estimator__min_child_weight  \\\n",
       "11                            67                             5.375   \n",
       "21                           200                             5.375   \n",
       "6                             23                             5.375   \n",
       "8                            177                             5.375   \n",
       "13                            67                             5.375   \n",
       "22                            89                               0.5   \n",
       "1                             89                             10.25   \n",
       "20                           177                             5.375   \n",
       "19                            67                              20.0   \n",
       "18                           177                            15.125   \n",
       "17                           200                            15.125   \n",
       "16                             1                             10.25   \n",
       "15                           133                              20.0   \n",
       "14                           200                              20.0   \n",
       "0                            200                             5.375   \n",
       "10                           177                             10.25   \n",
       "9                             89                              20.0   \n",
       "7                            111                              20.0   \n",
       "5                            155                            15.125   \n",
       "4                             23                               0.5   \n",
       "3                            155                             5.375   \n",
       "2                            133                             10.25   \n",
       "12                           177                               0.5   \n",
       "23                           177                             5.375   \n",
       "\n",
       "   param_estimator__max_depth param_estimator__gamma param_estimator__eta  \\\n",
       "11                          6                    0.5             0.227778   \n",
       "21                          8                   0.25             0.118889   \n",
       "6                           2                    1.0             0.227778   \n",
       "8                          10                    0.5             0.282222   \n",
       "13                          8                    0.5             0.227778   \n",
       "22                          8                    0.5             0.391111   \n",
       "1                           4                   0.25             0.282222   \n",
       "20                          8                   0.75             0.064444   \n",
       "19                          6                   0.25             0.173333   \n",
       "18                          6                    0.0             0.336667   \n",
       "17                          6                   0.75             0.227778   \n",
       "16                       None                    1.0             0.282222   \n",
       "15                         10                    0.5             0.391111   \n",
       "14                         10                    1.0             0.118889   \n",
       "0                           6                    0.5             0.118889   \n",
       "10                          6                    1.0             0.118889   \n",
       "9                           6                    1.0             0.282222   \n",
       "7                           6                   0.75             0.336667   \n",
       "5                          10                   0.25             0.227778   \n",
       "4                           4                   0.25             0.064444   \n",
       "3                           6                    0.5             0.064444   \n",
       "2                        None                    0.5             0.445556   \n",
       "12                          4                    1.0                  0.5   \n",
       "23                          2                   0.25             0.336667   \n",
       "\n",
       "   param_estimator__colsample_bytree  \\\n",
       "11                               0.2   \n",
       "21                               1.0   \n",
       "6                                0.4   \n",
       "8                                1.0   \n",
       "13                               0.4   \n",
       "22                               0.6   \n",
       "1                                0.2   \n",
       "20                               0.2   \n",
       "19                               0.2   \n",
       "18                               0.2   \n",
       "17                               0.6   \n",
       "16                               0.4   \n",
       "15                               1.0   \n",
       "14                               0.2   \n",
       "0                                0.6   \n",
       "10                               0.8   \n",
       "9                                0.2   \n",
       "7                                0.2   \n",
       "5                                0.8   \n",
       "4                                1.0   \n",
       "3                                0.2   \n",
       "2                                0.8   \n",
       "12                               0.6   \n",
       "23                               0.4   \n",
       "\n",
       "                                      param_estimator  \\\n",
       "11  XGBClassifier(base_score=None, booster=None, c...   \n",
       "21  XGBClassifier(base_score=None, booster=None, c...   \n",
       "6   XGBClassifier(base_score=None, booster=None, c...   \n",
       "8   XGBClassifier(base_score=None, booster=None, c...   \n",
       "13  XGBClassifier(base_score=None, booster=None, c...   \n",
       "22  XGBClassifier(base_score=None, booster=None, c...   \n",
       "1   XGBClassifier(base_score=None, booster=None, c...   \n",
       "20  XGBClassifier(base_score=None, booster=None, c...   \n",
       "19  XGBClassifier(base_score=None, booster=None, c...   \n",
       "18  XGBClassifier(base_score=None, booster=None, c...   \n",
       "17  XGBClassifier(base_score=None, booster=None, c...   \n",
       "16  XGBClassifier(base_score=None, booster=None, c...   \n",
       "15  XGBClassifier(base_score=None, booster=None, c...   \n",
       "14  XGBClassifier(base_score=None, booster=None, c...   \n",
       "0   XGBClassifier(base_score=None, booster=None, c...   \n",
       "10  XGBClassifier(base_score=None, booster=None, c...   \n",
       "9   XGBClassifier(base_score=None, booster=None, c...   \n",
       "7   XGBClassifier(base_score=None, booster=None, c...   \n",
       "5   XGBClassifier(base_score=None, booster=None, c...   \n",
       "4   XGBClassifier(base_score=None, booster=None, c...   \n",
       "3   XGBClassifier(base_score=None, booster=None, c...   \n",
       "2   XGBClassifier(base_score=None, booster=None, c...   \n",
       "12  XGBClassifier(base_score=None, booster=None, c...   \n",
       "23  XGBClassifier(base_score=None, booster=None, c...   \n",
       "\n",
       "                                               params  split0_test_Accuracy  \\\n",
       "11  {'preprocessor__text__vectorizer': CountVector...              0.349206   \n",
       "21  {'preprocessor__text__vectorizer': CountVector...              0.394180   \n",
       "6   {'preprocessor__text__vectorizer': CountVector...              0.441799   \n",
       "8   {'preprocessor__text__vectorizer': CountVector...              0.203704   \n",
       "13  {'preprocessor__text__vectorizer': CountVector...              0.267196   \n",
       "22  {'preprocessor__text__vectorizer': CountVector...              0.190476   \n",
       "1   {'preprocessor__text__vectorizer': CountVector...              0.142857   \n",
       "20  {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "19  {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "18  {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "17  {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "16  {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "15  {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "14  {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "0   {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "10  {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "9   {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "7   {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "5   {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "4   {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "3   {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "2   {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "12  {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "23  {'preprocessor__text__vectorizer': CountVector...              0.111111   \n",
       "\n",
       "    split1_test_Accuracy  split2_test_Accuracy  mean_test_Accuracy  \\\n",
       "11              0.370370              0.267196            0.328924   \n",
       "21              0.298942              0.219577            0.304233   \n",
       "6               0.203704              0.185185            0.276896   \n",
       "8               0.208995              0.240741            0.217813   \n",
       "13              0.111111              0.111111            0.163139   \n",
       "22              0.111111              0.111111            0.137566   \n",
       "1               0.111111              0.111111            0.121693   \n",
       "20              0.111111              0.111111            0.111111   \n",
       "19              0.111111              0.111111            0.111111   \n",
       "18              0.111111              0.111111            0.111111   \n",
       "17              0.111111              0.111111            0.111111   \n",
       "16              0.111111              0.111111            0.111111   \n",
       "15              0.111111              0.111111            0.111111   \n",
       "14              0.111111              0.111111            0.111111   \n",
       "0               0.111111              0.111111            0.111111   \n",
       "10              0.111111              0.111111            0.111111   \n",
       "9               0.111111              0.111111            0.111111   \n",
       "7               0.111111              0.111111            0.111111   \n",
       "5               0.111111              0.111111            0.111111   \n",
       "4               0.111111              0.111111            0.111111   \n",
       "3               0.111111              0.111111            0.111111   \n",
       "2               0.111111              0.111111            0.111111   \n",
       "12              0.111111              0.111111            0.111111   \n",
       "23              0.111111              0.111111            0.111111   \n",
       "\n",
       "    std_test_Accuracy  rank_test_Accuracy  split0_test_Weighted_F1  \\\n",
       "11           0.044496                   1                 0.347210   \n",
       "21           0.071380                   2                 0.357020   \n",
       "6            0.116849                   3                 0.408136   \n",
       "8            0.016356                   4                 0.092038   \n",
       "13           0.073579                   5                 0.135536   \n",
       "22           0.037413                   6                 0.093792   \n",
       "1            0.014965                   7                 0.050125   \n",
       "20           0.000000                   8                 0.026864   \n",
       "19           0.000000                   8                 0.020050   \n",
       "18           0.000000                   8                 0.026864   \n",
       "17           0.000000                   8                 0.026864   \n",
       "16           0.000000                   8                 0.026864   \n",
       "15           0.000000                   8                 0.026864   \n",
       "14           0.000000                   8                 0.026864   \n",
       "0            0.000000                   8                 0.026864   \n",
       "10           0.000000                   8                 0.026864   \n",
       "9            0.000000                   8                 0.026864   \n",
       "7            0.000000                   8                 0.026864   \n",
       "5            0.000000                   8                 0.026864   \n",
       "4            0.000000                   8                 0.026864   \n",
       "3            0.000000                   8                 0.026864   \n",
       "2            0.000000                   8                 0.026864   \n",
       "12           0.000000                   8                 0.026864   \n",
       "23           0.000000                   8                 0.026864   \n",
       "\n",
       "    split1_test_Weighted_F1  split2_test_Weighted_F1  mean_test_Weighted_F1  \\\n",
       "11                 0.370255                 0.262272               0.326579   \n",
       "21                 0.295769                 0.182842               0.278544   \n",
       "6                  0.078365                 0.061264               0.182589   \n",
       "8                  0.138210                 0.119950               0.116733   \n",
       "13                 0.020050                 0.020050               0.058546   \n",
       "22                 0.020050                 0.020050               0.044631   \n",
       "1                  0.020050                 0.020050               0.030075   \n",
       "20                 0.020050                 0.020050               0.022321   \n",
       "19                 0.026864                 0.020050               0.022321   \n",
       "18                 0.020050                 0.020050               0.022321   \n",
       "17                 0.020050                 0.020050               0.022321   \n",
       "16                 0.020050                 0.020050               0.022321   \n",
       "15                 0.020050                 0.020050               0.022321   \n",
       "14                 0.020050                 0.020050               0.022321   \n",
       "0                  0.020050                 0.020050               0.022321   \n",
       "10                 0.020050                 0.020050               0.022321   \n",
       "9                  0.020050                 0.020050               0.022321   \n",
       "7                  0.020050                 0.020050               0.022321   \n",
       "5                  0.020050                 0.020050               0.022321   \n",
       "4                  0.020050                 0.020050               0.022321   \n",
       "3                  0.020050                 0.020050               0.022321   \n",
       "2                  0.020050                 0.020050               0.022321   \n",
       "12                 0.020050                 0.020050               0.022321   \n",
       "23                 0.020050                 0.020050               0.022321   \n",
       "\n",
       "    std_test_Weighted_F1  rank_test_Weighted_F1  \n",
       "11              0.046435                      1  \n",
       "21              0.072143                      2  \n",
       "6               0.159639                      3  \n",
       "8               0.018986                      4  \n",
       "13              0.054441                      5  \n",
       "22              0.034762                      6  \n",
       "1               0.014178                      7  \n",
       "20              0.003212                      8  \n",
       "19              0.003212                      8  \n",
       "18              0.003212                      8  \n",
       "17              0.003212                      8  \n",
       "16              0.003212                      8  \n",
       "15              0.003212                      8  \n",
       "14              0.003212                      8  \n",
       "0               0.003212                      8  \n",
       "10              0.003212                      8  \n",
       "9               0.003212                      8  \n",
       "7               0.003212                      8  \n",
       "5               0.003212                      8  \n",
       "4               0.003212                      8  \n",
       "3               0.003212                      8  \n",
       "2               0.003212                      8  \n",
       "12              0.003212                      8  \n",
       "23              0.003212                      8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_columns', None):\n",
    "    display(pipeline.cv_results.sort_values(by=[\"rank_test_Weighted_F1\"]).head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_Weighted_F1</th>\n",
       "      <th>rank_test_Accuracy</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>param_estimator</th>\n",
       "      <th>param_preprocessor__text__vectorizer</th>\n",
       "      <th>param_preprocessor__text__tfidf</th>\n",
       "      <th>param_preprocessor__numerical__scaler</th>\n",
       "      <th>param_preprocessor__numerical__imputer</th>\n",
       "      <th>param_feature_selector</th>\n",
       "      <th>mean_test_Accuracy</th>\n",
       "      <th>mean_test_Weighted_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.277985</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.328924</td>\n",
       "      <td>0.326579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.567838</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.304233</td>\n",
       "      <td>0.278544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.078664</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>0.276896</td>\n",
       "      <td>0.182589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.778019</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.217813</td>\n",
       "      <td>0.116733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.126407</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>0.163139</td>\n",
       "      <td>0.058546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.229574</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(stop_words=['de', 'la', 'que',...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.137566</td>\n",
       "      <td>0.044631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.123770</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>0.121693</td>\n",
       "      <td>0.030075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.763731</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.181924</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.201903</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(4, 4),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.414289</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(stop_words=['de', 'la', 'que',...</td>\n",
       "      <td>TfidfTransformer()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.033627</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2.591169</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(4, 4),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.716587</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.752196</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.432170</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.224303</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.219563</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.633894</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.491963</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.887414</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.362478</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(stop_words=['de', 'la', 'que',...</td>\n",
       "      <td>TfidfTransformer()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.421572</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(stop_words=['de', 'la', 'que',...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.242504</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(4, 4),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank_test_Weighted_F1  rank_test_Accuracy  mean_fit_time  \\\n",
       "11                      1                   1       0.277985   \n",
       "21                      2                   2       1.567838   \n",
       "6                       3                   3       0.078664   \n",
       "8                       4                   4       0.778019   \n",
       "13                      5                   5       0.126407   \n",
       "22                      6                   6       0.229574   \n",
       "1                       7                   7       0.123770   \n",
       "20                      8                   8       0.763731   \n",
       "19                      8                   8       0.181924   \n",
       "18                      8                   8       0.201903   \n",
       "17                      8                   8       0.414289   \n",
       "16                      8                   8       0.033627   \n",
       "15                      8                   8       2.591169   \n",
       "14                      8                   8       0.716587   \n",
       "0                       8                   8       0.752196   \n",
       "10                      8                   8       0.432170   \n",
       "9                       8                   8       0.224303   \n",
       "7                       8                   8       0.219563   \n",
       "5                       8                   8       0.633894   \n",
       "4                       8                   8       0.491963   \n",
       "3                       8                   8       0.887414   \n",
       "2                       8                   8       0.362478   \n",
       "12                      8                   8       0.421572   \n",
       "23                      8                   8       0.242504   \n",
       "\n",
       "                                      param_estimator  \\\n",
       "11  XGBClassifier(base_score=None, booster=None, c...   \n",
       "21  XGBClassifier(base_score=None, booster=None, c...   \n",
       "6   XGBClassifier(base_score=None, booster=None, c...   \n",
       "8   XGBClassifier(base_score=None, booster=None, c...   \n",
       "13  XGBClassifier(base_score=None, booster=None, c...   \n",
       "22  XGBClassifier(base_score=None, booster=None, c...   \n",
       "1   XGBClassifier(base_score=None, booster=None, c...   \n",
       "20  XGBClassifier(base_score=None, booster=None, c...   \n",
       "19  XGBClassifier(base_score=None, booster=None, c...   \n",
       "18  XGBClassifier(base_score=None, booster=None, c...   \n",
       "17  XGBClassifier(base_score=None, booster=None, c...   \n",
       "16  XGBClassifier(base_score=None, booster=None, c...   \n",
       "15  XGBClassifier(base_score=None, booster=None, c...   \n",
       "14  XGBClassifier(base_score=None, booster=None, c...   \n",
       "0   XGBClassifier(base_score=None, booster=None, c...   \n",
       "10  XGBClassifier(base_score=None, booster=None, c...   \n",
       "9   XGBClassifier(base_score=None, booster=None, c...   \n",
       "7   XGBClassifier(base_score=None, booster=None, c...   \n",
       "5   XGBClassifier(base_score=None, booster=None, c...   \n",
       "4   XGBClassifier(base_score=None, booster=None, c...   \n",
       "3   XGBClassifier(base_score=None, booster=None, c...   \n",
       "2   XGBClassifier(base_score=None, booster=None, c...   \n",
       "12  XGBClassifier(base_score=None, booster=None, c...   \n",
       "23  XGBClassifier(base_score=None, booster=None, c...   \n",
       "\n",
       "                 param_preprocessor__text__vectorizer  \\\n",
       "11  CountVectorizer(ngram_range=(1, 3),\\n         ...   \n",
       "21  CountVectorizer(ngram_range=(1, 2),\\n         ...   \n",
       "6   CountVectorizer(ngram_range=(2, 2),\\n         ...   \n",
       "8   CountVectorizer(ngram_range=(2, 3),\\n         ...   \n",
       "13  CountVectorizer(ngram_range=(2, 2),\\n         ...   \n",
       "22  CountVectorizer(stop_words=['de', 'la', 'que',...   \n",
       "1   CountVectorizer(ngram_range=(2, 2),\\n         ...   \n",
       "20  CountVectorizer(ngram_range=(1, 3),\\n         ...   \n",
       "19  CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "18  CountVectorizer(ngram_range=(4, 4),\\n         ...   \n",
       "17  CountVectorizer(stop_words=['de', 'la', 'que',...   \n",
       "16  CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "15  CountVectorizer(ngram_range=(4, 4),\\n         ...   \n",
       "14  CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "0   CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "10  CountVectorizer(ngram_range=(1, 2),\\n         ...   \n",
       "9   CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "7   CountVectorizer(ngram_range=(1, 3),\\n         ...   \n",
       "5   CountVectorizer(ngram_range=(2, 3),\\n         ...   \n",
       "4   CountVectorizer(ngram_range=(1, 3),\\n         ...   \n",
       "3   CountVectorizer(ngram_range=(1, 2),\\n         ...   \n",
       "2   CountVectorizer(stop_words=['de', 'la', 'que',...   \n",
       "12  CountVectorizer(stop_words=['de', 'la', 'que',...   \n",
       "23  CountVectorizer(ngram_range=(4, 4),\\n         ...   \n",
       "\n",
       "                   param_preprocessor__text__tfidf  \\\n",
       "11  TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "21                     TfidfTransformer(norm='l1')   \n",
       "6   TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "8                      TfidfTransformer(norm='l1')   \n",
       "13                              TfidfTransformer()   \n",
       "22             TfidfTransformer(sublinear_tf=True)   \n",
       "1                      TfidfTransformer(norm='l1')   \n",
       "20                     TfidfTransformer(norm='l1')   \n",
       "19             TfidfTransformer(sublinear_tf=True)   \n",
       "18  TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "17                              TfidfTransformer()   \n",
       "16  TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "15             TfidfTransformer(sublinear_tf=True)   \n",
       "14             TfidfTransformer(sublinear_tf=True)   \n",
       "0   TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "10                              TfidfTransformer()   \n",
       "9                      TfidfTransformer(norm='l1')   \n",
       "7                      TfidfTransformer(norm='l1')   \n",
       "5              TfidfTransformer(sublinear_tf=True)   \n",
       "4              TfidfTransformer(sublinear_tf=True)   \n",
       "3              TfidfTransformer(sublinear_tf=True)   \n",
       "2                               TfidfTransformer()   \n",
       "12             TfidfTransformer(sublinear_tf=True)   \n",
       "23                     TfidfTransformer(norm='l1')   \n",
       "\n",
       "   param_preprocessor__numerical__scaler  \\\n",
       "11                      StandardScaler()   \n",
       "21                      StandardScaler()   \n",
       "6                         MinMaxScaler()   \n",
       "8                           Normalizer()   \n",
       "13                        MinMaxScaler()   \n",
       "22                        RobustScaler()   \n",
       "1                         MinMaxScaler()   \n",
       "20                      StandardScaler()   \n",
       "19                        RobustScaler()   \n",
       "18                        MinMaxScaler()   \n",
       "17                      StandardScaler()   \n",
       "16                          Normalizer()   \n",
       "15                      StandardScaler()   \n",
       "14                          Normalizer()   \n",
       "0                         RobustScaler()   \n",
       "10                      StandardScaler()   \n",
       "9                         RobustScaler()   \n",
       "7                       StandardScaler()   \n",
       "5                           Normalizer()   \n",
       "4                         MinMaxScaler()   \n",
       "3                         MinMaxScaler()   \n",
       "2                       StandardScaler()   \n",
       "12                      StandardScaler()   \n",
       "23                        MinMaxScaler()   \n",
       "\n",
       "     param_preprocessor__numerical__imputer  \\\n",
       "11  SimpleImputer(strategy='most_frequent')   \n",
       "21                             KNNImputer()   \n",
       "6   SimpleImputer(strategy='most_frequent')   \n",
       "8   SimpleImputer(strategy='most_frequent')   \n",
       "13  SimpleImputer(strategy='most_frequent')   \n",
       "22                             KNNImputer()   \n",
       "1          SimpleImputer(strategy='median')   \n",
       "20                          SimpleImputer()   \n",
       "19  SimpleImputer(strategy='most_frequent')   \n",
       "18                          SimpleImputer()   \n",
       "17                             KNNImputer()   \n",
       "16                          SimpleImputer()   \n",
       "15  SimpleImputer(strategy='most_frequent')   \n",
       "14         SimpleImputer(strategy='median')   \n",
       "0                           SimpleImputer()   \n",
       "10         SimpleImputer(strategy='median')   \n",
       "9                              KNNImputer()   \n",
       "7          SimpleImputer(strategy='median')   \n",
       "5          SimpleImputer(strategy='median')   \n",
       "4          SimpleImputer(strategy='median')   \n",
       "3   SimpleImputer(strategy='most_frequent')   \n",
       "2                              KNNImputer()   \n",
       "12         SimpleImputer(strategy='median')   \n",
       "23         SimpleImputer(strategy='median')   \n",
       "\n",
       "                     param_feature_selector  mean_test_Accuracy  \\\n",
       "11       SelectFromModel(estimator=Ridge())            0.328924   \n",
       "21                      VarianceThreshold()            0.304233   \n",
       "6   SelectFromModel(estimator=ElasticNet())            0.276896   \n",
       "8                       VarianceThreshold()            0.217813   \n",
       "13  SelectFromModel(estimator=ElasticNet())            0.163139   \n",
       "22                      VarianceThreshold()            0.137566   \n",
       "1   SelectFromModel(estimator=ElasticNet())            0.121693   \n",
       "20                      VarianceThreshold()            0.111111   \n",
       "19                      VarianceThreshold()            0.111111   \n",
       "18       SelectFromModel(estimator=Ridge())            0.111111   \n",
       "17                      VarianceThreshold()            0.111111   \n",
       "16                      VarianceThreshold()            0.111111   \n",
       "15                      VarianceThreshold()            0.111111   \n",
       "14       SelectFromModel(estimator=Ridge())            0.111111   \n",
       "0   SelectFromModel(estimator=ElasticNet())            0.111111   \n",
       "10       SelectFromModel(estimator=Ridge())            0.111111   \n",
       "9                       VarianceThreshold()            0.111111   \n",
       "7   SelectFromModel(estimator=ElasticNet())            0.111111   \n",
       "5        SelectFromModel(estimator=Ridge())            0.111111   \n",
       "4        SelectFromModel(estimator=Ridge())            0.111111   \n",
       "3        SelectFromModel(estimator=Ridge())            0.111111   \n",
       "2                       VarianceThreshold()            0.111111   \n",
       "12                      VarianceThreshold()            0.111111   \n",
       "23       SelectFromModel(estimator=Ridge())            0.111111   \n",
       "\n",
       "    mean_test_Weighted_F1  \n",
       "11               0.326579  \n",
       "21               0.278544  \n",
       "6                0.182589  \n",
       "8                0.116733  \n",
       "13               0.058546  \n",
       "22               0.044631  \n",
       "1                0.030075  \n",
       "20               0.022321  \n",
       "19               0.022321  \n",
       "18               0.022321  \n",
       "17               0.022321  \n",
       "16               0.022321  \n",
       "15               0.022321  \n",
       "14               0.022321  \n",
       "0                0.022321  \n",
       "10               0.022321  \n",
       "9                0.022321  \n",
       "7                0.022321  \n",
       "5                0.022321  \n",
       "4                0.022321  \n",
       "3                0.022321  \n",
       "2                0.022321  \n",
       "12               0.022321  \n",
       "23               0.022321  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.cv_results.sort_values(by=[\"rank_test_Weighted_F1\"])[[\n",
    "    'rank_test_Weighted_F1',\n",
    "    'rank_test_Accuracy',\n",
    "    'mean_fit_time',\n",
    "    'param_estimator',\n",
    "    'param_preprocessor__text__vectorizer',\n",
    "    'param_preprocessor__text__tfidf',\n",
    "    'param_preprocessor__numerical__scaler',\n",
    "    'param_preprocessor__numerical__imputer', 'param_feature_selector',\n",
    "    'mean_test_Accuracy',\n",
    "    'mean_test_Weighted_F1'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_Weighted_F1</th>\n",
       "      <th>rank_test_Accuracy</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>param_estimator</th>\n",
       "      <th>param_preprocessor__text__vectorizer</th>\n",
       "      <th>param_preprocessor__text__tfidf</th>\n",
       "      <th>param_preprocessor__numerical__scaler</th>\n",
       "      <th>param_preprocessor__numerical__imputer</th>\n",
       "      <th>param_feature_selector</th>\n",
       "      <th>mean_test_Accuracy</th>\n",
       "      <th>mean_test_Weighted_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.277985</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.328924</td>\n",
       "      <td>0.326579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.567838</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.304233</td>\n",
       "      <td>0.278544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.078664</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>0.276896</td>\n",
       "      <td>0.182589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.778019</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.217813</td>\n",
       "      <td>0.116733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.126407</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>0.163139</td>\n",
       "      <td>0.058546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.229574</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(stop_words=['de', 'la', 'que',...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.137566</td>\n",
       "      <td>0.044631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.123770</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>0.121693</td>\n",
       "      <td>0.030075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.763731</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.181924</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.201903</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(4, 4),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.414289</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(stop_words=['de', 'la', 'que',...</td>\n",
       "      <td>TfidfTransformer()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.033627</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2.591169</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(4, 4),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.716587</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.752196</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1', sublinear_tf=True)</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.432170</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.224303</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(3, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.219563</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=ElasticNet())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.633894</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(2, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.491963</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.887414</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(1, 2),\\n         ...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.362478</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(stop_words=['de', 'la', 'que',...</td>\n",
       "      <td>TfidfTransformer()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.421572</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(stop_words=['de', 'la', 'que',...</td>\n",
       "      <td>TfidfTransformer(sublinear_tf=True)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>VarianceThreshold()</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.242504</td>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>CountVectorizer(ngram_range=(4, 4),\\n         ...</td>\n",
       "      <td>TfidfTransformer(norm='l1')</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>SimpleImputer(strategy='median')</td>\n",
       "      <td>SelectFromModel(estimator=Ridge())</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank_test_Weighted_F1  rank_test_Accuracy  mean_fit_time  \\\n",
       "11                      1                   1       0.277985   \n",
       "21                      2                   2       1.567838   \n",
       "6                       3                   3       0.078664   \n",
       "8                       4                   4       0.778019   \n",
       "13                      5                   5       0.126407   \n",
       "22                      6                   6       0.229574   \n",
       "1                       7                   7       0.123770   \n",
       "20                      8                   8       0.763731   \n",
       "19                      8                   8       0.181924   \n",
       "18                      8                   8       0.201903   \n",
       "17                      8                   8       0.414289   \n",
       "16                      8                   8       0.033627   \n",
       "15                      8                   8       2.591169   \n",
       "14                      8                   8       0.716587   \n",
       "0                       8                   8       0.752196   \n",
       "10                      8                   8       0.432170   \n",
       "9                       8                   8       0.224303   \n",
       "7                       8                   8       0.219563   \n",
       "5                       8                   8       0.633894   \n",
       "4                       8                   8       0.491963   \n",
       "3                       8                   8       0.887414   \n",
       "2                       8                   8       0.362478   \n",
       "12                      8                   8       0.421572   \n",
       "23                      8                   8       0.242504   \n",
       "\n",
       "                                      param_estimator  \\\n",
       "11  XGBClassifier(base_score=None, booster=None, c...   \n",
       "21  XGBClassifier(base_score=None, booster=None, c...   \n",
       "6   XGBClassifier(base_score=None, booster=None, c...   \n",
       "8   XGBClassifier(base_score=None, booster=None, c...   \n",
       "13  XGBClassifier(base_score=None, booster=None, c...   \n",
       "22  XGBClassifier(base_score=None, booster=None, c...   \n",
       "1   XGBClassifier(base_score=None, booster=None, c...   \n",
       "20  XGBClassifier(base_score=None, booster=None, c...   \n",
       "19  XGBClassifier(base_score=None, booster=None, c...   \n",
       "18  XGBClassifier(base_score=None, booster=None, c...   \n",
       "17  XGBClassifier(base_score=None, booster=None, c...   \n",
       "16  XGBClassifier(base_score=None, booster=None, c...   \n",
       "15  XGBClassifier(base_score=None, booster=None, c...   \n",
       "14  XGBClassifier(base_score=None, booster=None, c...   \n",
       "0   XGBClassifier(base_score=None, booster=None, c...   \n",
       "10  XGBClassifier(base_score=None, booster=None, c...   \n",
       "9   XGBClassifier(base_score=None, booster=None, c...   \n",
       "7   XGBClassifier(base_score=None, booster=None, c...   \n",
       "5   XGBClassifier(base_score=None, booster=None, c...   \n",
       "4   XGBClassifier(base_score=None, booster=None, c...   \n",
       "3   XGBClassifier(base_score=None, booster=None, c...   \n",
       "2   XGBClassifier(base_score=None, booster=None, c...   \n",
       "12  XGBClassifier(base_score=None, booster=None, c...   \n",
       "23  XGBClassifier(base_score=None, booster=None, c...   \n",
       "\n",
       "                 param_preprocessor__text__vectorizer  \\\n",
       "11  CountVectorizer(ngram_range=(1, 3),\\n         ...   \n",
       "21  CountVectorizer(ngram_range=(1, 2),\\n         ...   \n",
       "6   CountVectorizer(ngram_range=(2, 2),\\n         ...   \n",
       "8   CountVectorizer(ngram_range=(2, 3),\\n         ...   \n",
       "13  CountVectorizer(ngram_range=(2, 2),\\n         ...   \n",
       "22  CountVectorizer(stop_words=['de', 'la', 'que',...   \n",
       "1   CountVectorizer(ngram_range=(2, 2),\\n         ...   \n",
       "20  CountVectorizer(ngram_range=(1, 3),\\n         ...   \n",
       "19  CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "18  CountVectorizer(ngram_range=(4, 4),\\n         ...   \n",
       "17  CountVectorizer(stop_words=['de', 'la', 'que',...   \n",
       "16  CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "15  CountVectorizer(ngram_range=(4, 4),\\n         ...   \n",
       "14  CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "0   CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "10  CountVectorizer(ngram_range=(1, 2),\\n         ...   \n",
       "9   CountVectorizer(ngram_range=(3, 3),\\n         ...   \n",
       "7   CountVectorizer(ngram_range=(1, 3),\\n         ...   \n",
       "5   CountVectorizer(ngram_range=(2, 3),\\n         ...   \n",
       "4   CountVectorizer(ngram_range=(1, 3),\\n         ...   \n",
       "3   CountVectorizer(ngram_range=(1, 2),\\n         ...   \n",
       "2   CountVectorizer(stop_words=['de', 'la', 'que',...   \n",
       "12  CountVectorizer(stop_words=['de', 'la', 'que',...   \n",
       "23  CountVectorizer(ngram_range=(4, 4),\\n         ...   \n",
       "\n",
       "                   param_preprocessor__text__tfidf  \\\n",
       "11  TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "21                     TfidfTransformer(norm='l1')   \n",
       "6   TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "8                      TfidfTransformer(norm='l1')   \n",
       "13                              TfidfTransformer()   \n",
       "22             TfidfTransformer(sublinear_tf=True)   \n",
       "1                      TfidfTransformer(norm='l1')   \n",
       "20                     TfidfTransformer(norm='l1')   \n",
       "19             TfidfTransformer(sublinear_tf=True)   \n",
       "18  TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "17                              TfidfTransformer()   \n",
       "16  TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "15             TfidfTransformer(sublinear_tf=True)   \n",
       "14             TfidfTransformer(sublinear_tf=True)   \n",
       "0   TfidfTransformer(norm='l1', sublinear_tf=True)   \n",
       "10                              TfidfTransformer()   \n",
       "9                      TfidfTransformer(norm='l1')   \n",
       "7                      TfidfTransformer(norm='l1')   \n",
       "5              TfidfTransformer(sublinear_tf=True)   \n",
       "4              TfidfTransformer(sublinear_tf=True)   \n",
       "3              TfidfTransformer(sublinear_tf=True)   \n",
       "2                               TfidfTransformer()   \n",
       "12             TfidfTransformer(sublinear_tf=True)   \n",
       "23                     TfidfTransformer(norm='l1')   \n",
       "\n",
       "   param_preprocessor__numerical__scaler  \\\n",
       "11                      StandardScaler()   \n",
       "21                      StandardScaler()   \n",
       "6                         MinMaxScaler()   \n",
       "8                           Normalizer()   \n",
       "13                        MinMaxScaler()   \n",
       "22                        RobustScaler()   \n",
       "1                         MinMaxScaler()   \n",
       "20                      StandardScaler()   \n",
       "19                        RobustScaler()   \n",
       "18                        MinMaxScaler()   \n",
       "17                      StandardScaler()   \n",
       "16                          Normalizer()   \n",
       "15                      StandardScaler()   \n",
       "14                          Normalizer()   \n",
       "0                         RobustScaler()   \n",
       "10                      StandardScaler()   \n",
       "9                         RobustScaler()   \n",
       "7                       StandardScaler()   \n",
       "5                           Normalizer()   \n",
       "4                         MinMaxScaler()   \n",
       "3                         MinMaxScaler()   \n",
       "2                       StandardScaler()   \n",
       "12                      StandardScaler()   \n",
       "23                        MinMaxScaler()   \n",
       "\n",
       "     param_preprocessor__numerical__imputer  \\\n",
       "11  SimpleImputer(strategy='most_frequent')   \n",
       "21                             KNNImputer()   \n",
       "6   SimpleImputer(strategy='most_frequent')   \n",
       "8   SimpleImputer(strategy='most_frequent')   \n",
       "13  SimpleImputer(strategy='most_frequent')   \n",
       "22                             KNNImputer()   \n",
       "1          SimpleImputer(strategy='median')   \n",
       "20                          SimpleImputer()   \n",
       "19  SimpleImputer(strategy='most_frequent')   \n",
       "18                          SimpleImputer()   \n",
       "17                             KNNImputer()   \n",
       "16                          SimpleImputer()   \n",
       "15  SimpleImputer(strategy='most_frequent')   \n",
       "14         SimpleImputer(strategy='median')   \n",
       "0                           SimpleImputer()   \n",
       "10         SimpleImputer(strategy='median')   \n",
       "9                              KNNImputer()   \n",
       "7          SimpleImputer(strategy='median')   \n",
       "5          SimpleImputer(strategy='median')   \n",
       "4          SimpleImputer(strategy='median')   \n",
       "3   SimpleImputer(strategy='most_frequent')   \n",
       "2                              KNNImputer()   \n",
       "12         SimpleImputer(strategy='median')   \n",
       "23         SimpleImputer(strategy='median')   \n",
       "\n",
       "                     param_feature_selector  mean_test_Accuracy  \\\n",
       "11       SelectFromModel(estimator=Ridge())            0.328924   \n",
       "21                      VarianceThreshold()            0.304233   \n",
       "6   SelectFromModel(estimator=ElasticNet())            0.276896   \n",
       "8                       VarianceThreshold()            0.217813   \n",
       "13  SelectFromModel(estimator=ElasticNet())            0.163139   \n",
       "22                      VarianceThreshold()            0.137566   \n",
       "1   SelectFromModel(estimator=ElasticNet())            0.121693   \n",
       "20                      VarianceThreshold()            0.111111   \n",
       "19                      VarianceThreshold()            0.111111   \n",
       "18       SelectFromModel(estimator=Ridge())            0.111111   \n",
       "17                      VarianceThreshold()            0.111111   \n",
       "16                      VarianceThreshold()            0.111111   \n",
       "15                      VarianceThreshold()            0.111111   \n",
       "14       SelectFromModel(estimator=Ridge())            0.111111   \n",
       "0   SelectFromModel(estimator=ElasticNet())            0.111111   \n",
       "10       SelectFromModel(estimator=Ridge())            0.111111   \n",
       "9                       VarianceThreshold()            0.111111   \n",
       "7   SelectFromModel(estimator=ElasticNet())            0.111111   \n",
       "5        SelectFromModel(estimator=Ridge())            0.111111   \n",
       "4        SelectFromModel(estimator=Ridge())            0.111111   \n",
       "3        SelectFromModel(estimator=Ridge())            0.111111   \n",
       "2                       VarianceThreshold()            0.111111   \n",
       "12                      VarianceThreshold()            0.111111   \n",
       "23       SelectFromModel(estimator=Ridge())            0.111111   \n",
       "\n",
       "    mean_test_Weighted_F1  \n",
       "11               0.326579  \n",
       "21               0.278544  \n",
       "6                0.182589  \n",
       "8                0.116733  \n",
       "13               0.058546  \n",
       "22               0.044631  \n",
       "1                0.030075  \n",
       "20               0.022321  \n",
       "19               0.022321  \n",
       "18               0.022321  \n",
       "17               0.022321  \n",
       "16               0.022321  \n",
       "15               0.022321  \n",
       "14               0.022321  \n",
       "0                0.022321  \n",
       "10               0.022321  \n",
       "9                0.022321  \n",
       "7                0.022321  \n",
       "5                0.022321  \n",
       "4                0.022321  \n",
       "3                0.022321  \n",
       "2                0.022321  \n",
       "12               0.022321  \n",
       "23               0.022321  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.cv_results.sort_values(by=[\"rank_test_Weighted_F1\"])[[\n",
    "    'rank_test_Weighted_F1',\n",
    "    'rank_test_Accuracy',\n",
    "    'mean_fit_time',\n",
    "    'param_estimator',\n",
    "    'param_preprocessor__text__vectorizer',\n",
    "    'param_preprocessor__text__tfidf',\n",
    "    'param_preprocessor__numerical__scaler',\n",
    "    'param_preprocessor__numerical__imputer', 'param_feature_selector',\n",
    "    'mean_test_Accuracy',\n",
    "    'mean_test_Weighted_F1'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model using a sample-weighting mechanism to try to compensate for the dataset imbalance\n",
    "if retrain_with_class_weight:\n",
    "    sample_weights = compute_sample_weight(\n",
    "        class_weight='balanced',\n",
    "        y=y_train,\n",
    "    )\n",
    "\n",
    "    best_model.fit(X_train, y_train, estimator__sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>micro_f1_score</th>\n",
       "      <th>macro_f1_score</th>\n",
       "      <th>weighted_f1_score</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>confusion_matrix_normalized</th>\n",
       "      <th>classification_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.77771</td>\n",
       "      <td>0.242781</td>\n",
       "      <td>0.378454</td>\n",
       "      <td>0.242781</td>\n",
       "      <td>0.180297</td>\n",
       "      <td>0.28352</td>\n",
       "      <td>[[347, 3, 93, 105, 65, 126, 11, 17, 15], [0, 7...</td>\n",
       "      <td>[[0.0030944015409584617, 2.6752751074568835e-0...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROC_AUC  accuracy balanced_accuracy micro_f1_score macro_f1_score  \\\n",
       "0  0.77771  0.242781          0.378454       0.242781       0.180297   \n",
       "\n",
       "  weighted_f1_score                                   confusion_matrix  \\\n",
       "0           0.28352  [[347, 3, 93, 105, 65, 126, 11, 17, 15], [0, 7...   \n",
       "\n",
       "                         confusion_matrix_normalized  \\\n",
       "0  [[0.0030944015409584617, 2.6752751074568835e-0...   \n",
       "\n",
       "                               classification_report  \n",
       "0                precision    recall  f1-score   ...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = pipeline.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtYAAAJNCAYAAAAVsTJGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAACmtElEQVR4nOzdd3xT1f/H8VeS7j3Ze12WgIIM4auouBX3nrj3FhH33gv3RtGfuHArigM3MmVzoMwyS1u6d3J/fyS0oLQESJumvJ+PRx5NTm5uPyc349zP/dwTh23biIiIiIjInnEGOwARERERkaZAA2sRERERkQDQwFpEREREJAA0sBYRERERCQANrEVEREREAkADaxERERGRAAgLdgD1JT+70N60JjvYYTQcjyfYEYiIiOz1ug3onA2kBzuO+mKX/2rjTG6Q/+UI3+c74MgG+WcB0mQH1pvWZHPNgfcEO4wG4ykpCXYIDc/hCHYEDcuxFx5gsveyHca9cRt73MGOQCSgpng+Wh3sGOqVMxk75+QG+VeOFkvTGuQfBVCTHViLiIiISOB5aJikRyimGkIxZhERERGRRkcZaxERERHxi22Du4HK9EJxkKqMtYiIiIhIAGhgLSIiIiISAKGYZRcRERGRoLDxYAc7iEZLGWsRERERkQBQxlpERERE/GLTcNPthSJlrEVEREREAkAZaxERERHxm9tWjXVtlLEWEREREQkAZaxFRERExC/eGmtlrGujgbWIiIiIhBzLst4EjgWyjDG9fW2PA8cBFcByYJQxJs93323ARYAbuNYY852v/UjgWcAFvG6MecTX3hGYCKQCs4BzjTEVdcWkUhARERER8ZONu4EufhgPHPmvtilAb2NMH2ApcBuAZVk9gTOAXr7HvGhZlsuyLBfwAnAU0BM407cswKPA08aYLsAWvIPyOmlgLSIiIiIhxxjzK5D7r7bvjTFVvpvTgDa+68cDE40x5caYlUAGMNB3yTDGrPBloycCx1uW5QAOAT72Pf5t4ISdxaRSEBERERHxS4jVWF8IfOC73hrvQHurtb42gMx/tQ/CW/6Rt80gfdvla6WBtYiIiIg0Ops3b04bNmzYzG2aXjXGvOrPYy3Luh2oAt6rl+BqoYG1iIiIiPitoeaxbpGenm2MGbCrj7Ms6wK8JzUeaozZGuw6oO02i7XxtVFLew6QZFlWmC9rve3ytVKNtYiIiIg0Cb4ZPkYDI40xJdvc9QVwhmVZkb7ZProC04EZQFfLsjpalhWB9wTHL3wD8p+BU3yPPx/4fGf/XxlrEREREfGLt8a6cbAs631gOJBmWdZa4G68s4BEAlMsywKYZoy53Biz0LKsD4FFeEtErjLGuH3ruRr4Du90e28aYxb6/sWtwETLsh4A5gBv7Cwmh91Ef5Zy6eyV9jUH3hPsMBqMp6Rk5ws1NQ5HsCNoWI698ACT3Vg+vhvI3riNPe5gRyASUFM8H80Cdrl8IVRUVMy1szcf1SD/q1Xr9SH3XO6Fn+IiIiIiIoGnUhARERER8ZufP96yV9LAeheFR3h4/P0FhEfYuMJsfp+cyrvP1pxMevmdKzn8lCxO6jsIgEtvX0WfQfkAREZ7SEqt5NT9BgYl9kAKj/Tw5KSM6ufht6+TmPBEi2CHVS9OuGgzR52Vg8MB3/5fCp++3ozzbtnAkMPzsW3Iyw7niRvakbspPNih7pYbn1jNoBH55GWHcdkI749NxSdVMfbFlTRvW8GmzAgevKIjRflh9BlSyD1vLGdjZiQAf3ybxHvPtAxm+HssNqGKG57IpINVhm3DUze1o/9BBRx1Vi75uS4A3nqkFTN+SghypLtvR9v44jvWMnhEPpWVDjasjuTJG9tTXOD9SujYo4RrH8kkNs6Nx4ZrjulOZXnoHuC88ak1DBpR6O3/IRYA/zs2j3Nv2kjbruVce3RXls2LCXKUgbOj/o59eRVtOpcDEJvgprjAxZWHWcEMM2B21N9zbtrIUWflkJ/rfU2/9XDLkH4PS+io14G1ZVknAJ8CPYwxS3xtbmC+b5E1xpiRvvargeuBzkC6MSbb1+7A+/vtRwMlwAXGmNn1GXddKiscjDm3F2UlLlxhHp6YuJCZvySx5J94uvYuIi6xarvlX32wQ/X1keduoHPP4gaOuH5UljsYfWpn3/Ng89RnGcz4KZ4ls2ODHVpAtbdKOeqsHK49phuVlQ4eem85f/+QyMcvNeOdx70DyuMv3Mw5N2xk3Ji2O1lb4/T9Ryl8MT6dW55ZVd122lUbmfNHPB++0ILTrtrI6Vdt4o2HvPPiL5gex10XdAlStIF3xX3rmPlzAg9c2pGwcA+R0R76HwSfvpbOx680C3Z4AbGjbTz71wTefLg1HreDi8au44yrvdvY6bIZPW4Vj1/bgRWLY4hPqsJdGdrnM3z/QQpfvJXGLc/W/AbEqiVR3HdxB659dG0QI6sfO+rvQ5d3qL5+6V3rKS4M3R2lf9tRf8H3Hn65abyHGxMbcCthXav6fmedCfzu+7tVqTGmn+8ycpv2P4ARwOp/reMovFOidAUuBV6qx3j94KCsxJvFCguzCQu3sW1wOm0uGrOaNx5tX+sjDzoum6lfpTVUoPVsm+ch3Mblex6amnZdy1kyJ4byMicet4N50+IYelQeJUWu6mWiYjwh3fcFf8dTmOfarm3I4fn88FEqAD98lMqQI/KCEFn9i4l3s8+gYia/nwJAVaWzOmvblOxoG8/+NQGP2ztgXjw7lrSWFQD0P6iAlYujWbHYm8EtzAvD4wntgfWCv+Mo3LL9ds3MiGLt8qggRVS/dtTfGjYHjszj58+SGzSm+lR3f0UaVr0NrC3LigOGARfhnROwTsaYOcaYVTu463jgHWOMbYyZhney7qAee3Y6bZ7/Yi7v/z2TOb8nYubGc9y5G5n2YzJbNkfs8DHNWpXTok05c/9KbOBo64/TafPiFMMH8xYy59c4zJymla0Gb1ar96Bi4pOriIzysP8hBaS3qgTggls38O6MhRxy4pbq7HVTkZxWRW6Wt7QlNyuM5LSaIzE9+hfz0veLeWBCBu27lQYrxIBo0a6c/Jwwbnp6DS98Z7j+8TVERntnqThu1GZemrKEG59c858jUU3NEadnM+Nn72HyNh3LsW0HD767jOe/XcypV2wMcnQSSL0HFbNlcxjrV0YGO5R6d9yobF76wXDjU03/PdzQPA10CUX1mbE+HphsjFkK5FiW1d/XHmVZ1kzLsqb5SkV2pjX//Q33nf5We33yeBxcPbIv5w7rT7e+RfTev4D/HZXDF+/UPrg66Nhsfp+cGvKZn215PA6uPMzi7P49sfqV0N4K7UHWjmRmRPHhC814+P+W8+B7y1mxMBqP790+/tGWnLN/L376NJmRozYHN9B65ajOyGfMj+HcQb254vAefP5WOne/sSK4oe0hlwu67FPCV++kcdURFmUlTk6/Oouv3klj1AE9ufJwi9yscC69a32wQ603Z16zAbfbwU+TvFl7V5hN7/2LePSajtx0osUBR+bTb2hBkKOUQDn4hDymfpYU7DDq3VdvpzJqSA+uPKwbuZvCufTupvselsalPgfWZwITfdcnUlMO0t7385RnAc9YltW5HmOoV8WFYcyblkCfwfm0bF/Gmz/OYfzU2URGe3jjx+3LwA86NpupXzaVMpDtFRe4mPtnHPsfXBjsUOrFdxNTufooi5tP7kpRvou1K7Y/fPzTpGSGHZ0fpOjqx5bsMFKaeTPzKc0qycvxHmYtKXJVlwDN+CkRV5hNQnLoZoKyN4SzeUN49dGW379Ooss+peRlh+PxOLBtB9++l4LVr2nOE3/YqTkMHFHAo1d3BLw7/Zs3hDP/7zgKtoRRXuZkxk8JdNmn6e00742cLpuhR+fzyxdJwQ6l3m3/Hk7F6qfXcKDYgBtHg1xCUb0MrC3LSgEOAV63LGsVcAtwmmVZDmPMOgBjzApgKrDvTlZX12+7N7jElEpi470DiYhIN/sOzSdjQRxnDxnABcP344Lh+1Fe6uSiQ/erfkybTqXEJbhZPCcuWGEHXGJKFbEJ3kPmEVEe9juwiMyMplmvmJjqHWCmt6pg6FH5/PxpEq06llffP+SIfDKXN63DqtOmJDLi1BwARpyaw1/fe0uYktMrwTfNktWvGKfTpmCLq7bVNHpbNoeTvT6CNp3LAOg3rJA1SyOrdyoADjgqn1Wm6b22BwzP59QrNnHPqE6Ul9V8Fcz6JYEO3UuJjPLgdNn0GVzEmqVNr/97o/3+V0hmRiTZG3ZcstiU7A3vYWmc6qva/xRggjHmsq0NlmX9AhxoWdY0Y0y5ZVlpwFDgsZ2s6wvgasuyJgKDgHxjzIZ6inunktMruPnxDJxOcDhtfvsmlek/130SyEHHZvPL16kQontfO5LSvJKbn12D0wlOJ/z6ZSJ//9A0pzK667VVxCdX4a5y8PztbSguCOPGJzJp07kcjwey1kUwbkybYIe528Y8v5I+QwpJTKni3RnzmfBkSz54vgW3v7ySI8/IIWutd7o9gP8ds4Vjz83G7XZQXubg4StrMp2h6oU7W3Prc6sJC7fZuCaCJ29sxxX3r6Nzz1JsGzatjWDcraE548tWO9rGZ1y9ifAIDw+/nwHAktmxjLutHUX5YUx6rRnPfb0E24bpPycw/afQPjdkzIur6TOkyNv/mYuY8GRzCreEceUD60hMreL+CStZvjCK288K2QOo29lRf797P5WDjm+aZSA76m+fIcV07rXNe3h06H5GN0aeED5hv77Vy0+aW5b1M/CoMWbyNm3XAicCaXhr0p3AM8aYN7a5fzTQAsgCvjHGXOybbu954Ei80+2NMsbM3FkM+knzvYB+0rzp00+aN336SXNpYpr6T5qXVcy1V29qmJ80t9qG3k+a10vG2hhz8A7axgHj6njMDu83xtjAVQENUERERER22dYaa9mxvTA9IiIiIiISeJpRXURERET8oox13ZSxFhEREREJAGWsRURERMQ/tgOPrYx1bZSxFhEREREJAA2sRUREREQCQKUgIiIiIuIXnbxYN2WsRUREREQCQBlrEREREfGbW3nZWumZEREREREJAGWsRURERMQvNmi6vTooYy0iIiIiEgDKWIuIiIiInxyaFaQOyliLiIiIiASAMtYiIiIi4hcbcNvKy9ZGz4yIiIiISAAoYy0iIiIifvMoL1srPTMiIiIiIgGgjLWIiIiI+MXWrCB1UsZaRERERCQAlLEWEREREb9pVpDa6ZkREREREQkADaxFRERERAJApSAiIiIi4hcb8OjkxVopYy0iIiIiEgBNN2Pt8eApKQl2FA3HsRfuPdp2sCNoWLY72BFIfdM2FpFGz4Fbedla6ZkREREREQmAppuxFhEREZGAstF0e3XRMyMiIiIiEgDKWIuIiIiI3zzKy9ZKz4yIiIiISAAoYy0iIiIifrFtB257L5yJzE/KWIuIiIiIBIAy1iIiIiLiFxs0j3Ud9MyIiIiIiASAMtYiIiIi4jeP5rGulZ4ZEREREZEAUMZaRERERPxi41CNdR30zIiIiIiIBIAG1iIiIiIiAaBSEBERERHxm34gpnbKWIuIiIiIBIAy1iIiIiLiFxvwKC9bKz0zIiIiIiIBoIy1iIiIiPjJgVs/EFMrPTMiIiIiIgGgjLWIiIiI+MVbY61ZQWqjjLWIiIiISAAoYy0iIiIiflONde30zIiIiIiIBIAy1iIiIiLiFxsHbuVla6VnJoAGDC/g9d+W8NYfiznt6k3BDifg2nQu48Xvl1RfJi2Zx4kXZ1Xff/JlWXy37h8SkquCGGX9aurb+N/U36btxqfW8MG8hbzykwl2KA1mb9vGsPf1eW/rrzQu9ZqxtizrBOBToIcxZomvzQ3M9y2yxhgz0td+NXA90BlIN8Zk+9q7A28B+wG3G2OeqM+Yd5fTaXPVQ+u47YxOZG8I57lvljHtu0TWLIsKdmgBs3Z5FFce3h3w9ve9WQv549skANJbVbDfgYVsWhsexAjr196wjbel/jbt/gJ8/0EKX7yVxi3PZgY7lAaxN27jva3Pe1t/g8IGj61ZQWpT3xnrM4HffX+3KjXG9PNdRm7T/gcwAlj9r3XkAtcCjXJAvZW1bwnrV0WwcU0kVZVOpn6exJAj8oMdVr3pN6yQDasjyVoXAcBl96zjjQdbYdtBDqwe7W3bWP1t2v0FWPB3HIVb9p6KwL1xG+9tfd7b+iuNT70NrC3LigOGARcBZ+xseWPMHGPMqh20ZxljZgCVAQ8ygFJbVLJ5fUT17ewN4aS1bNQh75Hhx+cx9bMkAIYcnk/2hnBWLIoOblD1bG/bxupv0+7v3mhv3MZ7W5/3tv4Ggw24cTbIJRTVZ9THA5ONMUuBHMuy+vvaoyzLmmlZ1jRfqYiEmLBwD4MPz+fXr5KIjPJwxjWbeOeJlsEOS0RERCSo6nNgfSYw0Xd9IjXlIO2NMQOAs4BnLMvqXI8xNJicjeGkt6qovp3WspLsDU2z3nj/gwvJmB9DXnY4LTuU06JdBS9NWcLb0xaS3rKSF74zJKc3vQzB3rSNQf1t6v3dG+2N23hv6/Pe1l9pfOplYG1ZVgpwCPC6ZVmrgFuA0yzLchhj1gEYY1YAU4F96yOGhmb+iaF1xwqaty0nLNzD8OPzmPZ9YrDDqhfDT9hSXQayakk0p/ftzfmDe3H+4F5s3hDOVUdYbNnc9D7I9qZtDOpvU+/v3mhv3MZ7W5/3tv4GhwOP7WyQSyiqr7NWTgEmGGMu29pgWdYvwIGWZU0zxpRblpUGDAUeq6cYGpTH7eCF21vz0P+twOmC7yemsHpp0zsLOTLazX4HFvLsrW2DHUqD21u28Vbqb9PuL8CYF1fTZ0gRiSlVvDtzEROebM5376cGO6x6szdu472tz3tbf6Xxcdj1MI2DZVk/A48aYyZv03YtcCKQBnjwZsufMca8sc39o4EWQBbwjTHmYsuyWgAzgQTf44qAnsaYgrpiWDpzuX3VwDEB71uj5dgLp75pylOQiIhISJri+WgWMCDYcdSXjaVL7XdXXdMg/+vmHt+F3HNZLxlrY8zBO2gbB4yr4zE7vN8YsxFoE9AARUREREQCbO+ZwFRERERE9lio1j83BD0zIiIiIiIBoIy1iIiIiPjFxoGbvfC8Lj8pYy0iIiIiEgDKWIuIiIiI31RjXTs9MyIiIiIiAaCMtYiIiIj4xQbcjSRjbVnWm8CxQJYxprevLQX4AOgArAJOM8ZssSzLATwLHA2UABcYY2b7HnM+cIdvtQ8YY972tfcHxgPRwDfAdcaYOn9Eo3E8MyIiIiIiu2Y8cOS/2sYAPxpjugI/+m4DHAV09V0uBV6C6oH43cAgYCBwt2VZyb7HvARcss3j/v2//kMDaxERERHxkwNPA112xhjzK5D7r+bjgbd9198GTtim/R1jjG2MmQYkWZbVEjgCmGKMyTXGbAGmAEf67kswxkzzZanf2WZdtdLAWkRERESaiubGmA2+6xuB5r7rrYHMbZZb62urq33tDtrrpBprEREREfGLbTdcjfXmzZvThg0bNnObpleNMa/6+3hjjG1ZVp010YGmgbWIiIiINDrp6enZxpgBu/iwTZZltTTGbPCVc2T52tcBbbdZro2vbR0w/F/tU33tbXawfJ1UCiIiIiIifvPYjga57KYvgPN9188HPt+m/TzLshyWZQ0G8n0lI98Bh1uWlew7afFw4DvffQWWZQ32zShy3jbrqpUy1iIiIiIScizLeh9vtjnNsqy1eGf3eAT40LKsi4DVwGm+xb/BO9VeBt7p9kYBGGNyLcu6H5jhW+4+Y8zWEyKvpGa6vW99lzppYC0iIiIiIccYc2Ytdx26g2Vt4Kpa1vMm8OYO2mcCvXclJg2sRURERMQvNg7cqiSulZ4ZEREREZEAUMZaRERERPy2BycWNnnKWIuIiIiIBIAy1iIiIiLiFxvwKC9bKz0zIiIiIiIBoIy1iIiIiPjNrRrrWiljLSIiIiISAMpYi4iIiIhfbPbo58abPGWsRUREREQCoOlmrB0OHJGRwY6iwdjl5cEOocE54+ODHULDqqwMdgQNzrbtYIfQoJx70WfWVu6CgmCH0LCcrmBH0PA87mBHIIFkg8dWXrY2emZERERERAKg6WasRURERCSgbMCNaqxro4y1iIiIiEgAKGMtIiIiIn7TrCC1U8ZaRERERCQANLAWEREREQkAlYKIiIiIiF+8PxCjvGxt9MyIiIiIiASAMtYiIiIi4jePpturlTLWIiIiIiIBoIy1iIiIiPjFBtyabq9WyliLiIiIiASAMtYiIiIi4ifNClIXPTMiIiIiIgGgjLWIiIiI+MW29ZPmdVHGWkREREQkAJSxFhERERG/aR7r2iljLSIiIiISAMpYi4iIiIjfVGNdO2WsRUREREQCQBlrEREREfGLrXms66RnRkREREQkADSwFhEREREJAJWCiIiIiIjfdPJi7TSw3kVpLcu55ckVJKVVgu3gm/fT+Xx8C257LoM2ncoAiEuooqggjKuO6Q1Ax+4lXPvgSmLiPHg8cO3xvaisCL2DBemtKrjl2TUkpVeBDd+8m8pnb6QDMPLCzYy8IAePG/7+MYE3HmgV5Gj3jNNpM+6Tf8jeFME9l/ei3+A8Lhq9EocTykpcPDmmKxvWRFcvP/TwbO54bgnXntyXZQvigxj5rguP8PD4B4sIj7BxuWx+n5zCu8+04fpHVtB1n2IcDpt1K6N48pbOlJW4SG9Vzk2PLycuwY3TZfPWY+2YMTUp2N3wW23v4U49irnmwVVERNq4q+D5uzqwdG4cgw/bwvk3rsXjceCuglfub8/CmaG1jcH7mn724znkZEVyz+W96Dsoj4tGryAs3CZjURzP3N4Nj9vBPgPzuOuFRWxcGwXAn1NSef/F9kGOfs85nTbPTV5KzoZw7jq/Ezc9vYY+Q4opLvR+Fj9xfTtWLIzeyVoarxufWM2gEfnkZYdx2YieAFx8x1oGj8instLBhtWRPHlje4oLwohPquLOV1fQrW8JUz5K5YU72gY5+j1T23fTebdsYMgRBdg25GWH8cT17cjdFB7scKWJq9eBtWVZJwCfAj2MMUt8bW5gvm+RNcaYkb72q4Hrgc5AujEm+1/r2h/4CzjDGPNxfcZdF0+Vg9cebEfGwliiY9089+UC5vyeyMPXdKle5pLb11Bc4ALA6bIZ/fRyHruxMysXxxCfVIm7KjT39NxVDl69rxUZ82OIjnXz/OSlzP41nuT0Kg44ooArRnSjssJJYmplsEPdY8eft541y2OIiasC4Kp7Mrjvyp5krojhmLM2cOYVmTx1WzcAomOrOP689Sz5J/QGWwCVFQ7GnN2DshIXrjAPT3y4iJlTE3n1gXaUFHk/Ii65fTXHnbeJj15uxZlXreO3b1L5+r3mtOtSwn1vGi44cN8g98J/tb2HL7otk/eebc3MX5LYf3geF4/JZPSZPfjnjwSmTekNOOjYvYSxz2dwyYg+we7GLjv+vHVkroghJs6Nw2Fz4yOGsaP2Yd2qGM65ZhUjTtjE95+0AGDhrETuubxXkCMOrBMuziZzWRQxce7qttfub8nvXycFL6gA+v6jFL4Yn84tz6yqbpv9awJvPtwaj9vBRWPXccbVm3jjodZUlDt4+/FWdLBK6dC9LHhBB0ht300fv9SMdx5vCcDxF23mnBs2MW5MmyBHG/ps9AMxdanvtOmZwO++v1uVGmP6+S4jt2n/AxgBrP73SizLcgGPAt/XZ7D+yN0cQcbCWABKi11kZkST2qJimyVsDjw6l6lfpgLQ/3/5rFwSw8rFMQAU5oXj8YTmCzI3K5yM+d5+ePseRVrLSo49L5sPnm9WnYXPzwntjEBa83IGDs/lu4+bb9PqqP5Cjo2rIicrovqe865bw0evtaGiPDS3KzgoK/HuCIaF2YSF2di2o3pQDTaRUR7vpylg2zXPRUy8m5xNETtYZ+NV63vYpmYbx7vJ8WW2vM+Nd9tGRbux7aCEvUdSm5ez/0G5fPeRd+Acn1RJVaWTdau87+c5fyYz9PDsulYR0tJaVjDw0AK+/b+UYIdSbxb8HU9hnmu7ttm/JuBxe1+7i2fHktbS+11VXupi4Yw4KspD78jpjtT23VRSVPN8REV7QvK9K6Gn3jLWlmXFAcOAg4EvgbvrWt4YM8f3uB3dfQ3wCbB/YKPcM81bl9O5Zwnmn7jqtt4DC9mSHcb6Vd7DqK07lmHb8ODbS0hMqWLqV6l8/ErLYIUcMM3bVNC5dylLZsdw8Z3r6T2omAtu3UhFuYPX7mvF0rkxwQ5xt102dgVvPN6R6Niq6rZnbu/Cfa8upKLcSUmRixtO6wtA555FpLUoZ8YvKZxy0dpghbzHnE6bcV8soFX7Mr56tzlmrvc1fcNjy9l/eB5rlkXz2oPtAHj32dY8+M4SRp63kcgYD2PP7RHM0PfItu/hl+9rz4NvGy4Zm4nDaXPjKT2rlzvg8FxGjV5LUmold13YLYgR757Lxi7nzSc6Eh3r3XEo2BKOy2XTtXchyxbEM+yIbNJbllcv371fAc9/NpvcrAhef6wjazJigxV6QFx+73pef6AlMXGe7dovGLORs2/YxD+/x/HmQy1DskTPX0ecns0vXyYHO4x6t+13E8AFt25gxKlbKC5wMfqUzkGOrulQjXXt6vNT5HhgsjFmKZBjWVZ/X3uUZVkzLcua5isVqZNlWa2BE4GX6i/UXRcV4+aOl5bxyv3tttsrHn5cTbYawBVm02tAIY9e35mbTu3B0MNz6XdAfjBCDpioGDd3vr6Kl+9qRUmRC5cL4pOquO7YLrx+fytuf2U11enNEDNweC55ueFkLIzbrv3EC9Zz16W9OPeggXw/qTmX3LYSh8Pm0jEreO3RjkGKNnA8HgdXH7sP5x6wL936FNG+WwkAT4/uzDmD9yNzeTQHHpsLwPCROfzwcTrnDt2Puy60uOXJDByO0Nve/34PH3tOFq880I5zh/bjlQfaccMjK6uX/fP7FC4Z0Yd7L+vKeTeuC2LUu27g8BzyciLIWLhtqZKDR27qziVjVvD0h3MoLXbh9mU2MxbGccEhA7n6hP344t1W3Pn8ouAEHiCDRhSQlx1WndHc6q2HW3Lx/yyuPbor8UluTrsqK0gR1r8zr9mA2+3gp0lNN2MP//1uAhj/aEvOGdCTnyYlMfLCpntURhqP+hxYnwlM9F2fSE05SHtjzADgLOAZy7J2tgv5DHCrMcazk+UajCvMw50vLePnz1P547uaDyqny2bokbn8+lXNwDp7QwTzp8dTsCWc8jIXM6Ym0aV3STDCDghXmM2dr6/ip0nJ/PFtEgDZG8L545skwIH5JwaPBxJT3HWtptHquV8Bgw/JZfyPMxjzlKHv4HzufWUhnboXY+Z5Bya/fpNGz30LiI51075bCY+9M5/xP86ge79C7n5pMV17Fwa5F7uvuDCMedMSGHBgzc6fx+Pgly9TGXqkd2B9xKmb+fUb7+t+yZx4wiNtElKqdri+xmpH7+ERJ2Xzx2RvRu+3r1Po1rfoP49bMD2BFu3KSUgOnfMIvK/pHN76cTq3PrmEPoPyuPmxJSz5J4HR5/TlhtP2Zf7MRNav8p64V1ocVl0aNPPXFMLCbRKSQqe//9Zz/2IGH17A238v4raXVtN3WBGjn1tNblY44KCywsn3H6Rg9Qvdz+W6HHZqDgNHFPDo1R2hCdfF7ui7aVs/fZrMsKNDO6nVaNgOPA10CUX1MrC2LCsFOAR43bKsVcAtwGmWZTmMMesAjDErgKnAzs56GgBM9K3nFOBFfzLd9cfmhkdXsiYjmklvbF/Sse/QfDKXR5O9sabmdNaviXS0SomM8s6gsM/AQtYsC9Uzz21ufDKTzGVRTHo1vbr1z8kJ9B3qHYS07lROeIRNfq6rtpU0auOf6sC5Bw3kgkP355EbLeZOS+TeK3sSE19F6w6lAOw7NI81y2MoKQrjjMGDueDQ/bng0P1Z8k88917RI+RmBUlMqSQ23jswjoj0sO+wAtauiKJl+60nNdkMHrGFtcu95U1Z6yPod0ABAG07lxIR6SE/J5QmGNrxezgnK5w+g7w7Rf0OKKgu5/I+D96MfJdexYRHeCjYEjr9Hf9UR84bPohRhw7k0Zu6M+/vJJ4Y3Z3EFG+9bVi4h1MvzuSbid7nIjmtgq397bZPIQ4HFOSFTn//7a2HvRnL8wf15OEr2jP39zgeu6Y9Kc227izYHHBkPqtMVFDjrA8Dhudz6hWbuGdUJ8rLmm6ZS23fTa061pQ3DTkin8yMyGAEJ3uZ+vq0PAWYYIy5bGuDZVm/AAdaljXNGFNuWVYaMBR4rK4VGWOqj7NbljUe+MoY81m9RO2HXgOKGHFSDiuXRPPC1wsAGP94G2ZMTfKWgXyRut3yRQVhTHqjBeM+X4Rtw4ypiUz/OSkIke+5XgOLGXHqFlYsiuLFKQbwfml9NzGFG5/K5JWfDJWVDh6/ri1NKTPicTsYd0cXbh+3GNuGovwwnh4benW2tUluVsnNjy/H6bJxOOC3b1KY/nMSj3+wiJh4Nw5g5ZIYnr+zAwCvP9SOax9ayYkXbsS24albOhFK27u29/Czt3Xk8rtW4wqzqSh38uxY70fPsCNzGXFSDlVVDirKHL4ZgEKnv7U5+aK1DByei9MJX7/fkrl/JwEw9IhsjjnDWzpQUebk0Zu60xT6+2+3Pr+GxNQqHA5YvjCKcbeG9mwRY55fSZ8hhSSmVPHujPlMeLIlZ1y9ifAIDw+/nwHAktmxjLvNe67E238tIDbeTVi4zZAj8hh7VpeQTfrU9t105Jm5tOlcjscDWesiQn4bNxY2qrGui8Ouh9NkLcv6GXjUGDN5m7Zr8dZKpwEevNnyZ4wxb2xz/2igBZAFfGOMufhf6x2Pd2C90+n2ls5aYV897K7AdCgE2OXlO1+oiXHGh1ZmeI9Vhu7h+N1VH59PjZkzcu/LqLkLCoIdQsNyhubRvD3iCc3SwN01xfPRLLxH25ukZYVr7Bv/ebxB/teX/3su5J7LeslYG2MO3kHbOGBcHY+p837fMhfscXAiIiIistuUsa5dUy66EhERERFpMKF7RoqIiIiINCj98mLdlLEWEREREQkAZaxFRERExG+qsa6dMtYiIiIiIgGggbWIiIiISACoFERERERE/BS6PzfeEJSxFhEREREJAGWsRURERMQvtq2TF+uijLWIiIiISAAoYy0iIiIiflPGunbKWIuIiIiIBIAy1iIiIiLiN1sZ61opYy0iIiIiEgDKWIuIiIiIX2zAgzLWtVHGWkREREQkAJSxFhERERE/6ZcX66KMtYiIiIhIAChjLSIiIiJ+06wgtVPGWkREREQkAJSxFhERERG/2LZ+ebEuyliLiIiIiASABtYiIiIiIgGgUhARERER8ZtOXqydMtYiIiIiIgHQtDPWHjvYEUg9sisqgh1Cg3JGRgY7hAZnl5QEO4QG5SkvD3YIUt9sT7AjENljOnmxdspYi4iIiIgEQNPOWIuIiIhIwNh4p9yTHVPGWkREREQkAJSxFhERERE/OfCgGuvaKGMtIiIiIhIAyliLiIiIiN80j3XtlLEWEREREQkAZaxFRERExC+23XjmsbYs6wbgYryTlcwHRgEtgYlAKjALONcYU2FZViTwDtAfyAFON8as8q3nNuAiwA1ca4z5bndjUsZaREREREKKZVmtgWuBAcaY3oALOAN4FHjaGNMF2IJ3wIzv7xZf+9O+5bAsq6fvcb2AI4EXLcty7W5cGliLiIiIiN9su2EufggDoi3LCgNigA3AIcDHvvvfBk7wXT/edxvf/YdaluXwtU80xpQbY1YCGcDA3X1uNLAWERERkZBijFkHPAGswTugzsdb+pFnjKnyLbYWaO273hrI9D22yrd86rbtO3jMLlONtYiIiIj4raFmBdm8eXPasGHDZm7T9Kox5lUAy7KS8WabOwJ5wEd4SzmCSgNrEREREWl00tPTs40xA2q5ewSw0hizGcCyrEnAUCDJsqwwX1a6DbDOt/w6oC2w1lc6koj3JMat7Vtt+5hdplIQEREREQk1a4DBlmXF+GqlDwUWAT8Dp/iWOR/43Hf9C99tfPf/ZIyxfe1nWJYVaVlWR6ArMH13g9LAWkRERET8ZtuOBrnUxRjzN96TEGfjnWrPCbwK3ArcaFlWBt4a6jd8D3kDSPW13wiM8a1nIfAh3kH5ZOAqY4x7d58blYKIiIiISMgxxtwN3P2v5hXsYFYPY0wZcGot63kQeDAQMWlgLSIiIiJ+sXE0mh+IaYxUCiIiIiIiEgDKWIuIiIiIf/z/8Za9kjLWIiIiIiIBoIy1iIiIiPitoX4gJhQpYy0iIiIiEgDKWIuIiIiI35Sxrp0y1iIiIiIiAaCMtYiIiIj4TZOC1E4D612U1rKcW55eSVJaJdjwzf+l8/lbLbh4bCaDDs2jqtLB+tWRPHVLR4oLvE9vx+4lXPvwKmLi3Hg8Dq4d2ZPK8tA7WJDeqoJbnl1DUnqVt+/vpvLZG+lcfOd6Bh9WQGWFgw2rI3jyhnYUF7iCHe5uSWtZzi1PrvBtXwffvJ/O5+NbcM51aznyjM3k54YDMP7xNsyYmkTz1uW8+sM81q6IBmDJnFieu6NjMLuw25xOm2c/nkNOViT3XN6LvoPyuGj0CsLCbTIWxfHM7d3wuB2cfOFahh+XBYDLZdO2cwlnHjCYovzwIPfAfzc8vopBh+aTlxPG5Yf1AqBTzxKueWgNEZEe3G4Hz9/ejqVzYzn4hBxOu2ITOGxKi1w8d3s7Vi6OCXIPdk1tr+tOPYq55sFVRETauKvg+bs6sHRuHH0GFXD3q8vYuDYSgD8mJ/N/z7UOci92T3ikhycnZRAeYeMKs/nt6yQmPNGC5m3LGfvSGhKSq1g2P5rHrmlHVWXofS7vSJvOZYx9aVX17RbtKpjwRAtiE9wcdVYu+bnez+e3HmnFjJ8SghRl/XI6bZ6bvJScDeHcdX6nYIcje5F6HVhblnUC8CnQwxizxNfmxvub7gBrjDEjfe3vAQOASmA6cJkxptKyrGTgTaAzUAZcaIxZUJ9x18XjdvDaA23JWBBLdKyb575ayJzfE5n9WwJvPtoGj9vBhWMyOf3KDbz5SFucLpvRz6zgsRs6sXJxDPFJVbgrQ7M2yV3l4NX7WpExP4boWDfPT17K7F/jmf1rPG8+1BKP28FFt6/njGs28caDrYId7m7xVDl47cF2ZCz0bd8vFzDn90QAPn2zBZ+81vI/j9mwOoqrjund0KEG3PHnrSNzRQwxcW4cDpsbHzGMHbUP61bFcM41qxhxwia+/6QFn7zZhk/ebAPAwINzOPH8dSE1qAaY8lEqX77djJufXlnddtHYtbz3TEtmTk1k/4PzuXjsWkafbrExM5JbTutGUX4YA4bnc90jq7n++B5BjH7X1fa6vui2TN57tjUzf0li/+F5XDwmk9Fnevu2YEYcd19sBTnyPVdZ7mD0qZ0pK3HhCrN56rMMZvwUz8mXbmbSa2n88nky1z6yliPPzOWrd9KCHW5ArF0exZWHdwe8A8z3Zi3kj2+TOPz0HD59LZ2PX2kW5Ajr3wkXZ5O5LIqYOHewQ2lybFRjXZf63j0/E/jd93erUmNMP99l5Dbt7wHdgX2AaOBiX/tY4B9jTB/gPODZeo65TrlZEWQsiAWgtNhFZkY0qc0rmP1bIh6394W2ZE4caS0rAOh/YD4rl0RXZ7gK88LweELzBZmbFU7GfG8/vH2PIq1lJbN/ia/u++JZsaS1rAxmmHskd3MEGQv/tX1bVAQ5qvqX2ryc/Q/K5buPWgAQn1RJVaWTdau823vOn8kMPTz7P48bfsxmpn6d3qCxBsKC6fEU5v3rqIrtICbe+yUcG+8mZ5N3Z2HxrDiK8r05iCVzQvP1Xevr2qZ64LFtn5sWB2Ul3m0dFm7jCrexbeg7rIjfvkoCYMpHyQw5Mj+IMdaffsMK2bA6kqx1EcEOpcGktaxg4KEFfPt/KcEORfZC9TawtiwrDhgGXAScsbPljTHfGGNsY4yNN2PdxndXT+An3zJLgA6WZTWvn6h3TfM25XTuVYL5J2679sNP28zMqd4sZ+uOZdi2gwffMTz/9UJOuWxDMEINuOZtKujcu5Qls7c/JH7EmblN5tBi89bldO5Zs31HnreJl76dzw2PriAuoap6uRZty3n+qwU8NnExvfYvDFa4e+Sysct584mOeHyFcwVbwnG5bLr29vZn2BHZpLcs3+4xkVFu+g/bwh/fN40s38v3tuHisWuZMG0eF9+xlrce/W/pwxGnZzPz59B+fW/7un75vvZcfFsmE/74h4vHruGtx9tWL9djvyJe/GY+979laN+1JIgR7zmn0+bFKYYP5i1kzq9xbFgdSXG+qzohkL0hnLQWVTtZS2gafnweUz9Lqr593KjNvDRlCTc+uYa4xKbZ58vvXc/rD7TEDtEkVqNnN+AlBNVnxvp4YLIxZimQY1lWf197lGVZMy3LmuYrFdmOZVnhwLnAZF/TXOAk330DgfbUDLqDJirGzR0vZ/DKfW0pKarJfJ1x9XrcVQ5++jQVAFeYTa/9C3n0uk7cdHJ3hh65hX5DC4IVdkBExbi58/VVvHxXq+36fua1m3BXwU+TkoIXXIBExbi546VlvHJ/O0qKXHz1XnNGHdSXK4/uTe7mcC65fQ0AuZvDOXdoX64+tjevPtCOMc8sD7lDjwOH55CXE0HGwvhtWh08clN3Lhmzgqc/nENpsQu3e/svqUEH57JoTkLIlYHU5thzN/PKfW05d3AfXrmvDTc8vnq7+/sMKeSI03N44+Ggf/zstn+/ro89J4tXHmjHuUP78coD7bjhEW9pTMbCWM4b1o8rj96HL95uzl2vLAty5HvG43Fw5WEWZ/fvidWvhLZdyoIdUoMIC/cw+PB8fvVl5r96J41RB/TkysMtcrPCufSu9cENsB4MGlFAXnZY9dFVkYZWnwPrM4GJvusTqSkHaW+MGQCcBTxjWVbnfz3uReBXY8xvvtuPAEmWZf0DXAPMAYI6cnGFebjz5Qx+/iyVPybXHGo67JRsBh2ax2PXdQK2ZkIimP93PAVbwikvczHj5yS69C4OUuR7zhVmc+frq/hpUjJ/fJtU3X7YabkMHFHAo1e3Z2vfQ5UrzMOdLy3j589T+eM77/bNyw7H43Fg2w4mv98Mq693G1ZWOCnM8w4sMxbEsmFNJK07lgYt9t3Rc78CBh+Sw1s/TufWJ5fQZ1AeNz+2hCX/JDD6nL7ccNq+zJ+ZyPpV0ds97sCjN/NLCJaB1GbEyTnVr+nfvkqmW9+a92nH7iVc/9gq7r24M4V5oXnO945e1yNOyuaPyckA/PZ1Ct36FgFQUuSqLp+YMTWJsHCbhOTQK4H5t+ICF3P/jKNH/xJiE904Xd6UWFrLSrI3huZ2rcv+BxeSMT+GvGzvZ9S2n2PfvpeC1S+0j0TsSM/9ixl8eAFv/72I215aTd9hRYx+bvXOHygSIPUysLYsKwU4BHjdsqxVwC3AaZZlOYwx6wCMMSuAqcC+2zzubiAduHFrmzGmwBgzyhjTD2+NdTqwoj7i9o/NDY+tYk1GNJNeb1Hd2v+gfE65fAP3XNSV8rKaLO6sXxLp2L2UyCjvh/g+gwpZsyx6RysOATY3PplJ5rIoJr1aM6AaMLyAU6/M4p4LOlJeGupn1dvc8OhK7/Z9o+ZExZT0mjrrA47Ywqql3m2YmFKJ0+n9cm7RtoxWHcrYsCaqYUPeQ+Of6sh5wwcx6tCBPHpTd+b9ncQTo7uTmOLtc1i4h1MvzuSbiTXPR0xcFfvsn89fP6YGK+yAy9kUQZ/B3oFlv6GFrF/l3Y7prSq489UVPH59R9atDK1tW2PHr+ucrHD6DPKW+/Q7oKC6z8lpFWw9DtutbxEOBxRsCc2BZ2JKFbEJ3lxMRJSH/Q4sInNZFHP/iON/x+YBcNipW/jru8QgRlk/hp+wZbsykJRmNTtHBxyVzyoTqq/n2r31cEvOGdCT8wf15OEr2jP39zgeu6Z9sMNqcmzb0SCXUFRfn5SnABOMMZdtbbAs6xfgQMuyphljyi3LSgOGAo/57r8YOAI41Bjj2eZxSUCJMaYC7wmNvxpjglZL0WtAESNOzmHl4mhe+MY7Ocn4x9twxT1rCI/w8NC7BvCewPjc7R0oKghj0uvNGfflImzbwYyfE5n+U1Kwwt8jvQYWM+LULaxYFMWLU7z9fOvhllx5/zrCI20e/mA5AEtmxTJuTGgeLu81oIgRJ+Wwckk0L3xds32Hj8yhUw9vdmfT2kjGje0AQO+BhZx3wzqqqhzYHnjujg7VJ7qFupMvWsvA4bk4nfD1+y2Z+3dS9X0HHJbD7D+SKC8NzWkVxzy3gj5DCklIrmLC3/N496lWPDumPZffk4nLZVNR7uDZMe0AOPu69cQnV3H1A97yH7fbwbXHhtasILW9rp+9rSOX37UaV5hNRbmTZ8d6p4ocdvQWjj07C7cbysucPHxtZ0L1SFRK80pufnYNTic4nfDrl4n8/UMCq5dGMval1VwweiMZC6L57v2mdaJbZLSb/Q4s5Nlba+rmL7pjPZ17lmLbsGltBOO2uU9EAsNh24GvDrcs62fgUWPM5G3argVOBNIAD95s+TPGmDd891cBq4GtZ39NMsbcZ1nWEOBtvOmThcBFxpgtO4th6awV9tUH3BnAXjVudmXTn7ni3xyRkcEOoUE597L+AnhKmt6h6jq5QnNHZU/Y5eU7X6gpcYTmDsoeqYdxRmM2xfPRLLzTBzdJ83PX2yf+8GaD/K+M0+4IueeyXlJrxpiDd9A2DhhXx2N2GIsx5i+gW+CiExEREREJvKZxzFpEREREGkDo1j83hFA/00xEREREpFFQxlpERERE/KeMda2UsRYRERERCQBlrEVERETEP/ZeN9HLLlHGWkREREQkAJSxFhERERH/KWNdK2WsRUREREQCQBlrEREREfGLDZrHug7KWIuIiIiIBIAy1iIiIiLiP9VY10oZaxERERGRANDAWkREREQkAFQKIiIiIiJ+08mLtVPGWkREREQkAJSxFhERERH/2OjkxTooYy0iIiIiEgDKWIuIiIjILlCNdW2UsRYRERERCQBlrEVERETEf6qxrpUy1iIiIiIiAaCMtYiIiIj4TxnrWiljLSIiIiISAMpYi4iIiIifHKBfXqyVMtYiIiIiIgHQZDPWsT08DJpZHOwwGsyMEa2CHUKDM2M7BzuEBhXToSDYITS44sz4YIfQoH48/slgh9DgrjrqwmCH0KBKOiYGO4QGF7tgY7BDkACzVWNdK2WsRUREREQCoMlmrEVEREQkwGw0K0gdlLEWEREREQkADaxFRERERAJApSAiIiIi4j9Nt1crZaxFRERERAKg1oy1ZVnPUUd5ujHm2nqJSEREREQaLYdOXqxVXaUgMxssChERERGREFfrwNoY8/a2ty3LijHGlNR/SCIiIiLSaCljXaud1lhbljXEsqxFwBLf7b6WZb1Y75GJiIiIiIQQf05efAY4AsgBMMbMBQ6sx5hEREREpLGyHQ1zCUF+zQpijMn8V5O7HmIREREREQlZ/sxjnWlZ1gGAbVlWOHAdsLh+wxIRERGRRkc/aV4nfzLWlwNXAa2B9UA/320REREREfHZacbaGJMNnN0AsYiIiIhIY6eMda12OrC2LKsT8CwwGO9T+RdwgzFmRT3HJiIiIiISMvwpBfk/4EOgJdAK+Ah4vz6DEhEREZFGym6gSwjy5+TFGGPMhG1uv2tZ1i31FZCIiIiISCiqdWBtWVaK7+q3lmWNASbi3X84HfimAWITERERkcYmROeYbgh1Zaxn4R1Ib332LtvmPhu4rb6CEhEREREJNbUOrI0xHRsyEBERERGRUOZPjTWWZfUGegJRW9uMMe/UV1AiIiIi0jg5QvTEwobgz3R7dwPD8Q6svwGOAn4HNLAWEREREfHxZ7q9U4BDgY3GmFFAXyCxXqMSERERkcanoabaC9GsuD+lIKXGGI9lWVWWZSUAWUDbeo6rUdnwnpOsT5xgQ7OTPbQ8x0OxgZUPuHCXOIhsZdPlYTdhcZD9tYP1b7uqH1uyFPaZWEVs95r1mWtdlK110HdSVRB6s2taty9mzGPzq2+3bFPKhBc7k9qsnEEHbaaq0smGtdE8fVdPigvDCQvzcM1di+naswCPx8Erj3Vj/syUOv5D4+AsraLZxBVEbCwBIOvMzsTOyyV24RZsl5PKtEiyzuyMJzqMsNwy2j0yl8r0aADK2sex+bROALR8ZTFhBZXgtinrFM/mUzqCs/GdPe1aV07KE2trbm+qpPDMdMp7x5L08gYcFR5sl4P8S1tS2S2a2E+zifk137uwG8LWlbNxvIUd6SDt9lU4qmxwQ9mQeArPbBakXtXNWVJFs/dXELmhFByw6axOxCzKI27+FnA4qIoLY9M5nXEnRgAQvayA9EmrwW3jjg1j3XU9AUiaupGEv7LAhoIh6eQd3DKY3drOOzd3Zf5PycSnVnLXlDkAzPo6la+ebsfGjBjGfDGX9n2KAPj703SmvNq6+rHrFscy9ut/aNuruLrtxYt6kL0mqnpdaxfF8t7YzpSXuEhtU86Fzxqi490N2MPaXX/zDAYO2kBeXiRXXnIEAGeft5Ajjl5Bfl4kAG+/uQ8zp7ckPqGcsXf9RTcrlx++68BLz+9XvZ6DDl7D6WctxrYd5ORE8cTDgygoiAxKn+qSnlzE2At/ITmhFBv46tfufPJjb7q0zeHGc34nItyN2+3k6fcOYMmqZsTFlHPrBb/SKr2AisowHhv/P1auT6l1PY3RdbfPZeABm8jbEslV5xwEwLBD1nPWRUtp26GIGy4aRsaSJADiEyoY+9AsuvbI44dv2vDyk/sAEBnp5rYHZ9GiTTEet4Ppvzdn/Es9gtUlaYL8GVjPtCwrCXgN70whRXh/fdEvlmWdAHwK9DDGLPG1uYGto7U1xpiRvvY3gAF4ZyJZClxgjCmyLCsSb+lJfyAHON0Ys8rfGPZEyTLI+sRJ7/eqcIbD4itdJB8IK+510f5GDwkDbLI+dbBhvJO2V3tIO8Ym7Ziq6sea68O2G1Tn/uDAGdMQkQfGutWxXHP6YACcTpt3pvzGXz+l07pDCePHdcbjdjLq+mWcdtEq3nqmK0eevA6AK08ZQmJKBfe9MIfrzxqI3cin5kmbtIqSHklsHNUNqjw4Kz04uiWSc0w7cDlI/XI1yT+sI+e49gBUpkaReUuf/6xn4/ldsaPCwLZpMX4Zcf/kULRfWkN3Z6fcrSPZ/HRn3w2b5hcvpWxQPIkvbqDwtDTK+8cTOauQhHc2kfNAB4pPTKP4RG8/ImcUEvdFDna8C2ybnPs6YEc7ocombexKyvaLo9JqfC/y9Emrvdv4It82rvBQ0SKa3GO8eYLEXzaSOnkdWad3xFlSRfqHK1l/RXeqUiJxFVYCELG+hIS/ssi8qRe2y0nrl5ZQ3DuZyvSouv51gxly6iaGn7+e8Td2q25r1a2Ey15Zwntju2y37KATNzPoxM0ArFsSw0uX9NhuUD3n21QiY7YfNE+4tQsn376SboML+OOD5kx5pTUjb15Tjz3y3w/fdeDLz7pw063Tt2v/7JNuTPrI2q6tosLFhPG96dAhn/Yd8qvbnU4Pl135D5dfdAQFBZFceMk8jjshg/fe6dUgfdgVbo+TFz8axLI1aURHVvDqnZ8xc1FrLjt5OuO/3I/pC9oyqHcml58yneufOJZzjv6HjMxU7nzxMNq1yOO6s/7kpqeOrnU9qzckB7uL//HD12346qMO3HjXP9Vtq5fH8+BtA7j61nnbLVtR4WTCqxbtOxfSvlPBdvdN+r9OzJudRliYhwefm0b/wVnMmtY4EwISenZaCmKMudIYk2eMeRk4DDjfVxLirzPx1mSfuU1bqTGmn+8ycpv2G4wxfY0xfYA1wNW+9ouALcaYLsDTwKO78P/3SOlKB3H72LiiwREGCf1tcn90UrbaQXx/73GKxCHetn/L/tZJ6pGe6tvuEtgwwUnrSxpHhmdX9R2Uy8bMaLI2RDPnr1Q8bm+fl8xLJK1ZOQDtOhUzd7o3Q52fG0FxYRhdexXUus7GwFlaRfSKQgoGpXsbwpx4osMo7Z4ELu8OQVn7eMLyKna6LjvKt6/qsXFUeWomq2zEIucX424RgbtZBDjAUep9zTpLPLhT/rvvHf1bPqX/81WDORzeQTXgcHuz1o2xz87SKqIzCikYss02jgnDE13TP2e5u/rIY/ysHIr7plCV4s1UuuPDAYjYVEpZ+zjsCBe4HJR2SSBubm5DdqVOXQcVEJO0/ZGwll1LadG5tM7HzfginQHHZVffLit28sPrrTjqmsztltu0Mpqug7zv5x7/28LsbxvPTuOC+ekUFkb4tWx5WRiLFqRRUeHart3hAIfDJiqqCrCJiakkJye6HqLdc7n5MSxb433+S8sjWL0hibSkYmwgNsr7WRUbU0F2XiwA7VvmMXtJKwDWbEyiRWohyfElta6nMVr4TyqFBeHbtWWujmfdmrj/LFteFsaieSlUlm//3Vxe7mLebG9/q6qcLDcJpDWr+/0hsivq+oGY/eq6zxgze2crtywrDhgGHAx8Cdxd1/LGmALf4xxANDUVNscD9/iufww8b1mWwxhT7xU4MV1sMp9zUJkHzkjI+91BbE+b6M42W352kHKITe73Tso3/vexOd85sZ6p+ZLLfMFJy/M8OBtHcmuXHXTkRqZObvGf9sNPWM+v3zUHYMXSOAYdtJmp3zYnvUU5XXoUkt68jKULGm9ZflhuOe64MJq9v5zI9SWUtYkl+8QO2JE1X7oJf2dRuG9q9e3w3HLaPjEPT5SLnKPaUtY5ofq+Vi8vJnJNESU9kijqm0pjF/1bASW+gXLBhS1IuW81ieM3gQ3ZD3fYbllHuYeoOUXkX7JN+YPbJv3mFbg2VlB8VAqV3Rpftjosx7uNm7+3goh1JZS3jWXzye2xI12kfpVJ/PRsPNEu1l3tPSQcnlWGw+2h9bhFOMvc5A1vQeHAdMpbxpD61VqcxZXY4U5iFuVR3i42yL3bczO/TOOK1xdX3/7iyfaMuGQ9EdGe7ZZr1bWEud+n0O+IXGZ/ncaWDf4NZIPpuOMzOPSw1SxbmszrL/elqKj2mN1uJ88/258XX/uesrIw1q+L48Xnav0qbDRapBbStW0Oi1c24/mJg3n8+slccep0HA6bqx85DoDla1M4cN9VzF/Wgu4dsmiRWkR6cglbCmN2uJ69QWxcJYOGZfHFh5pdeFc40KwgdakrY/1kHZcn/Fz/8cBkY8xSIMeyrP6+9ijLsmZaljXNVypSzbKst4CNQHfgOV9zayATwBhTBeQDDTJiie4ErUa5WXJ5GEuudBFj2Thc0PleN5s+cDL/jDDcJeDcfieawnkOnFEQ09V7u3gJlGc6SDk0NF+NYWEeBh2Uze/fb/+Be/rFK3G7Hfz8tXfA/f1nrcjeFMmz/zedS28xLJ6biMfTCFOY23C4bSLXFpM/tDmZN/fBjnCR/OP66vuTp6zDdjko6u/LciREsOqufcm8uQ/Zx7en+bsZOMpqdqDWX96DVff2x1FlE70s/z//r1GptImcUUjZAd4dg5jvtlBwYQs2vd6N/Aubk/TChu0Wj5xRSEX3GG8ZyFYuB5uf7sym17sRsayUsNVlDdkDvzg83m2cN6w5mbfugyfSSfIP3m2cc2xbVt23L4X9U0n8bVPN8pnFrL/MYt2V3Un5bh3hWaVUtohmy4iWtH5hCa1fMpS3jsF2NO7X986snBNHRLSH1pb3/ILMhbFkr45i3yNz/rPseY8v45cJLXnomH6UFbsIC2/cn2dff9GZi847mqsvO4zcnCguvnxuncu7XB6OOW45V19+GOecfiwrVyRy2pmL63xMsEVHVnLvFT/w/AeDKSmL4Pjhi3nhw8GcduuZvPDhYEaf/xsA//dtX+Jiynn9rkmcdMgilmWm4tmmRO/f62nqnC4Po++bzRcfdWDj+tDfOZbGo64fiDk4AOs/E3jWd32i7/YsoL0xZp1lWZ2AnyzLmm+MWe77v6Msy3LhHVSfDrwVgDj2SLOTbJqd5B04rRnnJKI5RHeEHq94SzpKV8GWX7f/cs35zkHaUTXZnsJ5TooWOZh9VBhUQWUuLLzIRa83QqMsZMCwbJYviScvt+YknhEj1zPwwGzGXrofW4//e9xOXnuipp7xibdnsHZ148tgbqsqKYKqxAjK28cDUNQ3pXpgHT89i9iFW1h3ZQ/vcWLwlhGEefdJy9vGUZUaSURWGeXtag5H2uFOinsnE7tgC6VWUoP2Z1dEzS6islMUniTvR0HMz3kUXOQ9+lB2QMJ/BtbRvxfUlIH8ix3rorx3LJFziqhq37gOy1QlRVCVFEF5B+82KuqXQsqU7ftWOCCNVq8Yco9uQ1VSBO7YJOxIF3aki9LOCUSuK6GyWTQFQ5pRMMS7g5n6ZSZVSaE9CJnxZTr7j9xcfXvF7HhWz4tj7NABeKocFOaE8+Tp+3DTB/Np0aWU695dCMCmFVHM/6lxn5icl1fzOpz8TSfueeD3Opfv1CUPgI0bvK+T335py6lnLKm3+PaUy+Xh3it+4Ie/u/DbHG/W9Yghy3hu4hAAps7syC3neQfWJWURPDr+IN8jbSY+/AHrN8fXup6m7pox81mfGcvnH3QKdiihqZGfNxVM/ky3t1ssy0oBDgFetyxrFXALcJqvhGMdgDFmBTAV2Hfbxxpj3HgH4if7mtbhm4nEsqwwvNP9/TedUk8qff+pfAPk/ugk7ShPdZvtgXWvuWh+as0g2vZ4y0C2ra9ucZqH/j9Usd+3VfQcX0VUe0JmUA1w0FGb+OXbmjKQ/gdkc8oFq7n3ur6Ul9VkLyOj3ERGe/u17+AcPG4HmSv+W//WmLgTIqhKiiQ8y1tnF7Msn4oW0cQsziP5pw2sv9jy1tT6OIsqwePN1IVllxGeXUZlahSOcjeufF8dttsmZtEWKps1zvrMraJ/z99uoOxODiNioTdzGTG/mKqWNYNGR7GbyIXFlA2Mr25z5lfhKPa9jss9RM4toqp145tBoXobb/JtY1NARYtowrNqsuux87dQ0cw7ECvaJ5noFYXgtnFUuIlaXURFc++23HoiY1huOXFzcyns3/jLfWrj8cCsr9IYsM3A+qBzN/LojBk89MdMbv54Hs07lnLTB95zzQuyw6sf981z7Tjw7B3UwDUiySk1tbMHDFvH6lV1l6TlZEfTrn0BCYnec0b27b+JzDUJdT4meGxGn/8razYk8dGUfapbc/Jj6NfNu9O4X/f1rM3yxh8XXU6Yy/tePeZ/hrnLWvgy0zteT1N27qVLiI2t5NVnGt9JqRL6/Prlxd10CjDBGHPZ1gbLsn4BDrQsa5oxptyyrDRgKPCYr666szEmw3d9JLA1VfAFcD7e2UhOAX5qiPrqrZbe5KIq34EjDDqOdROW4J2Cb9NE735JyqEe0k+oCadgloPIFjZRbRoqwvoVGe1m38G5PHd/zZREV9xmCI/w8ODL3lJ7Mz+R5x/oQWJKBQ+8NAePB3Kyonji9tD44Np8cgeaT8jA4bapTPVOrdfm6fk4qmxav+Q9FLx1Wr3o5QWkfLsWXA5sB2Sd0glPbBiuwgpavmG8U8/ZNqVdEsg/oHmQe1Y7R5mHyH+Kybu8pl46/8pWJLyxEYfHxg53kH9lzX1RfxdS3i8OO6pmf9y5pYrkceu9OxoeKB2aQPn+8TRGWae0p8U7y3G4PVSmRrHp7E40f3+Fd3DtgMrkSLJO92brKltEU9wjkXaPzAOng4LB6VS08h55afnGMpzFleByknVqBzwx9fkxumtev8Zi6V+JFG0JY8yg/TnuhjXEJFXxwd2dKMoN5/lRPWnbs5hrJ3izzsv+TiSlVTnp7cr9Wv+ML9L55R3va2LfI7M54LRN9daXXTV67DT69N1MQmI577z/Fe++3Ys+fbPo1CUP23awaWMMzz3Tv3r5t979mpiYSsLCPQwZup7bbz2QzDUJ/N+Enjz21M+43U6yNsXw1OP7B7FXtdunyyaOGJLB8rXJvH7XJABem7Q/T7zzP64+4y9cTpuKShdPvvM/ANq1zOO2C3/Bth2sWp/EY28fWOd6/l7Q+GbVHX3vbPbZL4eEpAre/vwH3nu9G4UF4Vx+40ISkyq458nprFiayF03DALgzUk/EhNbRViYhyEHbuKO6wZRUhzGGaMyyFwVx7jx3mz+lx934Psv2wWza6GncVeBBZXDtuvn2bEs62fgUWPM5G3argVOBNIAD96M+TPGmDcsy3ICvwEJeOsK5gJXGGMKLMuKAibgzWznAmf4st21WleSYb+8/OZ66FnjNGNEq2CH0ODM2M7BDqFBxXRo3LOr1IfizMY5SK8vPx7/ZLBDaHBXHXVhsENoUCUdG++J3PUldkHjPrIRaN+ufGoW3qmDm6T5mzbax7//fw3yv1Zcf2PIPZf+/KS5Azgb6GSMuc+yrHZAC2PM9Loet6MabWPMOGBcLct78Gavd3RfGXDqzmIVERERkXqmjHWt/DmG+SLe7PIhwH1AIfAJ0DiPj4mIiIjIXsH3I4avA73xDvkvBAzwAdABWAWcZozZ4ksWPwscDZTg/SHC2b71nA/c4VvtA8aYt3cnHn9OXhxkjLkKKAMwxmwBQvs0eBERERHZdbZ3HuuGuPjpWbxTO3cH+gKLgTHAj8aYrsCPvtsARwFdfZdLgZegesKNu4FBwEDgbsuyduvnR/0ZWFf6pr+zff88HW8GW0REREQkKCzLSgQOBN4AMMZUGGPy8P6OytaM89vACb7rxwPvGGNsY8w0IMmyrJbAEcAUY0yuL4E8BThyd2LypxRkHPAp0MyyrAfxzspxR90PEREREZEmqfHUWHcENgNvWZbVF+9vpVwHNDfGbP2xgo3A1im6qn9w0Getr6229l2204y1MeY9YDTwMLABOMEY89Hu/DMREREREX9s3rw5zfdL3Vsvl/5rkTBgP+AlY8y+QDE1ZR8A+KZnbrBdAX9mBWmHt8D7y23bjDFr6jMwEREREdl7paenZxtj6ppuby2w1hjzt+/2x3gH1pssy2ppjNngK/XI8t1f/YODPm18beuA4f9qn7o7MftTY/018JXv74/ACuDb3flnIiIiIhLi7Aa67IQxZiOQaVmW5Ws6FFhEzQ8L4vv7ue/6F8B5lmU5LMsaDOT7Ska+Aw63LCvZd9Li4b62XbbTjLUxZrvfOLUsaz/gyt35ZyIiIiIiAXQN8J5lWRF4k7+j8CaOP7Qs6yJgNXCab9lv8E61l4G3GmMUgDEm17Ks+4EZvuXuM8bk7k4wu/xbvMaY2ZZlDdqdfyYiIiIioW0XpsKrd8aYf9jxrzMeuoNlbeCqWtbzJvDmnsbjT431jdvcdOItEl+/p/9YRERERKQp8SdjHb/N9Sq8tdaf1E84IiIiItJ4OcB2BDuIRqvOgbXvh2HijTE3N1A8IiIiIiIhqdZZQSzLCjPGuIGhDRiPiIiIiDRWDTUjSCOq494VdWWsp+Otp/7HsqwvgI/wTrwNgDFmUj3HJiIiIiISMvypsY4CcoBD8O4/OHx/NbAWERER2cs0pllBGpu6BtbNfDOCLKBmQL2VnlIRERERkW3UNbB2AXFsP6DeSgNrERERkb2RRoG1qmtgvcEYc1+DRSIiIiIiEsLqGlhrkkIRERER2Y5qrGtX63R77OCnIEVEREREZMdqzVgbY3IbMhARERERCQHKWNeqroy1iIiIiIj4SQNrEREREZEA8OcHYkREREREQvrnxhuCMtYiIiIiIgHQZDPWuQVxvDflf8EOo8F0K18U7BAanMMT7AgaluO3pGCH0ODScvautEjHU+KCHUKDc5SUBTuEBhVWsvdtY7ugKNghSAA50HR7dVHGWkREREQkADSwFhEREREJAA2sRUREREQCoMnWWIuIiIhIPVCNda2UsRYRERERCQBlrEVERETEb5oVpHbKWIuIiIiIBIAy1iIiIiLiP2Wsa6WMtYiIiIhIAChjLSIiIiL+sVHGug7KWIuIiIiIBIAy1iIiIiLiN80KUjtlrEVEREREAkADaxERERGRAFApiIiIiIj4T6UgtVLGWkREREQkAJSxFhERERG/6eTF2iljLSIiIiISAMpYi4iIiIj/lLGulTLWIiIiIiIBoIy1iIiIiPhHP2leJ2WsRUREREQCQBlrEREREfGbZgWpnTLWIiIiIiIBoIy1H+LDy3l40C90TdyCDdz290GsKEji2aE/0CaukLVF8Vz7+2EUVEaSEF7OI4On0i6ugHKPizHThrMsP4WO8Xk8O+yH6nW2iyvgmXkDGG/6BK9jfnrrx+mUFrtwux143A6uO2Vfzr12FYMPzcHjcZCfG85Tt3UjNyuSNh1LuOHhpXTpWcTbz3Rg0pttgh2+X+LDy3l44C90TdqCbXu3cYuYYq7dZxadE7Zw0vcnsSA3HYCkiDKeHzaFfVKymLTS4t5Zw6rXc2Of6ZzYYSkJEeX0/fiiYHXHL06Hh/fP+YSsoliu+fRoxp/xKTERlQCkxJSyYEMzrv/8KAa0XcezJ0xmXX48AD8u68Qrfw0AYGiHNdx6yO84HTaT5vfgzen7Ba0/dYkIq+KVSz8nIsyDy+nhxwWdeO2H/dm/81quOWoaTodNSUU49318MGtzEjlr2FxGDliC2+Mgrzia+z8ZzsY8b/+P2c8w6uBZALz1c3++nm0Fs2vbefKGtvz9QwJJaVW8+rMB4LX7WjFtSgLhETYt25dz09OZxCW6qx+TtTacS4Z355ybNnLqFZtrXQ9AwRYXD13egU1rI2jepoLbX1lFfJKbxuC6sf8wcOgm8rZEctU5wwEYdvB6zrrI0LZDETdc/D8yliQB0G//zYy6YjFh4R6qKp288UJP5s1KIzqmisde/KN6nanNSvn5uza89mzvIPSobukpRdx6xW8kJ5Zi2w6+/qkbn37Xq/r+U45ewOVnz+Cky86koCgKgL49NnDFudMJc3nIL4zkpgeOBmD/Pmu58ty/cTptvp3ajYlfNv7vJYDY+Equu8/Qvksxtu3gmTstDhiRzaDh2VRVOtmQGc3Td1gUF4YTn1jJ2GcW0q13AT981oKXHuwW7PBDmzLWtarXgbVlWScAnwI9jDFLfG1uYL5vkTXGmJG+9jeAAYADWApcYIwpsiyrPfAmkA7kAucYY9bWZ9z/dmf/P/l1Q1uu/v1wwp1uolxVXNFrDn9tas0rP+/LZT3ncFmvOTz+z2Cu6DWbxVtSufK3I+iUsIV7BvzOeT8dx8rCJEZ+ewrgHdD8ccK7fJ/ZsSG7sUfGnNeHgrzw6tsfv9GGCeM6ADDy3HWcdeUanr+nK4X5Ybz8QGeGjMgJUqS7p3ob/1GzjQsqI7nyt8N5YP9ft1u23O3i6XkD6Ja0hW6Judvd99O69kxY2osfjp3YkOHvlrP3m8/K3CRifYPpCyaeWH3fUyMn83NGzetz9tqWXPPp0ds93unwMHbEb1z60XFsKozl/XM+YeryDqzISWmYDuyCiioXV74+ktKKcFxON69d/jl/mXbcesJv3PzOkazanMzJgxdw4cGzuO/jQzDr0zj/hZMorwzn5EELueaoadz+/mEkRJdx8aEzOf/5k7FtB+9c8zG/LupAYVlksLsIwOGn5zJyVDaPX9euum2/Awu5cOx6XGHw+gMtmfhcMy6+Y0P1/a/c25r9Dync6XoAPny+GfsOK+T0a7L44LlmfPD89usKph++actXH3fgxrv+qW5bvSKeB8fuz9Wj5223bEF+BPeOHkhudhTtOxVw39N/c/7xh1FaEsY1FxxUvdyzb/7Kn7+0bKgu7BK3x8nL7+1Pxqo0oqMqeemBL5i1oDVr1iWRnlLEgH3WsSk7tnr52Jhyrh31F7c9ejhZOXEkJZQC3vfxNRdM49aHj2Bzbgwv3P8lf85ux5p1SUHqmf8uuy2DWb+n8NANvQkL9xAZ5WbOX8mMf6YjHreTUTcu57RL1vDWU52pqHAy4bkOdOhSTPuuxcEOXZqw+i4FORP43fd3q1JjTD/fZeQ27TcYY/oaY/oAa4Crfe1PAO/42u8DHq7nmLcTF17O/s028OHy7gBUelwUVkYyos0qJq3w7vFOWtGNw9qsAqBLYh5/bWoNwIqCZNrEFpEaVbLdOg9ovo41RQmsL4lvuI4EWGlxzT5ZVLQH27f3mp8bwbIF8birHEGKbNfFhZezf/oGPlyx/TZeXpDMysKk/yxf6g5nVnZLyt2u/9z3T05zNpfF/qe9sWkeV8SBnVYzaV6P/9wXG1HBwHbr+Cmj7h2/3i2yWLMlkXX5CVR5XExe0oWDO6+qp4j3lIPSCu+OYZjLQ5jT4z2x3YbYqAoA4iIr2Fzg3XazVrSmvNK7/Pw1zWmWUATA4G6Z/L2sDQWlURSWRfL3sjYMsdY0fHdqsc/gYuKTt88g9x9eiMv3du3Rv4TsDTU7yH9+m0iLthW071a20/UA/PVdIiNO8+5Mjjgtl78mJwa4B7tv4T+pFBZEbNeWuTqedWvi/rPsiqWJ5GZ7s7irV8QTGekmLHz7/rZqW0RicjkL/2l8O4oAuXkxZKxKA6C0LJw16xNJS/YOGK84dzqvvr8/tl3zOXzoASv4fUZ7snK8z0deQTQAVuds1m+KZ8PmeKrcLqZO68TQ/o3nNV2bmLgqevfP57tPvDs+VZVOigvDmfNnCh63d2izZG4Cac3LASgvdbFodhIVFaqADQi7gS4hqN4y1pZlxQHDgIOBL4G761reGFPge5wDiKbmKe0J3Oi7/jPwWT2EW6u2sYXklkfx6OCp9EjKYUFuOvfPOoC0qNLqAdTmshjSorx7/0vyUjii7Upmbm5Jn9QsWsUW0iK6mJyymOp1HtN+OV+t7tKQ3dgjtg0PvDEfGwffftCCyR96P8jOu34Vhx6/ieLCMMacv0+Qo9x91dt40FR6JNds41J3+M4fHKJGH/IHT/06hNiIiv/cd0iXlfy9pg3FFTWDlL6tNvLReR+yuTiGJ6cewPKcFJrHF7OpsGYnYlNRLPu0zGqQ+HeH0+Hhnas/oU1qPh9P683CzOY8OGk4z1zwDWWVYRSXRXDRSyf+53Ej91/MX0u9mdv0hGKy8msGalkFcaQnhE7267v3Uzjo+DwASoudfPhiMx6euJyPX2rm1+O3ZIeT2rwKgJRmVWzJDv33yNCDN7DcJFJVuf2O8kEj1vPbj63wHkRt3JqnFdKlfS5LlqdzQP/VZOfGsGLN9jsErVsUEBbm4cnbvyU6upJPJ/dkyu9dSEspISun5n28OTeG7p03N3QXdlmLNqXkbwnnhgeX0MkqJmNhHC8/0pXy0prtePhJG/n12/QgRil7o/rcdTsemGyMWQrkWJbV39ceZVnWTMuypvlKRapZlvUWsBHoDjzna54LnOS7fiIQb1lWaj3GvR2X06ZXcjb/t6wnIyefQok7jMt6/fOvpRzVewGvLNyX+PByvjjqY87rtoBFW9LwbJM1CHe6ObT1ar5Z06mhurDHbjmrL9eevB93XdKLY8/aQO8B+QC880wHzj94EFO/asZx5zSOw8G7o3obZ/i2cVUYl/X8J9hh1ZsDO60itySaxZt2/IVzVI9lfLu4Zsdv8aZ0jnj1XE595zT+b/Y+PHPC5IYKNaA8tpNznjuVYx85l55tsujUPJczh87j+vFHc9wj5/LVLIvrj/lzu8cc2W8pPVpvZsKv/YITdAD937PNcYXZHHLSFgAmPNGCEy/ZTHSsZ7fW53CAI8SnBmjXsZBRVy7mucf+W1N84Ih1/DKldRCi2jVRkZXcff3PvDhhIG63kzNHzuPtj/97roPL5aFbxxxuf2IEYx45nLNP/IfWLfKDEHFguFw2XXoU8s3E1lxzygDKSl2cdnFNpv30S1fjrnLw81fNgxhl0+TAOytIQ1xCUX0OrM8EthaaTqSmHKS9MWYAcBbwjGVZnbc+wBgzCmgFLAZO9zXfDBxkWdYc4CBgHdBgZ8tsLIllY0ksc3O8b87JazrRKzmb7LJo0qO8mar0qGJyyryH1YqqIhjz98GM/PYUbv7rYFIiS8ksSqhe30EtM1m0JW27DHZjl5PlrR/Nz43grx9S6dZn+3rMn79MZ+hh2cEILSD+s40zvdu4qerXeiPDO6/i20ve5bFjpzCw3ToeOtp7Ym1SdCm9W2Tx64r21csXV0RQ6iuL+H1le8KcHpKiS9lUGEvz+JpsbfO4YrIKG38ZTFFZJLNWtOKAbmvo2jKHhZne7T5lXmf2abepern9O69l1MGzufmdo6j0lf1sLoilWWJR9TLNEoqqy0cas+8/SGH6Dwnc+vxqHL79/CVzYnjjgVacN7Ann76ezsTnmvP5m2l1ric5rZKcTd4DnTmbwkhKrarv0OtNanopdzw8gyfv25eN67bfhh275ONy2WSYpOAE5yeXy8M91//Ej3904veZHWjVvIAW6UW88vDnvPvMR6SnFPPyg1+QnFhCdm4sM+a1pqw8nIKiKOYvaUHndrlk58bQLLXmfZyeUkLOlsb/ms7eFEn2pkjMfO/36+/fp9O5h/e7acQJGxh4UA6P39qDUDjiIE1LvQysLctKAQ4BXrcsaxVwC3CaZVkOY8w6AGPMCmAqsO+2jzXGuPEOxE/23V5vjDnJGLMvcLuvLa8+4t6R7LIYNpTE0THe+y8PaLGOjPwkflzbnpM6LQXgpE5L+WFtB8A7u0S40zvuP73zEmZsbklRVc0h9WM7ZPDl6s6EishoN9GxVdXX9x26hdVLY2jVvrR6mcGH5rB2ZXSwQtxj/9nGzdeRUZAU1Jjq07jfBnPYK+dx1GvnMPqrw5i+pjVjvxkBwGHdVvDrivZUuGuqxFJjSthamdW7xSacDpu80igWbmxG++Q8WicWEOZ0c2T3DKYu7xCEHu1cUmwpcVHeWsvIsCoGdVnLqs3JxEVV0C4tD4BBXdeyanMSAN1aZnPbib9y8ztHsqW45rU9bWlbBnddS3xUOfFR5QzuupZpS9s2dHd2yYyf4/noxWbcM34FUTE1KaCnPsvgnemLeGf6Ik68eDNnXLOJ4y+se4dy8OEF/PCht8Tghw9TGHJEaGY8Y+MqueeJ6Yx/qQeL5/+3hvqgw9aHQLba5uZLfmf1uiQ++dY7a8nKzBROvfJMzrn+VM65/lQ258Zy+e0j2ZIfw5+z2tG72yacTg+REVV077yZNeuTMCvSaN2igBbphYS53AwfvII/ZzXu1zTAluxINm+MonUH7zlM/QZvYc3yWPoPy+GUCzO59+relJf99zwYkfpWXzXWpwATjDGXbW2wLOsX4EDLsqYZY8oty0oDhgKP+eqqOxtjMnzXRwJbZxFJA3KNMR7gNrwzhDSo+2YO5akDfiTc6SGzKIFbpw3H6bAZN2wKp3ZewrrieK793Tsw6ZK4hccGT8UGluUnc9u04dXriXZVMrTFWu6Y/r+G7sJuS06t4I7nFwPeQ29Tv0pn1u8p3D5uEa07lGLbkLU+iufv9pYOJKdV8OzHc4iJc+PxwAnnreOyY/pvd7JjY3TfrKE8NeRHwl012/iwNiu5u/8fpESW8vpB37J4Syqjph4DwNTj3iMuvJJwp5vD2qzigp+PIaMgmdH9pjGyfQbRYVX8fvy7fLi8O+MWDAhy7/x3ZPcM3vx7u31dDrOWc1rfhbg9TsqrXIz+6jDAgdt28NCP/+Olk7/C5bT5bH53ljfCGUEA0uJLuPvUn3A6bJwOmx/md+b3Je15aNJBPHL299i2g4LSCO7/+GAArj36L6IjKnn4rCkAbMyL4+YJR1FQGsUbP/Vn/NWfAPD6T/0pKI0KWr/+7eEr2jPvrzjyc8M4u39Pzr1pIxOfb05luYPbTve+R7v3L+a6R+ueWGlH6znyrFxOv3oTD17egckTU2nW2jvdXmMx+t5Z7LNvDglJFbz92RTee92isCCcy29cQGJSBfc88TcrliVy1w2DOfaUlbRqU8yZo5Zy5ihvguSOGwaTv8V7dO5/h6zn7psHBrM7O9W7WxaH/W85K9Yk8/JDnwPw5gf7MX3ujgfFa9YnMXNea1575DM8HgffTu3GqrXJADw3fjCP3Po9TqfN5F+6snpdcoP1Y0+8/FAXRj+6iLBwm41ro3j6ju4888EswsNtHnx9LgBmbgLP3+edEvOt7/8iJs5NWLiHIYdkc/ulfclc3viz841OCJ9Y2BActh34Z8eyrJ+BR40xk7dpuxZvjXQa4MGbLX/GGPOGZVlO4DcgAe9xm7nAFcaYAsuyTsE7E4gN/ApcZYwp31kM8zdutI//v/cC3LPGq9t9i4IdQoNbenfPYIfQoGLW7X1ns0fl7F2f3tMffinYITS4Yw4YufOFmpDyjnWX2zRFEf+sDHYIDWpyzquz8E4f3CQtzNxknz7u/xrkfy14/IaQey7rJY1ojDl4B23jgHG1LO/Bm73e0X0fAx8HNEARERER2S2hemJhQ9j7UmAiIiIiIvWgcRe+ioiIiEjjoox1rZSxFhEREREJAGWsRURERMR/yljXShlrEREREZEAUMZaRERERPym37OsnTLWIiIiIiIBoIy1iIiIiPhPNda1UsZaRERERCQAlLEWEREREf/Y+uXFuihjLSIiIiISAMpYi4iIiIj/lLGulTLWIiIiIiIBoIG1iIiIiEgAqBRERERERPynUpBaKWMtIiIiIhIAyliLiIiIiN803V7tlLEWEREREQkAZaxFRERExH/KWNdKGWsRERERkQBQxlpERERE/OLQT5rXSRlrEREREZEAUMZaRERERPynjHWtlLEWEREREQkAZaxFRERExG+qsa6dMtYiIiIiIgHQZDPWketK6DJmZrDDaDDuqqpgh9DgOt8yI9ghNChHeJN9u9bOs3elRY76YHCwQ2hwjo6RwQ6hQYXnlAQ7hIbnUg6vydm7Ppp3iV7tIiIiIiIBsBemwERERERktzWijLVlWS5gJrDOGHOsZVkdgYlAKjALONcYU2FZViTwDtAfyAFON8as8q3jNuAiwA1ca4z5bnfjUcZaRERERELVdcDibW4/CjxtjOkCbME7YMb3d4uv/WnfcliW1RM4A+gFHAm86Bus7xYNrEVEREQk5FiW1QY4Bnjdd9sBHAJ87FvkbeAE3/Xjfbfx3X+ob/njgYnGmHJjzEogAxi4uzFpYC0iIiIi/rFrfta8vi9+eAYYDXh8t1OBPGPM1hkd1gKtfddbA5kAvvvzfctXt+/gMbtMA2sRERERaXQ2b96cZlnWzG0ul269z7KsY4EsY8ysIIb4Hzp5UURERET810AnL6anp2cbYwbUcvdQYKRlWUcDUUAC8CyQZFlWmC8r3QZY51t+HdAWWGtZVhiQiPckxq3tW237mF2mjLWIiIiIhBRjzG3GmDbGmA54Tz78yRhzNvAzcIpvsfOBz33Xv/Ddxnf/T8YY29d+hmVZkb4ZRboC03c3Lg2sRURERMRPNg67YS676VbgRsuyMvDWUL/ha38DSPW13wiMATDGLAQ+BBYBk4GrjDHu3f3nKgURERERkZBljJkKTPVdX8EOZvUwxpQBp9by+AeBBwMRiwbWIiIiIuK/RvQDMY2NSkFERERERAJAGWsRERER8Zufc0zvlZSxFhEREREJAGWsRURERMQ/NqqxroMy1iIiIiIiAaCMtYiIiIj4xYFqrOuijLWIiIiISAAoYy0iIiIi/lPGulbKWIuIiIiIBIAG1iIiIiIiAaBSEBERERHxm05erJ0y1iIiIiIiAaCMtYiIiIj4Rz8QUycNrHfRDY+vYtCh+eTlhHH5Yb0A6NijhGsfWkNUrJtNayN57NqOlBS5iE+q4o6Xl9OtbwlTPkrlxbvaBTn6PRMe6eHJSRmER9i4wmx++zqJCU+0qL7/ivvXccQZuZzQdZ8gRrnnbnxiNYNG5JOXHcZlI3oCcN7N6xlyRB62x0FedhhP3Nie3E0RnHL5Jg45MRcAl8umbdcyTu/bh8K80HlrpbUs55YnV5CUVgm2g2/eT+fz8S247bkM2nQqAyAuoYqigjCuOqY3AKdfsZ4jTtuMx+PgpXvbMevXpCD2YNektSznlqdX+voL3/xfOp+/1YK4xCrGvrCc5m3K2bQ2koeu7ExRQRhxCVXc8PhKWrUvp6LcyVO3dGD10phgd2OXhEd4ePyDRd73rsvm98kpvPtMG447dyMnjNpIqw7lnN5/Pwq2hAMweEQu5924Fo/Hgdvt4NX727NwZnyQe1G362+eycDBG8jLi+TKiw/f7r4TT13KJZfP44wTj6OgIJKY2EpuuW066c1KcLlsJn3YjSnfdQDgy+8/ZtXKRAA2Z8Vw351DG7orfrvhhukMHLSevLxIrrj8qOr2kSOXcuxxGXg8DqZPb8Wbb/QFoEPHPK69diYxMZV4PA6uu/YwnE6bsbf/ScuWRXg8Dv6e1oq33uobrC75rXWHYsY8trD6dss2pUx4sROpzcoZdFA2VZUONmRG8/RdPSguDCc+sZKxT86nW+9Cfvi8BS89bAUxemnK6vXb37IsNzB/m6aJxphHLMu6Grge6AykG2Oyfcs7gGeBo4ES4AJjzGzffY8Cx/jWc78x5oP6jL02Uz5K5cu3m3Hz0yur2254bDWvPdCG+X/Hc/hp2Zxy2UbeebI1FeUO3nmyNe2tUjp0Kw1GuAFVWe5g9KmdKStx4QqzeeqzDGb8FM+S2bF07VNCXKI72CEGxPcfpfDF+HRueWZVddvHLzfnnSdaAXD8hVmcc/1Gxt3Wjo9fbs7HLzcHYNCIPE66JCukBtUAnioHrz3YjoyFsUTHunnuywXM+T2Rh6/pUr3MJbevobjABUC7LqUcdFwOlx2xDynNKnn43SVcfEgiHo8jWF3YJR63g9ceaEvGAl9/v1rInN8TOeyUbP75I4EPX2rJaVds4LQrN/DmI2054+oNrFgUw/2XdaVN51Kuun81t53VPdjd2CWVFQ7GnN3D99718MSHi5g5NZFFs+L5+6dkHnt/0XbL//NnItN+SAYcdOhewtjnlnHpYY17sPXDd+358vPO3HTrjO3a09JL2K//JrI21ewMHXt8BmtWx3PvHUNJSCzntfGT+fnHdlRVOamocHHNZYc1dPi7ZcqUDnzxZRduvvnv6rY+fTYxeMh6rrryCCorXSQmeneOnU4Po0dP4/HHBrFyZTLx8eW43Q6cTptPPraYN685YWFuHn5kKgMGbGDmzJbB6pZf1q2K5ZrTBgLgdNq888Mf/PVjGq07lDD+2U543E5GXZ/BaRet5q1nulBR4WTCC53o0KWY9l2Kghx96FONde3qu8a61BjTb5vLI772P4ARwOp/LX8U0NV3uRR4CcCyrGOA/YB+wCDgZsuyEuo59h1aMD2ewjzXdm2tO5Yx/+84AGb/lsDQo/MAKC91sXBGHJVloTHg2DkHZSXevoeF27jCbWzb+6F2yZ3reeOBxv1B7K8Ff/93G5cU1dyOivZg7+BD5eATtjD185T6Di/gcjdHkLEwFoDSYheZGdGktqjYZgmbA4/OZeqXqQAMOWwLv3yZSmWFk01rI9mwOhKrb+h8UeVmRZCx4F/9bV7BkMPy+OETbx9/+CSVAw7PA6Bd11L++dP7cbN2eTTN21R4s90hZZv3bphNWJiNbTtYviiWrHWR/1nau6z3cysq2r3D13tjs2B+OoUFEf9pv/TKubz56j7b98F2EB1dBdhER1dRWBiB2x16n9MLFjSjsHD77XfMscv58MPuVFZ6t3d+fhQA/ftvZOXKJFauTAagsDASj8dJeXkY8+Z5kwNVVS4yMpJJSytpwF7sub6DctmYGU3Whmjm/JWKx+0d2iyZl0ha83LA+328aE4SFeU6tUzqV1BSa8aYOQCW9Z9DMccD7xhjbGCaZVlJlmW1BHoCvxpjqoAqy7LmAUcCHzZg2LVavTSaIYfn89f3SRx4zBbSW1bs/EEhyum0ef67pbTqUMGX41Mxc2I54aLN/PV9IrlZ4cEOr15dMHodI07JpbjAxejTum53X2SUhwHDC3jhjrZBii4wmrcup3PPEsw/cdVtvQcWsiU7jPWrvF/QqS0qWDKn5v7sDRGktgi1gaZX8zbldO7l7W9SWiW5Wd6BWW5WePXgecWiGIYeuYWFM+Lp1reI5q3LSWtRQV52aL3enU6bcV8soFX7Mr56tzlmblydyx9weC4X3JJJUmold10UmofNBx+wnpzsaFauSNqu/cvPOnPXA3/y7odfEx1TySP3D8a2vQPriAgPz774I263g48mWvz1R+sgRL77WrcupHevbM4/fz6VFS5ef70vS5em0rp1IbYNDzz4C4mJZfwytR0ff9xju8fGxlYwaNB6Pv+sW5Ci3z0HHZnF1G+b/6f98BPX8+vk/7ZLAITC3naQ1PeuW7RlWf9sczl9J8u3BjK3ub3W1zYXONKyrBjLstKAg4FGM4J56pYOHHteFs99vZjoODdVlaGX+fCXx+PgysMszu7fE6tfCb0HFfG/4/L4/M20YIdW78Y/1ppzBu7DT5+mMHLU5u3uG3xYHgtnxIZcGci2omLc3PHSMl65v912Gfrhx9Vkq5uSqBg3d7ycwSv3td2uv16O6nNzPnypJXEJbl74ZgHHX5DF8oUxIVP2si2Px8HVx+7DuQfsS7c+RbTvVndW8s/vU7j0sL7cd1k3zrtxbQNFGTiRkVWcftZiJozv9Z/79tt/EysyEjnntGO4+tLDuOKaOUTHeHekLjjraK678lAee2ggl145lxYtQ+doDIDL5SE+vpwbrh/B66/35baxfwHe2vpevbJ57NHB3HzToRwwdB39+m2qfpzT6eHWMX/xxedd2bix7p2uxiQszMOg4dn8/n2z7dpPv2QV7ioHP3+tgbU0rPoeBZQaY/rt6UqMMd9blrU/8CewGfgLaDQFvWuXR3H7Od49/NYdyxh4SH6QI6p/xQUu5v4ZR9+hRbTqUMFbfy4GIDLaw1t/LGbU0B47WUPo+unTFB54J4MJT7aqbjvo+NAsA9nKFebhzpeW8fPnqfzxXU0/nC6boUfmcs1xvavbcjZGbHdUJq1lBTkbQyt76wrzcOfLGfz8WSp/TPb2Ny87nJRmFeRmRZDSrIJ8X0a6pMjFU7d09D3S5u3f57FxzX/LJ0JFcWEY86YlMODAfL9OwlwwI4EWbVeQkFxZfXJjKGjZqpjmLUp44dUpAKSllzLu5R+44apDOeyIVXw00QIcbFgfx6aNsbRtW8hSk0JOdjQAGzfEMW9uOp275rFxQ+gMNLOzY/jjjzaAg6VLU7E9kJhYTnZ2DAvmp1NQ4H3tzpjRks5dtvDPP96B53XXzWT9+ng++yy0jk4MGJbD8sVx5OXWlAGNGLmBgQdmM/aSfdla0iSBpRrr2jW2YqN1bJ+JbuNrwxjzoK9O+zC875SlQYhvhxJTvZkOh8PmzGs38PW76UGOqH4kplQRm+Ddn4mI8rDfgUVkzIvhzH69OH9QT84f1JPyUmeTHFS36lhWfX3IEXlkLo+qvh0T76bP4CL+/C4xGKEFgM0Nj65kTUY0k97Yvk5+36H5ZC6PJntjzZfWtB+SOOi4HMIjPDRvU06rDuU7LStoXGxueGyVt7+v18xqM+2HJEacnAPAiJNz+GtKEgCxCVWEhXsAOPKMbOZPj99BhrtxS0ypJDa+CoCISA/7Disgc0VUrcu3bF/G1vm0OvcqJjzCQ8GW0Doas2plImedchyjzj6aUWcfTfbmaK69fARbtkSxOSuGfvtmAZCUXEbrtoVs3BBLXFwFYeHez7iEhHJ69sphzeqgnM6z2/76szV9+3r71rp1IWHhHvLzI5k1qwUdOuYRGVmF0+lhn302s2aNt2/nnT+fmNhKXnl532CGvlsOOmoTv2xTBtJ/aA6njFrNvdf2obwstN6n0jQ0tk/KL4CrLcuaiPckxXxjzAbLslxAkjEmx7KsPkAf4PtgBDjmuRX0GVJIQnIVE/6ex7tPtSIq1s1x53lLA/6YnMT3H9YcNn/7j/nExLsJC7cZckQet5/TlTXLooMR+h5LaV7Jzc+uwekEpxN+/TKRv38IrS8df4x5fiV9hhSSmFLFuzPmM+HJlgw8pIA2ncrw2JC1NoJxt9VMnTj0yDxm/ZJAeWlofoj3GlDEiJNyWLkkmhe+XgDA+MfbMGNqkrcM5Ivty0BWL4vh169TeeX7+XjcDl64q31IlUb0GlDEiJNzWLk4mhe+qenvBy+2ZOyLGRxx+may1kXy4JWdAWjXpYybnlwBtoPVy6J4ujp7HTqSm1Vy8+PLcbpsHA747ZsUpv+UzMjzN3LqpetJTq/kxW/mM2NqEs/e1olhR+Zy6InZVFU5qChz8si1XWnsmb/Rt/9Nn76bSUgs552JX/Pu2z35/tsdb6v33+3BjaNn8OJr34MD3nptHwoKIunRM5trbpiNx3bgdNh8NNEisxEPrG8d8xd9+mSRkFDOhAlfMOHd3nz/fUduuHEGL738LVVVTp58YhDgoKgogkmTLJ4dNwXbhhkzWjFjeivS0ko488xFrFkTz3PPe79Wv/yyC99N7hzczvkhMtrNvkNyee7+mll6rrhtKeERHh585R8AzLwEnn/Ae/9b3/5JTFyV9/v4kGxuv6wfmStigxF66FPGulYOux4L0Hcw3d5kY8wYy7KuBUYDLYAs4BtjzMW+6faex3tiYgkwyhgz07KsKGC2bx0FwOXGmH/q+t9LZ62wrx5ye2A71IjZVVXBDqHhOUNzILu7HOGNbT+4AXj2rk9vh6uxHUSsf46OjeZ0mQZhh+9dn1sAjvWbd75QEzI56+VZwIBgx1FfFq/YaF94+3sN8r/+ev+mkHsu6/Wb2hizw08QY8w4YNwO2m3gqh20l+GdGUREREREgsUGhyfYQTRee196RERERESkHuyFx5ZFREREZLftXVV6u0QZaxERERGRANDAWkREREQkAFQKIiIiIiJ+caAfiKmLMtYiIiIiIgGgjLWIiIiI+K8efwMl1CljLSIiIiISAMpYi4iIiIh/bNVY10UZaxERERGRAFDGWkRERET8p4x1rZSxFhEREREJAGWsRURERMRvqrGunTLWIiIiIiIBoIy1iIiIiPhP81jXShlrEREREZEAUMZaRERERPymGuvaKWMtIiIiIhIAyliLiIiIiH9sNI91HZSxFhEREREJAA2sRUREREQCQKUgIiIiIuI3nbxYO2WsRUREREQCQBlrEREREfGfRynr2ihjLSIiIiISAE03Y+1y4UxODnYUDca9eXOwQ2hwzojwYIfQoBxRkcEOocHZFZXBDqFBORPigx1Cg8vdNzXYITSoolZ7Xz4rfV5csEOQQFPCulZ73ztcRERERKQeNN2MtYiIiIgElMPWrCB1UcZaRERERCQAlLEWERERET/ZYCtlXRtlrEVEREREAkAZaxERERHxm2qsa6eMtYiIiIhIAChjLSIiIiL+U8a6VspYi4iIiIgEgDLWIiIiIuI3h2YFqZUy1iIiIiIiAaCBtYiIiIhIAKgURERERET8YwOeYAfReCljLSIiIiISAMpYi4iIiIjfdPJi7ZSxFhEREREJAGWsRURERMR/SljXShlrEREREZEAUMZaRERERPynGutaKWMtIiIiIhIAyliLiIiIiH9scChhXStlrEVEREREAkAZaxERERHxn2qsa6WMtYiIiIhIAChjvYtaty9mzGPzq2+3bFPKhBc78/l77QA48bzVXHLTMs446EAK8iIYPDyLc69agccDHreDVx63WDQnKUjR75nwSA9PTsogPMLGFWbz29dJTHiiBWBzwa0b+d+xeXg8Dr56J5XP30gPdri7LTzCw+MfLPL202Xz++QU3n2mDdc/soKu+xTjcNisWxnFk7d0pqzERXiEh5ueWE7X3sUU5IXx8DVdyVoXGexu7JK3pvxNabELt8eBp8rBdaftx5gnF9O6YwkAcfFVFBWGcc1J/WnWqoxXvprJ2lXRAJi5CTx/b9dghr9LwiM8PP7+gurX8e+TU3n32bb0G5LPRWNW43DYlJW4ePLWzmxY7e3j/47O5pxr12LbsGJxDI/d2C3Ivdh1J5y9msNP8PZhdUY8T9/Ti6vHLqZ3/1xKisIBePruXqxYmsDwozZwygUrcQClJS5eeKgnK5fFB7cDOxERVsVLV35BeJgbl9Pm53kdef37/bnj9J/Zt/MGikojAHjgg+EsW5/Gvp3X89gF37E+19uvXxZ05M0p/avX53R4eOv6SWzOj+XmN48KSp/84XR4mHj2J2QVxXL1Z0czqO1abjzwL5wOm5LKcO747hAy8xI5b7+5nLTPYtweB7ml0dz13cFsKPT2fWTPJVw6aDYAr/69H18s6h7MLtUqPaWIMZf8SnJCKQBfTbWYNKU3ACeOWMjxhy7m/9u77/goqvWP459NIT0ESAy9iHAEEVA6KFi4CnZs2Ct2wIpyVewFy09QsYuFK3a9VopcG4pgQUWKHKSFUEJI76Ts/v6YJQQkkIRNdjd+369XXtk9Ozv7nJ3d2TPPPDPjdrtYtKQdL77bn+TEfF576ANS05oCsGLNAUx9fcgu83zg+nm0Ssrj8jvPaNjOBDEX4HL7O4rAVa8Da2NMBbC0StPb1trJxpixwA1AZyDJWpvhnf5g4FXgcOAOa+3jVeZ1PXAFzjJ9yVo7tT5jr86mlBjGjR4IQEiIhxnzvmPhV84gMjG5hMMHZZK+ObJy+t9/bM6ib5IAFx275PPvx5Zy1WmD/RH6fivb7uLWs5zBZGiYhyc+Ws3PX8XRvst2klqXMWbowXg8Lpq2KPN3qPulrNTFxPO7efvp5vF3V/DLN0158YH2FBU4X5kr7kjh5Iu28t7zrTnu7G0U5IVx+TG9GXZSJpfdtoHJ44NnoLnDxEt6kZcTXnl/8s3dKm+PuXUNhfk7VxdbUiMZd3ofglFZqYuJFx6yc/m+vZxfvk3guvvWct/VhtQ10Zx4fhrnXruJJ247iNYdihl99SZuPrsHBXlhNG0efJ/vFkklnHxOCtecOYTS7aFMnLyEYcenAfDK1K4s+LLlLtNv3RTFxDH9KMgPp8/gbYy7czk3XTzQH6HXWGl5KGOfP5ni0nBCQyp4YewnLFzpJDymfTaQr/848G/PWbKuZbWD5rOPXMb6rc2IiSyt17j31wWHLWVdVgIxTZzP5Z3D5zP+45Gsy2rG6F7LuGrAYu6cewx/bkvknJlnUFIeztk9l3HT0IVM+Pw44iNLuGbgL4x+80zwuHjn/Pf4Zk0n8rYHXnKgoiKE59/uz18piURFlvL8PR+zeHkbmsUXM/iwDVwxaRRl5aEkxBVXPmdzehxX3jVqj/M7ss96ikuUXxTfqu9SkGJrbe8qf5O97QuA4UDKbtNnAeOBx6s2GmN64Ayq+wO9gJOMMQfVb+j71mtAFmmpUaRvcbJaV05YxStTuuxSelRSHIazLQCRURVBXpbkoqQoFICwcA+h4R48HjjpogxmTknG43H6mZsZvreZBIEq/QzzEBbmweNxVQ6qwUNEpLvyylODhmfzvw8SAfhudnN6D86jcV2WysORx2/j21kH+DsQH9lt+Xo/x3ggOrYCgJi4CjLTnQzniNHpfPpGSwrynOWfmxWcn+/QUA9NItyEhLqJiKogc1v1A6c//0igIN/pp12aQIvk7Q0V5n5wUVzqxBwW6iYsxF3nb2FS0wKGdEvhk58CM3O7Q3JsAUcemMIHS3duBHs8ENvE2RiIbVJKekE0AD+ntqGk3Hl//tiSTHJsIQBDOqSycEM78koiydsewcIN7RjScUMD96RmsnKj+SvFWdcWlzRhw+YEEpsVccoxK3nr856UlTvf65z8qH3OKzKijDOPX8Ybn/auz5AbKY/zQWuIvyDkl001a+1vAMaY3dvTgXRjzIm7PaUb8KO1tsj7vG+B04FH6z/a6g0bkcY3c5xMz8Cj0slMj2Ddqr/vLh10TDqXjF9NQvNS7h7bu4Gj9K2QEA/T5q6idcdSPn2tBfa3GFp1KGXYKTkMHplLbmYYz05qw+Z1gZftqI2QEA9PfbKM1h1K+OyNZOySWABufHQN/Y7KYcNfUbz0oJMNa5FcSsYWZxDmrnBRlB9KfLNy8rKDZwDm8cADLy/F44HZ77ZiznutKh/r0SeXnMwmbE7Z+WPVsk0JT3+wmKKCMGY81ZHli5v6I+w6Cwnx8NRHf3iXb0vskjim3t6Z+15eSen2EIoKQrnxTGcXc5tOTvbr8XeWERrq4Y2n2rJ4fjN/hl9rmdsi+fA/HXlt1nxKt4fw68IW/LYokaNGpHHRdas598q1LPmpOa8+1ZXysl3zLcedtonFCxL9FHnt7CjfaJuYywc/HMKKDcmcPmgFV434icuGL+aX1W149vMBlFU4A7AeHbYy46b3yMiL4elPB7Jua3MAbjj1B6Z9NpDoyMDeO3HrUQuYMn8Q0U12ZtXvmXcUz476nO3lYRSUNuH8t07/2/NOP3Ql36931l8HxBaSlh9b+djW/BgO8A66A1lyYj4HdcjkzzVJXDX6Jw7tupXLz1hMaVkoz7/TH7vO2ZPcMqmAF+79L0XFTXjlwz4sXeX8Zl92+mLem9ODklJlrMW36vsTFWWM+b3K/Yette/UYT7LgAeNMS2AYuAE4BcfxFdnYWFuBgzL4LUnDyIisoLRY9Zzx9WH73HahV8dwMKvDqDH4dlceN1a7rhqz9MFA7fbxbX/MsTEV3D39HV0MMWER3go3e5i3MiuDBmZw81PpHLzKL/vUNgvbreLsScdSkxcOZOeX0WHrkWkrIpmyq2dCQnxcM096xl6Uhbz3g/eWvKqJlzQm8z0CJo2L+XBl5eycW0UyxYnADDsxG18UyVbnbWtCRcfO4D83HAO6p7PpKeXc/UpfSkuDJ4fKLfbxdhTejnL9zlLhy5FjLp0C3eNORi7JI4zxmziittTePL2zoSGemjTsZjbzu9OYstSHntrOdec0GuX0phAFxtXxsCj0rnspCMpLAjj348s4egTNvPatC5kZzQhLNzDuDuXc9Yl63jrpc6Vz+vZN4vjTtvEhMv6+TH6mnN7Qrh4ypnERm5n8iVfcGDLLJ6b1Z/M/GjCQ91MPGs+Fx7zO6/M64PdmMioB8+nuDScQQdv4JFL5nL2I+cypFsK2QVR2E1JHNZ5s7+7VK2hndaTVRTFivQk+rbdVNl+4eF/cO1/T2RpWjKX9P2NCcMWcM+8oysfP6nbKronp3Ppu6f5IWrfiIwo496xX/LsmwMpKmlCaIib+NjtXHf/yRzcKYO7rv2K8yecTVZONOfeNJq8wki6dMjg/vH/47I7TqfVAfm0PiCfZ98aSHJivr+7E3y8e/gCgTGmHTADSMaJ6kVr7ZPGmObAO0BHYD1wtrU22xjjAp7EGUcWAZdYa3/1zuti4E7vrB+w1r5el5gauhSkLoNqrLV/Ao8AXwBzgN+BCt+FWXt9j8hgzco4crIiaNW2mOQ2xTzz7iJenfU9icnbeertH2nWYtfdp8t+bUbLtsXEJwR2zV5NFOaFsuSHWPodnU/GlnC+n+VkLBfMbkqnbsX7eHbwKMwP449F8fQdmlvZ5na7+PbTFgwZkQVA5tYmJLZylmlIqIfouArysoNn0AWQme7sYcjNasLCL1vQtafzYxMS6mHw8Azmz965AVFeFkJ+rpONX70iji2pUbTtGJzLvHL5DsvmwG6F2CXOHqf5nyfS/XDnPchIi2DRl82pKA9h68ZINq2LpE3HEn+GXWu9B2SydVM0eTlNqCgP4YevkunWM4fsjAjARXlZCP/7pA1de+z8nHfsks/4Scu578be5Oc28V/wdVBQEsGva1oz0KSSmR8DuCirCOWznw3d26UDULS9SWXpyMKV7QkLddM0upieHdM4snsKH94+k/vP/x99DtrM3ed+6cfe7NlhbdI4uvN65lz+Bo+dOI/+7TbxzGmfY5IyWZqWDMAcexC9W2+tfM7A9hu5ov9ixn80sjJrn14QQ8u4gsppkuMKSS+IadjO1EJoqJt7x37J/xZ25rvFHQHYlh3Dd790AFysXJfkHOsTV0JZeSh5hc4xT3+lJLJ5WxxtW+ZySOd0unbM4M3H3+Gp2z+jbcs8npj4uf86JfujHLjZWtsdGAhcZ4zpDkwEvrTWdgG+9N4HGAl08f5dCTwH4B2I3w0MwCk7vtsYU6ddk0Fzuj1r7XRrbR9r7VAgG1jlz3iGjdzKt7OdXUrrV8dy3tHDuPSEI7j0hCPI2BrB+HMGkJ0ZQat2RezYtOt8cB7hTdy7HCAWTJo2Lycm3tmeaRLp5vChBaSujuSHOfH0GuKsmHsOKmTj2uAuA2navIyYuHIAmkS4OeyIPDaujaRVhx2DKQ8Dh2ezcY2zwl70ZQLDz8gA4MiRWSxZGM+OuvpgEBFVQVR0eeXtwwbnkPKX88N62KBsNq6LJnPrzmUa36yUkBDnM92ybTGtOxSzZWPk32ccoHZdvhUcNiSX1DXRRMdW0Ma7gXDYETlsWO2Uviz8XzN6DsgDIL5ZGW06lbAlNbg+49vSIjGH5hARWQF46NU/k9R1sTRL3LHx72Hg0emkrHZKApJaFnPH47/zf5MOZfOGwB1kVZUQU0xspNOfiLBy+nXZSEp6Ai3idpQ1eBh2yDrWpDnlHs3jdq6bu7dLx+WC3KJInps9gFMfuIDTHzqfSTOHs3h1a+5961g/9Gjvnvx+IMNfuogR0y9gwuf/4qfUNoz/eCSxEaV0SMgBYFCHjazNSgDg4KRt3DX8W8Z9PJKs4ujK+SxIacegDqnER2wnPmI7gzqksiClnR96VBMeJlz2HRu2JPD+3EMrWxf82oHe3bYA0DY5l7BQN7n5kTSNKybEe/qKVkl5tE3OY8u2eD75uhtn33gu590ymvEPncTGtHhumrx7BaoEA2vtlh0ZZ2ttPvAn0AY4FdiRcX4dOM17+1RghrXWY61dBCQYY1oBxwPzrLVZ1tpsYB4woi4xBU1azRhzgLU23RjTHqe+2m+HqEdEVXDYwCyevr/bPqcdMjydY0/eQnmZi9LtoUy+9VCCadBVVfPkMm55cgMhIRASAvM/bcqP/4tn2U8x3DYthdOvyKC4MISptwTqSrlmmh1Qxi2PrSEk1IPLBd/Nas5PXyfw2DsriI6rwAWsWxnNtEkdAZj7zgFMeGIN07/6nfzcMCaPD64ymGYtSrnzqRUAhIZ5+ObzA1j8vTP4GDpyG9/O2rXc5dC+uVwwLoXychcet4tp93ahIDd4NhabJZVyy2OrCQkBV4iH72a14Kevm/HUHQdyxzMWj9tFQV4YUyY6JRGL5ydw+BG5vDDndyoqYPrkDuQH2caxXZbAgi+TeXLmQioqXKy18cz+sC33TVtM04QycHlYtyqeaQ8667Rzr1hLfNMyrv33nwBUVLi44YLAPitIi/gi7jrna0JcHlwhHr5a0pkFf3bg6as/pVlMCbg8/LWpBY9+MBSAY3quZdSgFVS4XWwvC+OuN44lWNfNO1R4Qrhn3jCmnDIXt8dFXkkEd33hlIHcPHQh0eFl/N9JXwCwJT+W8R+fQF5JJC8s6sNb578PwAuL+pJXEpgbyj26bOW4IatZk9qMF+/7LwDT3+/L7PldmXD5d0x/4APKy0N55OWhgIueJo1LR/1KeUUIHreLKa8PIb8wuDaKA5UrAA8sNMZ0BA4DfgSSrbVbvA+l4ZSKgDPoTq3ytI3eturaa83lqcc3Zw+n25tjrZ1ojBkP3Aq0BNKBWdbaMcaYlji10/GAGygAultr84wx3wEtgDLgJmvtXvfNrfo9xTP++Ed836kAVbFtm79DaHAhkYG58q8vrsh/3g+CpzSwDx7ztZD4wD5XdH3IOqaTv0NoUAWtg2ZHsc8k/RH85Y+18c2c2xYDff0dR31Z9edmz9jLpjfIa8385OqUI444IqNK04vW2hd3n84YEwt8Czxorf3QGJNjrU2o8ni2tbaZMeYzYLK19ntv+5fAbcBRQKS19gFv+ySccubHqaV6zVhba0OraX8KeGoP7WlA22qec6RvoxMRERGRWmugjHVSUlKGtXavGynGmHDgA2CmtfZDb/NWY0wra+0Wb6lHurd9E1B1t3pbb9smnMF11fZv6hLzP2/TWURERESCnvcsH9OBP621T1R56BPgYu/ti4GPq7RfZIxxGWMGArnekpG5wHHGmGbegxaP87bVWtDUWIuIiIhIAAicS5oPAS4EllY5vfPtwGTgXWPM5TgXIzzb+9gsnFPtrcY53d6lANbaLGPM/cDP3unus9Zm1SUgDaxFREREJOh4a6WrO+r4b6fzsdZ6gOuqmdcrwCv7G5MG1iIiIiJSM57APCtIoFCNtYiIiIiIDyhjLSIiIiI1p4x1tZSxFhERERHxAWWsRURERKSGPMpY74Uy1iIiIiIiPqCMtYiIiIjUXOCcxzrgKGMtIiIiIuIDyliLiIiISM3oPNZ7pYy1iIiIiIgPaGAtIiIiIuIDKgURERERkZpTKUi1lLEWEREREfEBZaxFREREpOaUsa6WMtYiIiIiIj6gjLWIiIiI1Jwy1tVSxlpERERExAeUsRYRERGRmvGgS5rvhTLWIiIiIiI+oIy1iIiIiNSQR5c03wtlrEVEREREfEAZaxERERGpOWWsq9V4B9bl5VRs2+bvKKQeuUtK/B1Cw9q+3d8RSD1zl/zzlnGzz4r9HUKDahbeeH92qxUS6u8IRBrMP/AbLiIiIiJ15lbGujqqsRYRERER8QFlrEVERESkZjyoxnovlLEWEREREfEBDaxFRERERHxApSAiIiIiUnMqBamWMtYiIiIiIj6gjLWIiIiI1JBHGeu9UMZaRERERMQHlLEWERERkZrTBWKqpYy1iIiIiIgPKGMtIiIiIjXjATxuf0cRsJSxFhERERHxAWWsRURERKTmdFaQailjLSIiIiLiA8pYi4iIiEgNeXRWkL1QxlpERERExAeUsRYRERGRmlONdbWUsRYRERER8QFlrEVERESkZjwoY70XyliLiIiIiPiABtYiIiIiIj6gUhARERERqTmVglRLGWsRERERER9QxlpEREREasgDbre/gwhYGlj7QEiIh6fnrCJzSzh3XXwg4OGS29I48qQc3G4Xn81owcfTk/wdpk8ltS5lwpMbSEgqBw/MeqMFHzWyPu6w+/K9bVoKXXoVU1Hmwv4exZO3tqOi3OXvMH0mJr6cGx9PpaMpweOBJ25uT2KrMi68KY12XUoYf2JX/voj2t9h+szri5ZTXBCK2w0V5S7GnWAqHzvjqnSuvGszZ/XoQV528K4ub3o8hQHDc8nJCOOq4d13eeyMK7dy5V2bOOvQnuRlh3H0qCzOvjYNlwuKC0J5+t/tWPtncC7vkBAPT77/G5npEdxz9SH0GpjN5RPW4QqBkqJQnvh3V7ZsiGL4qK1cPmEtGVsjAPhsZmvmvt/Sz9HXTkxcGdffY+nQpRCPB6bedTCnXbCRNh2LAIiNK6cgP4xxZ/XjgNbFvPDxT2xc7yxX+0c80+43e5t9wGnToZCJjy6tvN+qbTH/ebYz8QllDDxqG2435GY34YlJh5C1LYLYuDJuuG8FrdoWU1oawtS7u5OyOtaPPZDGql5/KYwxFcDSKk1vW2snG2PGAjcAnYEka23Gbs/rBywEzrHWvu9tewQ40TvJ/dbad+oz9to4bUwGqX9FEh1bAcBxo7NJal3GmKEH4/G4aNqizM8R+l5FuYsX72vN6qXRRMVUMG3OKn6dH8eGvyL9HZrP7b58v/qwGY+MbQ/AxGc3MPK8TD6bkejPEH3qmvs28cvX8TxwZSfCwt1ERLkpyA3lvis6Mn5yqr/Dqxe3nnXQ3wbOSa1LOXxoPls3hvspKt/54r3mfPJaEhOmrt+lPalVKYcPzWPrxiaVbVs3NGHCmV0pyA2j79G5XP/oBq4/+eAGjtg3Tr1oE6lroyu/u2PvWcN913YndW00J567mXOu2cCUfzsDyvmzk3ju/oP8Ge5+ueq21Sxe0JyHbu5BWJibiKgKJk84pPLxMbesprAgtPL+ltQoxp3Vzx+h+sSmlBjGjR4IOBtQM+Z9x8KvksjPC+c/z3QG4JTzNnDeVWuZ9kA3zh6znrUr43jgxl607VjItbev5PYr+/izC8FNNdbVqu8a62Jrbe8qf5O97QuA4UDK7k8wxoQCjwBfVGk7ETgc6A0MAG4xxsTXc+w1ktiqlP7H5jH7zeaVbSddlMHMKcl4PE4WMzcz+H+Yd5eVHs7qpU62o7gwlNTVkSS2anwbEHtavj9/FQ+4ABf2t+hG1e/ouAoOHVDInLec/paXhVCYF0bq6kg2rml8G017c9U9m5j+YOtG8fux7Mc48nNC/9Z+1T0bmf5gm136uGJxLAW5zkbGyl9jgvbz3SJ5O/2GZTH3vZ2ZZ48HomPLAYiJqyArPcJf4flUdGw5PfrkMPfDVgCUl4dQmF/1d8fDkcen8+2sZP8EWM96DcgiLTWK9C1RFBfu3ECOjKyo/Gy3P7CAJT81A2Dj+hiSW5eQ0Hy7P8KVRs4v+zattb8BGLPHXU/jgA+AqpvS3YH51tpyoNwY8wcwAni3nkPdp6vv3czLD7QiOnZnvVGrDqUMOyWHwSNzyc0M49lJbdi8rnGswPckuW0pnXsUs/LX4NxdvDd7Wr47hIZ5OPbMbJ6f1NoPkdWPlu23k5sZxs1TNnBg9xL++iOK5+5qw/bivw/KGg2Pi4feWgMe+PyNFsyemcig43LJ2BLO2hVR/o6u3gw6LoeMtPC9lnmMOCeTn78OiBxGrV11+xpeebwTUTEVlW1P3tmFe19cTmlJCEUFodw4unflY0P+lUGPvrlsWh/Fiw93JiMteNbZLdsUk5sdzo0PrOTArgWsXhHH8490qfze9uiTS05mEzZviN7lOU+/+zNFhaHMePpAlv+a4Kfo99+wEWl8M2fnBtRFY1dz7MlbKCwIY+IYJyu9blUcg49NZ/lvzejaI5cDWpWQmLydnKzgWc4BQxeI2av6zlhHGWN+r/I3em8TG2PaAKOA53Z7aAkwwhgTbYxJBI4G2tVPyDU3YHgeORlhlZnbHcIjPJRudzFuZFdmz2zOzU80zt3nAJHRFUx6eT3P39WaooLGNfiqbvnuMO7hjSxbFMOynxpPnV5oKBx0aBGfzUjkuuMNJUUhjB6b7u+w6tVNow5i7AjDHRccyCmXZNBjQAHnjNvKjMdb+Tu0ehMR6eaccWnMeLz6jcJeg/M5/pwMpj/YpgEj843+R2WSk9mE1cvjdmk/7eJN3H3lIVx01ADmfdiSKyeuBeDHr5tzybH9ue7UPvz2QzNunmz9EXadhYZ6OKhbAbPeac24s/tRUhzK2Zfv3CE8bORWvpl1QOX9rG0RXHzcYMad3Y+XHuvCrY+sICqm3B+h77ewMDcDhmXw/Rc7+zdj2kFcfPyRfPN5S04+x/n9ffeVjsTGl/P0O4s45dxU1qyMw+1uPMfGSOCo74x1sbW2dy2mnwrcZq11V81mW2u/8NZd/wBsw6m/rtjjHBpQ936FDDwuj37HrqBJhIfouApufTqFjC3hfD+rKQALZjfl5imNc2AdGuZh0svr+erDZiyYneDvcHyuuuX76LgOnH9TGk1blPPkrR39HaZPZWwJZ9uWcOxvMQB8/3kCZzfygXVmmlNfnJsZzoLZTek5qICW7Ut5bt5KAJJalfHMXMv4E7uSva1xlHW16ridlu1Kee6LPwGn1vqZOX8y/qSDyd4WTqduRdzwaAp3XngQ+TnBd9Bm98PzGHhMJv2GZRHexE10bAX3PL+MdgcWY/9wMvDzZydy/0vLAMjP2blc577XkstuWeeXuOsqY2sEGVsjsEud353v5yVxlndgHRLqZvDwbYwf3bdy+vKyEPJznbza6hVxbEmNom2HIv5aEXx7J/oekcGalXF7zDx/PasV9z7zGzOf60xxYRhT7tpRc+7h1VkL2LKx8e6RqnduZayrE2hrzL7A295BdSJwgjGm3Fr7kbX2QeBBAGPMm8Aq/4XpePXhVrz6sJPV6jmogDOvTufRcR247PbN9BpSwBdvR9BzUCEb1zbGXU0ebvq/VFL/iuTDFxvn2UCqW74jzsuk71H53HZ258o6+sYie1s4GZub0LZzCRvXRNL7iHw2rGqMn19HRFQFISHOcQIRURX0GZbPzCktGd2rR+U0ry9azriRJqjPCrK79SujGN27Z+X91xcuY9wJB5OXHUZS61Luemkdj13fkU3rgrOu/rUnOvHaE50AOLR/Dmdcton7ruvOzO8X0aZjEZvWR3PY4BxS1zp7o5ollZK9zdnAGnBMJqlrgqusLTszgm1pEZV96z0gmw1rnI3jwwZms3FdNJlbdy7L+GalFOSG43a7aNm2mNbti4J2kDls5Fa+nb2zDKR1+6LKkpeBR6ezcZ3zPsTElbG9OJTy8hCOP30zy35N2KUeW8RXAupTZa3ttOO2MeY14DNr7UfeAxoTrLWZxpieQE+qHNwYaN6Zlsxt01I4/YoMigtDmHqL36tWfO6Q/oUMPyubtSsieXaes9v01YdbeQ/sa9zGT97I1o1NmPrpXwAsmNWUmVOC69Rce/PMpDbc9nQKYeEe0jY04f9uas/gETlc+8AmmjYv5/4Za1mzPIo7zu/s71D3W7Okcu6e7mQnQ0Ph648S+OWbxvcZnjhtHT0H5dO0eTlv/LyU//xfK+a+vecz2Zx/4xbiEsoZ+5Czp62i3MW4E4PzrCBVuStcPDWpC3c89Sdut4uCvDCm3t4FgFMv3MSAo7OoqHCRnxvGE//u6udoa+/5h7tw6+QVhIW7SdsYxZRJzjIbOvLvBy0e2ieHC65bR3l5CB43TLvfUJAXfHtjIqIqOGxgFk/f362y7dLr/6JNxyI8bhfpWyKZ9oDzPrTrVMjND6zA44GUNTE8eXf36mYr++TB49F5rKvj8tRjAfoeTrc3x1o70RgzHrgVaAmkA7OstWN2e+5rOAPr940xkcCv3ofygKuttb/v7bVX/bLGc13/ib7piEggcDWu7LjsgeufdzHc0NgYf4fQsMIDKp/VMEIa1/E3+zJn67OLcfbAN0qrfl/vGT/8oQZ5rTkZLwbde1mv33Br7R6/Tdbap4Cn9vHcS6rcLsE5M4iIiIiI+IsH1VjvxT8vPSIiIiIiUg/+gfukRERERKTOdB7railjLSIiIiLiAxpYi4iIiIj4gEpBRERERKTm3DrdXnWUsRYRERER8QFlrEVERESkZjweHby4F8pYi4iIiIj4gDLWIiIiIlJjHtVYV0sZaxERERERH1DGWkRERERqTjXW1VLGWkRERETEB5SxFhEREZGa8XjArYx1dZSxFhERERHxAWWsRURERKTmPDorSHWUsRYRERER8QFlrEVERESkxjyqsa6WMtYiIiIiIj6gjLWIiIiI1JBHNdZ7oYy1iIiIiIgPaGAtIiIiIuIDKgURERERkZrxBM7Bi8aYEcCTQCjwsrV2sp9DUsZaRERERIKLMSYUeAYYCXQHzjXGdPdvVMpYi4iIiEhtBMbBi/2B1dbatQDGmLeBU4EV/gxKGWsRERERCTZtgNQq9zd62/yq0Wasu/btnDHP/V6Kv+MQERGRf5QO/g6gPnXt23nuPPd7iQ3xWhs2bIj817/+9UuVphettS82xGvXVaMdWANJ/g5AREREpJEZ0VAv1L59e6y11T28CWhX5X5bb5tfNeaBtYiIiIg0Tj8DXYwxnXAG1OcA5/k3JNVYi4iIiEiQsdaWA2OBucCfwLvW2uX+jQpcHk9gnItQRERERCSYKWMtIiIiIuIDGliLiIiIiPiADl6sIWPMacB/gW7W2pXetgpgqXeSDdbaU7ztY4EbgM5AkrU2w9vuwrn05glAEXCJtfbXBuxGjfmovwcDrwKHA3dYax9vyD7Uhi/6W2Ve/YCFwDnW2vcbpAN1UMs+zwT6AmXAT8BV1toyY0wz4BWc96IEuMxau6xBO1JDtezvdJz+uoBVON/VAmNMBDAD6ANkAqOttesbsh+14aM+d8BZxklAFnCBtXZjg3akBnbrF8Db1trJdVkfG2MeAU70zud+a+07DdOL2qlDn6tdJxtjrgeuwFn+L1lrpzZIJ2qhtv2t8ry/rZODZRlL8NHAuubOBb73/r/b21Zsre29h2kXAJ8B3+zWPhLo4v0bADzn/R+IfNHfLGA8cFq9ROhbvujvjkusPgJ8US9R+lZt+jwTuMB7+01gDM7n93bgd2vtKO+P9jPAsfUZ9H6oTX9vtNbmARhjnsA5QGYycDmQba09yBhzDs6yHl3fge8HX/T5cWCGtfZ1Y8wxwMPAhfUdeB34ZH1sjDkRZ+DZG4gAvjHGzN7x3gQYn6yTjTE9cAbV/YFSYI4x5jNr7Wofx7u/fLJODrJlLEFGpSA1YIyJBY7A+VE9Z1/TW2t/qyaLdSrOD5THWrsISDDGtPJpsD7gq/5aa9OttT/jZDkDlg+XL8A44AMg3WcB1oM69HmW93PrwclYt/U+1B34yjvNSqCjMSa5fqKuuzr0d8cA0wVEATuO8j4VeN17+33gWO80AceHfa5cxsDXOO9B0KjD+rg7MN9aW26tLQT+oAHP2+sLdVgndwN+tNYWec+08C1wev1H6ht1WCcH/TKWwKWBdc2cCsyx1q4CMo0xfbztkcaYX4wxi7y7XPclIC+/uQe+6m+w8El/jTFtgFE4ma9AV6c+G2PCcbKVc7xNS/D+ABtj+uNccazt7s8LALXurzHmVSANOBh42ttc+R32DkBygRYNEH9d+KrPlcsY5/MdZ4wJxD5HGWN+r/K3rz0J1a2PlwAjjDHRxphE4Gh2vQhFIKltn6uzDDjSGNPCGBONUx4TiH2uVX/3sk4OpmUsQUYD65o5F3jbe/tt732ADtbavjgnJJ9qjOnsj+DqgfrrqG1/pwK3WWvd9RKlb9W1z8/iZHq+896fjJPp+x0nM/QbUFGfgddRrftrrb0UaI1zftRALveojq/6fAswzBjzGzAM50IMgbiMi621vav81alm1lr7BTAL+AF4C6c2NxD7C77r85/sLJeYA/xOYPa5tv2dyh7WyUG2jCXIqMZ6H4wxzYFjgEONMR4gFPAYYyZYazcBWGvXGmO+AQ4D1uxldgF5+c2qfNzfgOfj/vYF3jbGACQCJxhjyq21H9VjF2qtrn02xtyNcwDbVTvm5S0fuNT7uAtYB6xtuN7s2/4sY2tthTHmbeBWnIO+dnyHNxpjwoCmOAcxBhRf9tlau5mdeyVigTOstTkN2Z96Uu362Fr7IPAggDHmTZyDORs1a+10YDqAMeYhnAx+sKt2nfxPXMbSMJSx3rczgf9YaztYaztaa9vhDB6GGucMAXh3JQ0BVuxjXp8AFxljXMaYgUCutXZLfQZfB77sbzDwWX+ttZ288+iIU397baANqr1q3WdjzBjgeODcqtkfY0yCMaaJ9+4YnGx2oB0AVKv+er+fB3nbXcApwErvvD4BLq4y36+8deeBxmd9NsYkGmN2/Fb8G+cMIY3BHtfHxpjQHaUuxpieQE+C42Dk/WKMOcD7vz3OhtSb/o1o/1W3Tv6nLmNpGMpY79u5OLvIqvoAuAdINMa4cTZQJltrdwxCxuNke1oCfxhjZllrx+DsejoBWI1zeqdLG6QHteOz/hpjWgK/APGA2xhzA9A9wAZevly+waLWfQaeB1KAhd7sz4fW2vtwDnp63ZsVXY5zoFygqVV/vYPI140x8TinHlsCXON93nTgP8aY1ThnWNjnQYF+4ss+HwU87F3G84Hr6j/8OonyliTtMMdaO7EO6+Nw4Dvv5zwP5/SC5Q3Uh9qqVZ/3sU7+wDvYLAOuC9C9ErVdxtUJpmUsQUaXNBcRERER8QGVgoiIiIiI+IAG1iIiIiIiPqCBtYiIiIiID2hgLSIiIiLiAxpYi4iIiIj4gAbWIuJ3xpgK7yWKlxlj3jPOZZXrOq/XjDFnem+/bIzpvpdpjzLGDK7Da6z3nge6Ru27TVNQy9e6xxhzS21jFBGRhqeBtYgEgh2XKu4BlAJXV33Qe5XDWrPWjqlyLu49OQqo9cBaRERkT3SBGBEJNN8BPY0xRwH3A9nAwcaYbsBknMFwBPCMtfYF75UCnwb+BaTiDMwB8F6y+xZr7S/GmBHAQziX987AuZjN1UCFMeYCYBzO1QafB9p7Z3GDtXaB98IZbwFtgIU4F1HZK2PMRziXzI4EnrTWvljlsSnAcUAacI61dpsxpjPwDM5l44uAK6y1K/82YxERCVjKWItIwPBmpkcCS71NhwPXW2u74gyEc621/YB+wBXGmE7AKMAA3YGL2EMG2hiTBLwEnGGt7QWcZa1djzOInuLNln8HPOm93w84A3jZO4u7ge+ttYcA/2XnwHtvLrPW9gH6AuN3XEIZiAF+8c7rW++8AV4ExnmfcwvwbA1eQ0REAogy1iISCKpeqvg7nEuHDwZ+stau87Yfh5PJPtN7vynQBRgKvGWtrQA2G2O+2sP8BwLzd8zLWptVTRzDge7eSx0DxBtjYr2vcbr3uZ8bY7Jr0KfxxphR3tvtvLFmAm7gHW/7G8CH3tcYDLxX5bUjavAaIiISQDSwFpFAUGyt7V21wTvALKzS5MLJ6M7dbboTfBhHCDDQWluyh1hqzFvGMhwYZK0t8pakRFYzucf7ujm7vwciIhJcVAoiIsFiLnCNMSYcwBjT1RgTA8wHRhtjQo0xrYCj9/DcRcBQb+kIxpjm3vZ8IK7KdF/g1Frjna639+Z84Dxv20ig2T5ibQpkewfVB+NkzHcIAXZk3c/DKTHJA9YZY87yvobLGNNrH68hIiIBRgNrEQkWLwMrgF+NMcuAF3D2uv0X+Mv72Aycgwt3Ya3dBlyJU3axhJ2lGJ8Co7yn+jsSGA/0Ncb8YYxZwc6zk9yLMzBfjlMSsmEfsc4Bwowxf+IccLmoymOFQH9vH44B7vO2nw9c7o1vOXBqDd4TEREJIC6Px+PvGEREREREgp4y1iIiIiIiPqCBtYiIiIiID2hgLSIiIiLiAxpYi4iIiIj4gAbWIiIiIiI+oIG1iIiIiIgPaGAtIiIiIuIDGliLiIiIiPjA/wMtUtRyBgdBDgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_decoded = label_encoder.inverse_transform(best_model['estimator'].classes_)\n",
    "\n",
    "with sns.axes_style(\"dark\"):\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=score.loc[0,'confusion_matrix'],\n",
    "                                display_labels=labels_decoded)\n",
    "    disp.plot(ax=ax, cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A510</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.04</td>\n",
       "      <td>782.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A511</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.10</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A514</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2091.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A529</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1576.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A530</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.21</td>\n",
       "      <td>48469.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A539</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.39</td>\n",
       "      <td>37927.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E109</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.22</td>\n",
       "      <td>5022.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E119</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>13950.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E149</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2246.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.18</td>\n",
       "      <td>112138.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.28</td>\n",
       "      <td>112138.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score    support\n",
       "A510               0.02    0.44      0.04     782.00\n",
       "A511               0.05    0.99      0.10      75.00\n",
       "A514               0.04    0.23      0.06    2091.00\n",
       "A529               0.03    0.32      0.06    1576.00\n",
       "A530               0.56    0.13      0.21   48469.00\n",
       "A539               0.47    0.33      0.39   37927.00\n",
       "E109               0.18    0.30      0.22    5022.00\n",
       "E119               0.38    0.35      0.36   13950.00\n",
       "E149               0.13    0.33      0.18    2246.00\n",
       "accuracy           0.24    0.24      0.24       0.24\n",
       "macro avg          0.21    0.38      0.18  112138.00\n",
       "weighted avg       0.46    0.24      0.28  112138.00"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.loc[0,'classification_report'].round(2).rename(index={str(class_label):label for class_label, label in zip(best_model['estimator'].classes_, labels_decoded)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Characterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['categorical__Genero_Hombre', 'categorical__Genero_Mujer',\n",
       "       'categorical__GrupoEtnico_Blanco', ..., 'text__vulneren',\n",
       "       'text__vulneren derechos', 'text__vulneren derechos requiere'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.named_steps['feature_selector'].get_feature_names_out(best_model.named_steps['preprocessor'].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted :['E119'], real: ['A539']\n"
     ]
    }
   ],
   "source": [
    "# Model test\n",
    "print(f'Predicted :{label_encoder.inverse_transform(best_model.predict(X_test.iloc[905].to_frame().T))}, real: {label_encoder.inverse_transform([y_test[905]])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20), dpi=800)\n",
    "plot_tree(best_model.named_steps['estimator'], num_trees=2, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the best performing model and pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/output/best_model_score.pickle']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "dump(best_model, str(save_path))\n",
    "dump(score, str(save_path.parent / f'best_model_score{save_path.suffix}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and save the full prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/output/prediction_pipeline.pickle']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_pipeline = PredictionPipeline(estimator=best_model, preprocessing_fn=clean_and_preprocess_datasets, label_encoder=label_encoder)\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "dump(\n",
    "    prediction_pipeline,\n",
    "    str(save_path.parent / f\"prediction_pipeline{save_path.suffix}\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['E119', 'A539', 'A530', ..., 'A539', 'A510', 'A530'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_pipeline.predict(X_test, preprocess_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A539', 'E119', 'E119', ..., 'E119', 'E119', 'A539'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_pipeline.predict(\n",
    "    X={\n",
    "        \"df_sociodemograficos\": pd.read_csv(\"data/sociodemografico.csv\", sep=\";\"),\n",
    "        \"df_laboratorios\": pd.read_csv(\"data/laboratorios.csv\", sep=\";\"),\n",
    "        \"df_notas\": pd.read_csv(\"data/notas.csv\", sep=\";\"),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_and_preprocess_datasets({\n",
    "        \"df_sociodemograficos\": pd.read_csv(\"data/sociodemografico.csv\", sep=\";\"),\n",
    "        \"df_laboratorios\": pd.read_csv(\"data/laboratorios.csv\", sep=\";\"),\n",
    "        \"df_notas\": pd.read_csv(\"data/notas.csv\", sep=\";\"),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'df_sociodemograficos':    Edad Genero GrupoEtnico AreaResidencial EstadoCivil TSangre  IDRecord\n",
       " 0    39  Mujer     Mestizo     Zona Urbana    Separado     NaN         0,\n",
       " 'df_laboratorios':    Codigo             Fecha Valor  IDRecord\n",
       " 0  902045  22/02/2022 18:43   NaN         0\n",
       " 1  902046  22/02/2022 18:43    20         0,\n",
       " 'df_notas':                   Tipo                                               Plan  \\\n",
       " 0  Confirmado Repetido  - Paciente se remite para analisis de urologia...   \n",
       " \n",
       "    IDRecord  \n",
       " 0         0  }"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.preprocessing_utils import preprocess_json\n",
    "\n",
    "import json\n",
    "with open('scripts/utils/sample_example.json') as in_file:\n",
    "    sample_data = json.load(in_file)\n",
    "sample_data\n",
    "\n",
    "preprocess_json(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['E119'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_pipeline.predict(\n",
    "    X=preprocess_json(sample_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DS4A_Project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32aecc324f8df26357122ac4599014cc1f704524e77889bea473ffbdc8bf89d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
